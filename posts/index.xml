<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Rohit Goswami</title>
        <link>https://rgoswami.me/posts/</link>
        <description>Recent content in Posts on Rohit Goswami</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 16 Mar 2020 02:24:00 +0000</lastBuildDate>
        <atom:link href="https://rgoswami.me/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Everyone Should Get an A - David MacKay</title>
            <link>https://rgoswami.me/posts/mackay-all-a/</link>
            <pubDate>Mon, 16 Mar 2020 02:24:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/mackay-all-a/</guid>
            <description>Background  I recently read this post written by the now deceased Prof. David MacKay&amp;nbsp;1 It should be read widely, however, given that it is distributed as a ps.gz which is then a .ps file, and thus probably inaccessible to many of the people who should read it, I decided to rework it for native online consumption (there is also a pdf) THIS IS NOT MY CONTENT&amp;nbsp;2 Now, enjoy the post  Everyone Should Get an A Imagine a University – call it Camwick – where all students arrive with straight A grades.</description>
            <content type="html"><![CDATA[

<h2 id="background">Background</h2>

<ul>
<li>I recently read this post written by the now deceased <a href="http://www.inference.org.uk/mackay/" target="_blank">Prof. David MacKay</a>&nbsp;<sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup></li>
<li>It should be read widely, however, given that it is distributed as a <code>ps.gz</code>
which is then a <code>.ps</code> file, and thus probably inaccessible to many of the
people who should read it, I decided to rework it for native online consumption (there is <a href="http://www.inference.org.uk/mackay/exams.pdf" target="_blank">also a pdf</a>)</li>
<li><strong>THIS IS NOT MY CONTENT</strong>&nbsp;<sup class="footnote-ref" id="fnref:fn-2"><a href="#fn:fn-2">2</a></sup></li>
<li>Now, enjoy the post</li>
</ul>

<h2 id="everyone-should-get-an-a">Everyone Should Get an A</h2>

<p>Imagine a University – call it Camwick – where all students arrive with straight
A grades. They are successful, enthusiastic, and curious. By the time they
leave, only one third still receive straight As. The other two thirds get lower
grades, do not enjoy their studies, and are not fun to teach. Is Camwick
University a success? Camwick could point to its excellent teaching assessment
scores and argue that it is ‘adding value’: students emerge knowing more. Future
employers love the University’s policy of assigning grades – the University
ranks its students, saving companies the bother of assessing job applicants
themselves. But should a University be a sorting service? Isn’t something wrong
with an institution that takes in mainly A-quality input and turns out less than
half A-quality output? If a University fails to turn out as much A-quality
enthusiasts as come in, is it in fact a place of intellectual <strong>destruction</strong>,
throwing away the potential of the majority of its students? What are the roots
of this destruction?</p>

<h3 id="exams">Exams</h3>

<p>I would recommend that Camwick consider abolishing traditional exams. In the
current system, Camwick teaches Anna, Bob, and Charlie, who are  all smart, then
examines them; Anna comes &lsquo;top&rsquo;, Bob &lsquo;second&rsquo; and Charlie &lsquo;third&rsquo;. Perhaps
Charlie, given a little more time, would have figured out the material, but he
wasn&rsquo;t quite ready when the exam arrived - perhaps because other courses
consumed his attention.</p>

<p>Bob&rsquo;s response to his &lsquo;failure&rsquo; is to adopt strategies of tlittle educational
value: he parrot learns, he crams, and he asks lecturers to tell him what&rsquo;s
going to be on the exam. The exams become the focus of attention, even though
the purpose of Bob&rsquo;s going to the University was learning.</p>

<p>Charlie&rsquo;s response is to give up on doing &lsquo;well&rsquo;, and coast through University,
no longer understanding everything. He loses self-worth and resents the
University for making him feel bad.</p>

<p>Some courses at Camwick assign grades using continuous assessment instead of
exams. But continuous assessment has the same effect as exams on Bob and
Charlie. So course grades based on continuous assessment should be abolished at
the same time as exams.</p>


    <figure class="left" >
        <img src="/macKayAs/fig1.jpg"   />

        
            <figcaption class="center" >Figure 1: Everyone can get an A, regardless of learning rate, if their education is not halted by exams. Traditional system on the left, with an educational system on the right.</figcaption>
        
    </figure>



<p>If Camwick had no exams, the focus of attention would have to be elsewhere. How
about education, for example? Students could spend their time at Camwick
exploring subjects that interest them, and attending classes that offer
something they want to know about, free from the stress and misdirection of the
exam system. Lecturers would at all times be friends rather than adversaries.
[When I was an undergraduate at Cambridge, I asked a physis lecturer to clarify
topic \(N\), which I felt had not been covered clearly. His response: &ldquo;That&rsquo;s
what I love about \(N\): some students get it, some don&rsquo;t - so we get beautiful
bell shaped curves in the exam.&ldquo;]</p>

<p>Of course the extreme suggestion of abolishing all exams will not go down well:
&ldquo;What about standards?&rdquo; &ldquo;How can we get funding if we do not&rdquo; &ldquo;How do we award
degrees that people will respect?&rdquo; Traditionalists might say that students
appreciate exams for the targets and feedback. Well, there&rsquo;s nothing to stop us
giving students targets or feedback. We can provide events just like exams, if
students want them - self-administered tests, for example, would allow students
to check how well they have assimilated all the material in a course. Other
systems of targets and feedback that students enjoy include project work,
problem-based learning, and portfolio-based assessment.</p>

<p>As a compromise, let&rsquo;s modify our proposal a little: <strong>Camwick should become a place where the only achievable grade is an A.</strong> I&rsquo;m not recommending that we
simply give everyone an A. It&rsquo;s a crime to let standards slip. When I say
everyone should get an A, I mean that everyone should be allowed to get <em>to</em> an
A.</p>

<p>Think back to Alice, Bob, and Charlie. Alice grasped most of the material in the
course and achieved an A. Given a little more time and little less stress, Bob
and Charlie could probably have grasped it all too, and become equally strong
masters of the material. What good does it do Bob and Charlie to record the fact
that they were a little slower than alice? Wouldn&rsquo;t it have been better,
educationally, to give Bob and Charlie a little more time and help, so that they
achieved the same A standard?</p>

<p>Does a bus-driver-training school <em>rank</em> its graduating drivers? No, it ensures
that all attain the standard required of a bus-driver. Would you like to be
treated by a C-grade doctor? No, everyone wants an A-grade doctor! So doctors
and drivers are (I hope!) trained and trained and not let out until they are
A-grade in standard. Why should other professions be treated differently?</p>

<p>Figure 1a shows the command of the material of each student as a function of
time in the traditional system. A traditional exam interrupts the learning
process, and Bob and Charlie are recorded as having achieved a lower standard.
Figure 1b shows the same students in an exam-free system, assuming they learn at
the same rate as in the old system. Each student takes a different time to
achieve full command of the course material. Every student has the satisfaction
of achieving full command of the material.</p>


    <figure class="left" >
        <img src="/macKayAs/fig2.jpg"   />

        
            <figcaption class="center" >Figure 2: Everyone can get an A, regardless of learning rate, if their education is not halted by exams. Traditional system on the left, with an educational system on the right.</figcaption>
        
    </figure>



<p>The difference between the two systems is also striking if we assume that
students start the course at different levels of ability. In Figure 2, albert
comes from a privileged background and already knows half the course material
when he arrives. Brenda and Catharine arrive at a lower educational level.
Brenda and Catharine are actually faster learners than Albert, but, as Figure 2a
shows, the traditional exam system rewards Albert with the A grade
(&lsquo;congratulations, you started first!&rsquo;), and brands Brenda and Catharine
failures. In the &lsquo;Only A-grades&rsquo; system, everyone attains an A-grade in due
course; and Albert isn&rsquo;t actually first to finish.</p>

<p>The information about &lsquo;who finished when&rsquo; could in principle be retained in
order to provide some sort of student-ranking service to employers, but I would
strongly urge the destruction of all such records. Only the achieving of an A
grade should be recorded, nothing else. Why?</p>

<ol>
<li>Because being ranked creates stress.</li>
<li>Because students who are competing with each other for ranks may be reluctant
to help each other learn. In contrast, in the &lsquo;Only A-grades&rsquo; system, the top
students lose nothing if they help their peers; indeed, they may gain in
several ways: peer-teaching strengthens the students&rsquo; grasp on material, and
often speeds up the whole class.</li>
<li>Evidence that a student is a quick learner may well make itself evident in
her transcript without rankings being made: Alice, covering material quickly,
will have time to take extra courses. So in one year she&rsquo;ll accumulate a
slightly fatter sheaf of A-grade qualifications.</li>
<li>What value are rankings? If future employers want students to be formally
evaluated, they can pay for an evaluation service. Why ruin a great
institution? The very best students might like grades too, as they enjoy
being congratulated. But the &lsquo;only A-grades&rsquo; system will congratualte them
too.</li>
</ol>

<p>These ideas are not new, nor are they unprecedented. In many German
Universities, first- and second-year courses have no grades, no obligatory
coursework, and no obligatory exams. End-of-course exams are provided only as a
service to students, to help them find out if they have indeed grasped the
material and are ready progress to the next stage.</p>

<p>In practice, how should we organize courses so that everyone reaches 100%
mastery? For Bob and Charlie&rsquo;s benefit, the average pace probably has to be
reduced. Figure 3 shows one way of organizing the material in stages, so that a
class is kept together. Whenever Alice has completed the material in a stage,
she can spend time on other interests, or can help other members of the class.</p>


    <figure class="left" >
        <img src="/macKayAs/figs_ABC21.jpg"   />

        
            <figcaption class="center" >Figure 3: Possible course plan. This scheme assumes that the students have rates of progress ranging from A (fastest) to C (slowest). Every two weeks, a consolidation period is inserted to ensure that C has assimilated all the learning objectives. Alice can use the consolidation period to pursue others interests or act as a peer-teacher.</figcaption>
        
    </figure>



<p>Camwick staff who say &ldquo;we can&rsquo;t possibly cover a full degree course if we reduce
the pace!&rdquo; should bear in mind that, had Bob and charlie gone to a less
prestigious University, they probably would have got first-class degrees. How
can this paradox - going slowing and arriving at almost the same time - be
explained? I suspect an important factor is this: struggling students get ever
slower if we pile on new material before they have assimilated the old. For
example, 2ⁿᵈ-year Lagrangian dynamics is difficult to absorb if one hasn&rsquo;t
grasped 1ˢᵗ-year Newtonian dynamics. So the steady linear progress assumed in
Figures 1 to 3 is a poor model of Carlie. The more Charlie is left behind, the
slower he learns. This means that the true difference in pace between Alice and
Charlie need not be very big. If Charlie gets lost and left behind, we are
wasting everyone&rsquo;s time by having him sit in classes where new material is
presented. A stitch in time saves nine (Figure 4).</p>


    <figure class="left" >
        <img src="/macKayAs/figs_ABC31.jpg"   />

        
            <figcaption class="center" >Figure 4: A stitch in time saves nine. Curve C shows Charlie&#39;s progress in a course taught at the pace that is ideal for Alice. The more Charlie is left behind, the slower he learns. By the end of the course, there is a big gap between A and C. Curve C shows Charlie&#39;s progress in a course taught at the pace that is ideal for him. Just a small decrease in class pace allows the big gap between Alice and Charlie to be eliminated.</figcaption>
        
    </figure>



<p>Teaching methods must be modified to ensure that everyone in the class benefits.
I advocate interactive teaching: students are asked questions and encouraged to
ask questions and to be active participants in their own learning. It&rsquo;s not
enough to ask a question and let one person in the class (Alice!) answer it. The
whole class must have the chance to think, puzzle and discuss; the teacher must
ascertain the level of understanding of the whole class. In large classes, I
find Mazur&rsquo;s voting method works well: a lecture is centered on two or three
carefully chosen questions with multiple-choice answers. Students discuss a
question with their neighbors, then all vote. The vote informs the lecturer
whether previous material has been understood. Diversity of votes can seed a
useful discussion.</p>

<p>To conclude, here are a few further advantages of the educational approach
advocated here:</p>

<ul>
<li>Happy, curious, and self-motivated students are fun to teach.</li>
<li>At present, British students have little choice of university teaching and
assessment style: all universities give out grades. Shouldn&rsquo;t we offer them a
choice? Some students would like the chance to go to a place with high
standards where only A-grades are awarded.</li>
<li>If some universities adopt student-centered educational policies and stop
ranking students, perhaps these attitudes will spread to schools, with
consequent benefits to pupils, and in due course, to universities. Dumbed-down
A levels could be replaced by educational programmes that ensure that everyone
attains their maximum potential and feels happy about it.</li>
<li>Happy graduates who get A grades are likely to become grateful alumni donors.</li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1">Also known for the fabulous free book called <a href="http://www.inference.org.uk/mackay/itila/" target="_blank">Information Theory, Inference, and Learning Algorithms</a>
 <a class="footnote-return" href="#fnref:fn-1"><sup>[return]</sup></a></li>
<li id="fn:fn-2">If you have good cause why this should not be distributed here in this manner, please contact me and I will do the needful
 <a class="footnote-return" href="#fnref:fn-2"><sup>[return]</sup></a></li>
</ol>
</div>
]]></content>
        </item>
        
        <item>
            <title>D3 for Git</title>
            <link>https://rgoswami.me/posts/d3git/</link>
            <pubDate>Mon, 16 Mar 2020 00:17:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/d3git/</guid>
            <description>Background  I have had a lot of discussions regarding the teaching of git This is mostly as a part of the SoftwareCarpentries, or in view of my involvement with univ.ai, or simply in every public space I am associated with Without getting into my views, I just wanted to keep this resource in mind  The site  Learning git is a highly contentious thing People seem to be fond of GUI tools, especially since on non *nix systems, it seems that there is a lot of debate surrounding obtaining the git utility in the first place  One of the best ways of understanding (without installing stuff) the mental models required for working with git is this site</description>
            <content type="html"><![CDATA[

<h2 id="background">Background</h2>

<ul>
<li>I have had a lot of discussions regarding the teaching of <code>git</code></li>
<li>This is mostly as a part of <a href="https://static.carpentries.org/maintainers/#HaoZeke" target="_blank">the SoftwareCarpentries</a>, or in view of my
<a href="https://www.univ.ai/teams/rohit-goswami" target="_blank">involvement with univ.ai</a>, or simply in every public space I am associated with</li>
<li>Without getting into my views, I just wanted to keep this resource in mind</li>
</ul>

<h2 id="the-site">The site</h2>

<ul>
<li>Learning <code>git</code> is a highly contentious thing</li>
<li>People seem to be fond of GUI tools, especially since on non *nix systems, it
seems that there is a lot of debate surrounding obtaining the <code>git</code> utility in
the first place</li>
</ul>

<p>One of the best ways of understanding (without installing stuff) the mental
models required for working with <code>git</code> is <a href="https://onlywei.github.io/explain-git-with-d3/#checkout" target="_blank">this site</a></p>


    <figure class="left" >
        <img src="/ox-hugo/d3git.png"   />

        
            <figcaption class="center" >Figure 1: A screenshot of the site</figcaption>
        
    </figure>



<ul>
<li><p>However, as is clear, this is not exactly a replacement for a good old command-line.</p></li>

<li><p>It does make for a good resource for teaching with slides, or for generating
other static visualizations, where live coding is not an option</p></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Shorter Posts</title>
            <link>https://rgoswami.me/posts/shortpost/</link>
            <pubDate>Mon, 16 Mar 2020 00:16:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/shortpost/</guid>
            <description> Background Sometime this year, I realized that I no longer have access to a lot of my older communication. This included, a lot of resources I enjoyed and shared with the people who were around me at that point in time. To counter this, I have decided to opt for shorter posts, even if they don&amp;rsquo;t always include the same level of detail I would prefer to provide.
Alternatives  I have an automated system based around IFTTT combined with Twitter, Diigo, and even Pocket However, that doesn&amp;rsquo;t really tell me much, and trawling through a massive glut of data is often pointless as well There&amp;rsquo;s always Twitter, but I don&amp;rsquo;t really care to hear the views of others when I want to revisit my own ideas  Conclusions  I will be making shorter posts here, like the random one on octobox  </description>
            <content type="html"><![CDATA[

<h2 id="background">Background</h2>

<p>Sometime this year, I realized that I no longer have access to a lot of my older
communication. This included, a lot of resources I enjoyed and shared with the
people who were around me at that point in time. To counter this, I have decided
to opt for shorter posts, even if they don&rsquo;t always include the same level of
detail I would prefer to provide.</p>

<h3 id="alternatives">Alternatives</h3>

<ul>
<li>I have an automated system based around IFTTT combined with Twitter, Diigo,
and even Pocket</li>
<li>However, that doesn&rsquo;t really tell me much, and trawling through a massive glut
of data is often pointless as well</li>
<li>There&rsquo;s always Twitter, but I don&rsquo;t really care to hear the views of others
when I want to revisit my own ideas</li>
</ul>

<h2 id="conclusions">Conclusions</h2>

<ul>
<li>I will be making shorter posts here, like the random one on <a href="https://rgoswami.me/posts/ghnotif/" target="_blank">octobox</a></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Provisioning Dotfiles on an HPC</title>
            <link>https://rgoswami.me/posts/prov-dots/</link>
            <pubDate>Mon, 16 Mar 2020 00:06:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/prov-dots/</guid>
            <description>Background My dotfiles turned 4 years old a few months ago (since 9th Jan 2017) and remains one of my most frequently updated projects for obvious reasons. Going through the changes reminds me of a whole of posts I never got around to writing.
Anyway, recently I gained access to another HPC cluster, with a standard configuration (bash, old CentOS) and decided to track my provisioning steps. This is really a very streamlined experience by now, since I&amp;rsquo;ve used the same setup across scores of machines.</description>
            <content type="html"><![CDATA[

<h2 id="background">Background</h2>

<p><a href="https://github.com/HaoZeke/Dotfiles" target="_blank">My dotfiles</a> turned 4 years old a few months ago (since 9th Jan 2017) and remains one of my most
frequently updated projects for obvious reasons. Going through the changes
reminds me of a whole of posts I never got around to writing.</p>

<p>Anyway, recently I gained access to another HPC cluster, with a standard configuration
(bash, old CentOS) and decided to track my provisioning steps. This is really a
very streamlined experience by now, since I&rsquo;ve used the same setup across scores
of machines. This is actually also a generic intro to configuring user setups on
HPC (high performance cluster) machines, if one is inclined to read it in that
manner. To that end, sections of this post involve restrictions relating to user
privileges which aren&rsquo;t normally part of most Dotfile setups.</p>

<h3 id="aside">Aside</h3>

<ul>
<li>Dotfiles define most people who maintain them</li>
<li>No two sets are ever exactly alike</li>
<li>They fall somewhere between winging it for each machine and using something
like <a href="https://www.habitat.sh/learn/" target="_blank">Chef</a> or <a href="https://www.ansible.com/" target="_blank">Ansible</a></li>
<li>Tracking dotfiles is really close to having a sort of out-of-context journal</li>
</ul>

<p>Before I settled on using <a href="https://github.com/kobus-v-schoor/dotgit" target="_blank">the fabulous dotgit</a>, I considered several
alternatives, most notably <a href="https://www.gnu.org/software/stow/" target="_blank">GNU stow</a>.</p>

<h2 id="preliminaries">Preliminaries</h2>

<p>It is important to note the environment into which I had to get my
setup.</p>

<h3 id="ssh-setup">SSH Setup</h3>

<ul>
<li>The very first thing to do is to use a new <code>ssh-key</code></li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export myKey<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;someName&#34;</span>
ssh-keygen -f $HOME/.ssh/$myKey
<span style="color:#75715e"># I normally don&#39;t set a password</span>
ssh-add $HOME/.ssh/$myKey
ssh-copy-id $myHPC
<span style="color:#75715e"># myHPC being an IP address</span></code></pre></div>
<p>I more often than not tend to back this up with a cutesy alias, also because I
do not always get my username of choice on these machines. So in
<code>$HOME/.ssh/config</code> I use:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-conf" data-lang="conf">Host myHPC
 Hostname 127.0.0.1
 User somethingIgot
 IdentityFile ~/.ssh/myKey</code></pre></div>
<h3 id="harvesting-information">Harvesting Information</h3>

<ul>
<li>I normally use <a href="https://github.com/dylanaraps/neofetch" target="_blank">neofetch</a> on new machines</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p $HOME/Git/Github
cd $HOME/Git/Github
git clone https://github.com/dylanaraps/neofetch.git
cd neofetch
./neofetch</code></pre></div>

    <figure class="left" >
        <img src="/ox-hugo/sampleHPC.png"   />

        
            <figcaption class="center" >Figure 1: Neofetch Output</figcaption>
        
    </figure>



<p>Where the top has been tastefully truncated. Just for context, the latest <code>bash</code>
as of this writing is <code>v5.0.16</code> so, that&rsquo;s not too bad, given that <code>neofetch</code>
works for <code>bash</code> ≥ 3.2</p>

<h2 id="circumventing-user-restrictions-with-nix"><span class="org-todo todo TODO">TODO</span> Circumventing User Restrictions with Nix</h2>

<ul>
<li>A post in and of itself would be required to explain why and how users are
normally restricted from activities in cluster nodes</li>
<li>Here, we leverage the <a href="https://nixos.org/nix/manual/#chap-installation" target="_blank">nix-package management system</a> to circumvent these</li>
<li>User installation of <code>nix</code> is sadly non-trivial, so this might be of some use&nbsp;<sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup></li>
</ul>

<h3 id="testing-nix-user-chroot">Testing nix-user-chroot</h3>

<ol>
<li>We will first check namespace support</li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Errored out</span>
unshare --user --pid echo YES
<span style="color:#75715e"># Worked!</span>
zgrep CONFIG_USER_NS /boot/config-<span style="color:#66d9ef">$(</span>uname -r<span style="color:#66d9ef">)</span>
<span style="color:#75715e"># CONFIG_USER_NS=y</span></code></pre></div>
<p>Thankfully we have support for namespaces, so we can continue with <code>nix-user-chroot</code>.</p>

<ol>
<li>Since we definitely do not have <code>rustup</code> or <code>rustc</code> on the HPC, we will use <a href="https://github.com/nix-community/nix-user-chroot/releases" target="_blank">a
prebuilt binary</a> of <code>nix-user-chroot</code></li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cd $HOME <span style="color:#f92672">&amp;&amp;</span> wget -O nix-user-chroot  https://github.com/nix-community/nix-user-chroot/releases/download/1.0.2/nix-user-chroot-bin-1.0.2-x86_64-unknown-linux-musl</code></pre></div>
<ol>
<li>Similar to <a href="https://nixos.wiki/wiki/Nix%5FInstallation%5FGuide#Installing%5Fwithout%5Froot%5Fpermissions" target="_blank">the wiki example</a>, we will use <code>$HOME/.nix</code></li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cd ~/
chmod +x nix-user-chroot
mkdir -m <span style="color:#ae81ff">0755</span> ~/.nix
./nix-user-chroot ~/.nix bash -c <span style="color:#e6db74">&#39;curl https://nixos.org/nix/install | sh&#39;</span></code></pre></div>
<ul>
<li>Only, this <strong>doesn&rsquo;t work</strong></li>
</ul>

<p>Turns out that since <code>unshare</code> is too old, <code>nix-user-chroot</code> won&rsquo;t work either.</p>

<h3 id="using-proot">Using PRoot</h3>

<p>PRoot is pretty neat in general, they even have a <a href="https://proot-me.github.io/" target="_blank">nice website describing it</a>.</p>

<ol>
<li>Set a folder up for local installations (this is normally done by my
Dotfiles, but we might as well have one here too)</li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p $HOME/.local/bin
export PATH<span style="color:#f92672">=</span>$PATH:$HOME/.local/bin</code></pre></div>
<ol>
<li>Get a binary from the <a href="https://gitlab.com/proot/proot/-/jobs" target="_blank">GitLab artifacts</a></li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cd $HOME
mkdir tmp
cd tmp
wget -O artifacts.zip https://gitlab.com/proot/proot/-/jobs/452350181/artifacts/download
unzip artifacts.zip
mv dist/proot $HOME/.local/bin</code></pre></div>
<ol>
<li>Bind and install <code>nix</code></li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir ~/.nix
export PROOT_NO_SECCOMP<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
proot -b ~/.nix:/nix
export PROOT_NO_SECCOMP<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
curl https://nixos.org/nix/install | sh</code></pre></div>
<p>If you&rsquo;re very unlucky, like I was, you may be greeted by a lovely little error
message along the lines of:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">/nix/store/ddmmzn4ggz1f66lwxjy64n89864yj9w9-nix-2.3.3/bin/nix-store: /opt/ohpc/pub/compiler/gcc/5.4.0/lib64/libstdc++.so.6: version `GLIBCXX_3.4.22&#39; not found (required by /nix/store/c0b76xh2za9r9r4b0g3iv4x2lkw1zzcn-aws-sdk-cpp-1.7.90/lib/libaws-cpp-sdk-core.so)</code></pre></div>
<p>Which basically is as bad as it sounds. At this stage, we need a newer compiler
to even get <code>nix</code> up and running, but can&rsquo;t without getting an OS update. This
chicken and egg situation calls for the drastic measure of leveraging <code>brew</code>
first<sup class="footnote-ref" id="fnref:fn-2"><a href="#fn:fn-2">2</a></sup>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sh -c <span style="color:#e6db74">&#34;</span><span style="color:#66d9ef">$(</span>curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh<span style="color:#66d9ef">)</span><span style="color:#e6db74">&#34;</span></code></pre></div>
<p>Note that nothing in this section suggests the best way is not to lobby your
sys-admin to install <code>nix</code> system-wide in multi-user mode.</p>

<h2 id="giving-up-with-linuxbrew">Giving Up with Linuxbrew</h2>

<ul>
<li>Somewhere around this point, <a href="https://docs.brew.sh/Homebrew-on-Linux" target="_blank">linuxbrew</a> is a good idea</li>
<li>More on this later</li>
</ul>

<h2 id="shell-stuff">Shell Stuff</h2>

<p><code>zsh</code> is my shell of choice, and is what my <code>Dotfiles</code> expect and work best with.</p>

<ul>
<li>I did end up making a quick change to update the <code>dotfiles</code> with a target
which includes a snippet to transition to <code>zsh</code> from the default <code>bash</code> shell</li>
</ul>

<h2 id="dotfiles">Dotfiles</h2>

<p>The actual installation steps basically tracks <a href="https://github.com/HaoZeke/Dotfiles" target="_blank">the readme instructions</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone https://github.com/kobus-v-schoor/dotgit.git
mkdir -p ~/.bin
cp -r dotgit/bin/dotgit* ~/.bin
cat dotgit/bin/bash_completion &gt;&gt; ~/.bash_completion
rm -rf dotgit
<span style="color:#75715e"># echo &#39;export PATH=&#34;$PATH:$HOME/.bin&#34;&#39; &gt;&gt; ~/.bashrc</span>
echo <span style="color:#e6db74">&#39;export PATH=&#34;$PATH:$HOME/.bin&#34;&#39;</span> &gt;&gt; ~/.zshrc</code></pre></div><div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1">Much of this section is directly adapted from <a href="https://nixos.wiki/wiki/Nix%5FInstallation%5FGuide#Installing%5Fwithout%5Froot%5Fpermissions" target="_blank">the NixOS wiki</a>
 <a class="footnote-return" href="#fnref:fn-1"><sup>[return]</sup></a></li>
<li id="fn:fn-2">This used to be called linuxbrew, but the <a href="https://docs.brew.sh/Homebrew-on-Linux" target="_blank">new site</a> makes it clear that it&rsquo;s all one <code>brew</code> now.
 <a class="footnote-return" href="#fnref:fn-2"><sup>[return]</sup></a></li>
</ol>
</div>
]]></content>
        </item>
        
        <item>
            <title>Switching to Colemak</title>
            <link>https://rgoswami.me/posts/colemak-switch/</link>
            <pubDate>Sat, 29 Feb 2020 14:06:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/colemak-switch/</guid>
            <description>Background I just realized that it has been over two years since I switched from QWERTY to COLEMAK but somehow never managed to write about it. It was a major change in my life, and it took forever to get acclimatized to. I do not think I&amp;rsquo;ll ever again be in a position to make such a change in my life again, but it was definitely worth it.
Touch Typing My interest in touch typing in I decided to digitize my notes for posterity, during the last two years of my undergraduate studies back in Harcourt Butler Technical Institute (HBTI) Kanpur, India.</description>
            <content type="html"><![CDATA[

<h2 id="background">Background</h2>

<p>I just realized that it has been over two years since I switched from QWERTY to
COLEMAK but somehow never managed to write about it. It was a major change in my
life, and it took forever to get acclimatized to. I do not think I&rsquo;ll ever again be
in a position to make such a change in my life again, but it was definitely
worth it.</p>

<h2 id="touch-typing">Touch Typing</h2>

<p>My interest in touch typing in I decided to digitize my notes for posterity, during the
last two years of my undergraduate studies back in Harcourt Butler Technical
Institute (HBTI) Kanpur, India. in one of my many instances of yak shaving, I
realized I could probably consume and annotate a lot more content by typing
faster. Given that at that stage I was already a fast talker, it seemed like a
natural extension. There was probably an element of nostalgia involved as well.
That and the end of a bachelors involves the thesis, which generally involves a
lot of typing.</p>

<p>There were (and are) some fantastic resources for learning to touch type
nowadays, I personally used:</p>

<dl>
<dt><a href="https://www.typing.com/" target="_blank">Typing.com</a></dt>
<dd>This is short, but a pretty good basic setup. The numbering and
special characters are a bit much to take in at the level of practice you get
by completing all the previous exercises, but eventually they make for a good workout.</dd>
<dt><a href="https://www.typingclub.com/en-gb/login/" target="_blank">TypingClub</a></dt>
<dd>This is what I ended up working my way through. It is
comprehensive, beautiful, and fun.</dd>
</dl>

<p>Also, later, I ended up using <a href="https://www.keybr.com/" target="_blank">keybr</a> a lot, simply because typing gibberish is a
good way of practicing, and it is independent of the keyboard layout.</p>

<p>Just to foreshadow things, the enemy facing me at this point was the layout
itself<sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup>.</p>


    <figure class="left" >
        <img src="https://www.keyboard-design.com/kb-images/qwerty-kla.jpg"   />

        
    </figure>



<h2 id="alternate-layouts">Alternate layouts</h2>

<p>Having finally broken into the giddy regimes of 150+ wpm, I was ecstatic, and
decided to start working my way through some longer reports. However, I quickly
realized I was unable to type for more than a couple of minutes without getting
terribly cramped. Once it got to the point of having to visit a physiotherapist,
I had to call it quits. At that stage, relearning the entire touch typing
corpus, given that I already was used to QWERTY, seemed pretty bleak.</p>

<p>It took forever, and I ended up applying my choices to my phone keyboard as
well, which presumably helped me in terms of increasing familiarity, had the
unintended effect of making me seem distant to people I was close to, since my
verbose texts suddenly devolved to painful one-liners.</p>

<p>The alternative layouts I tried were:</p>

<dl>
<dt><a href="https://www.dvorak-keyboard.com/" target="_blank">DVORAK</a></dt>
<dd>At the time, TypingClub only supported QWERTY and DVORAK, so it was
pretty natural for me to try it out. There are also some <a href="https://www.dvzine.org/" target="_blank">very nice comics
about it</a>. I remember that it was pretty neat, with
a good even distribution, until I tried coding. The placement of the
semicolons make it impossible to use while programming. I would still say it
makes for a comfortable layout, as long as special characters are not required.</dd>
</dl>


    <figure class="left" >
        <img src="https://www.keyboard-design.com/kb-images/dvorak-kla.jpg"   />

        
    </figure>



<dl>
<dt><a href="http://mkweb.bcgsc.ca/carpalx" target="_blank">CarpalX</a></dt>
<dd>I experimented with the entire carpalx family, but I was unable to get
used to it. I liked QFMLWY best. I do recommend reading the training methodology, especially if
anyone is interested in numerical optimization in general. More importantly,
though it was relatively easy to set up on my devices and operating systems,
the fact that it wasn&rsquo;t natively supported meant a lot of grief whenever I
inevitably had to use a public computer.</dd>
</dl>


    <figure class="left" >
        <img src="https://www.keyboard-design.com/kb-images/qgmlwy-kla.jpg"   />

        
    </figure>



<dl>
<dt>Colemak</dt>
<dd>Eventually I decided to go with <a href="https://colemak.com/" target="_blank">Colemak</a>, especially since it is
widely available. Nothing is easier than <code>setxkbmap us -variant colemak -option grp:alt_shift_toggle</code> on public machines and it&rsquo;s easy on Windows as
well. Colemak seems like a good compromise. I personally have not been able to
reach the same speeds I managed with QWERTY, even after a year, but then
again, I can be a lot more consistent, and it hurts less. Nowadays, Colemak
has made its way onto most typing sites as well, including TypingClub</dd>
</dl>


    <figure class="left" >
        <img src="https://www.keyboard-design.com/kb-images/colemak-kla.jpg"   />

        
    </figure>



<h3 id="what-about-vim">What about VIM?</h3>

<ul>
<li>DVORAK makes it impossible, so do most other layouts, but there are some
tutorials purporting to help use vim movement with DVORAK</li>
<li>Colemak isn&rsquo;t any better, but the fact of the matter is that once you know VIM
on QWERTY, and have separately internalized colemak or something else, hitting
keys is just hitting keys</li>
</ul>

<p>All that said, I still occasionally simply remap HJKL (QWERTY movement) to HNEI
(Colemak analog) when it is feasible.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Changing layouts was a real struggle. Watching my WPM drop back to lower than
hunt and peck styles was pretty humiliating, especially since the reports kept
coming in, and more than once I switched to QWERTY. However, since then, I have
managed to stay on course. I guess if I think about it, it boils down to a few
scattered thoughts:</p>

<ul>
<li>Typing is kinda like running a marathon, knowing how it is done and doing it
are two different things</li>
<li>Tell <strong>everyone</strong>, so people can listen to you lament your reduced speed and not
hate you for replying slowly</li>
<li>Practice everyday, because, well, it works out in the long run, even when you
plateau</li>
<li>Alternate shifts! That&rsquo;s really something which should show up more in
tutorials, especially for listicles, not changing the shifts will really hurt</li>
<li>Try and get a mechanical keyboard (like the <a href="https://www.annepro.net/" target="_blank">Anne Pro 2</a> or the <a href="https://www.coolermaster.com/catalog/peripheral/keyboards/masterkeys-pro-l-white/" target="_blank">Coolermaster Masterkeys</a>), they&rsquo;re fun and easy to change layouts on</li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1">The images are <a href="https://www.keyboard-design.com/best-keyboard-layouts.html" target="_blank">from here</a>, where there&rsquo;s also an effort based metric used to score keyboard layouts.
 <a class="footnote-return" href="#fnref:fn-1"><sup>[return]</sup></a></li>
</ol>
</div>
]]></content>
        </item>
        
        <item>
            <title>Bojack Horseman</title>
            <link>https://rgoswami.me/posts/bojack-horseman/</link>
            <pubDate>Thu, 27 Feb 2020 22:28:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/bojack-horseman/</guid>
            <description>Background For a while I was worried about writing about a TV show here. I thought it might be frivolous, or worse, might outweigh the other kinds of articles I would like to write. However, like most things, that which is ignored just grows, so it is easier to just write and forget about it.
The Show Much has been said about how Bojack Horseman is one of the best shows ever, and they&amp;rsquo;re all correct.</description>
            <content type="html"><![CDATA[

<h2 id="background">Background</h2>

<p>For a while I was worried about writing about a TV show here. I thought it might
be frivolous, or worse, might outweigh the other kinds of articles I would like
to write. However, like most things, that which is ignored just grows, so it is
easier to just write and forget about it.</p>

<h2 id="the-show">The Show</h2>

<p>Much has been said about how Bojack Horseman is one of the best shows ever, and
they&rsquo;re all correct. For that matter I won&rsquo;t be going into the details of how
every episode ties together a tapestry of lives in a meaningful way, or any of
that. The show was amazingly poignant. The characters felt real. Which actually
leads me to the real issue.</p>

<h2 id="the-end">The End</h2>

<p>The end of Bojack was <strong>good</strong>. It was the way it was meant to be. For a
slice-of-life show, it is a natural conclusion. It isn&rsquo;t necessary that any
catharsis occurs or that the characters change or become better or all that
jazz. It isn&rsquo;t about giving the viewers closure. It is simply about a window
onto the lives of (fictional) characters being shut. To that end, I disliked
attempts to bring closure in the show itself.</p>

<p>One of the main reasons why I felt strongly enough to write this, is simply
because when I looked around, the prevailing opinion was that the main character
should have been killed off, <span class="underline">for his sins</span>. This strikes me as a very flippant
attitude to take. It reeks of people trying to make the show a cautionary tale,
which is frankly speaking a weird approach to take towards any fictional story.
The idea that the character should be redeemed also seemed equally weak, for
much the same reasons.</p>

<p>The fact that the characters are hypocrites, and that none of them are as good
or bad as they make themselves out to be is one of the best parts of the show.</p>

<h2 id="conclusion">Conclusion</h2>

<p>That&rsquo;s actually all I have to say about this. I thought of adding relevant memes
or listing episodes or name dropping sites, but this isn&rsquo;t buzzfeed. The show is
incredible, and there are far better ways of proving that. Bust out your
favorite search engine + streaming content provider / digital piracy eye-patch
and give it a whirl. The only thing I&rsquo;d suggest is watching everything in order,
it&rsquo;s just that kind of show.</p>
]]></content>
        </item>
        
        <item>
            <title>ISLR :: Moving Beyond Linearity</title>
            <link>https://rgoswami.me/posts/islr-ch7/</link>
            <pubDate>Wed, 19 Feb 2020 09:47:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/islr-ch7/</guid>
            <description>Chapter VII - Moving Beyond Linearity All the questions are as per the ISL seventh printing.
Common libsUsed&amp;lt;-c(&amp;#34;dplyr&amp;#34;,&amp;#34;ggplot2&amp;#34;,&amp;#34;tidyverse&amp;#34;, &amp;#34;ISLR&amp;#34;,&amp;#34;caret&amp;#34;,&amp;#34;MASS&amp;#34;, &amp;#34;gridExtra&amp;#34;, &amp;#34;pls&amp;#34;,&amp;#34;latex2exp&amp;#34;,&amp;#34;data.table&amp;#34;) invisible(lapply(libsUsed, library, character.only = TRUE))## ## Attaching package: &amp;#39;dplyr&amp;#39;## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## filter, lag## The following objects are masked from &amp;#39;package:base&amp;#39;: ## ## intersect, setdiff, setequal, union## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──## ✔ tibble 2.1.3 ✔ purrr 0.3.3 ## ✔ tidyr 1.</description>
            <content type="html"><![CDATA[

<h2 id="chapter-vii---linear-model-selection-and-regularization">Chapter VII - Moving Beyond Linearity</h2>

<p>All the questions are as per the
<a href="https://faculty.marshall.usc.edu/gareth-james/ISL/" target="_blank">ISL seventh
printing</a>.</p>

<h3 id="common">Common</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">libsUsed<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">c</span>(<span style="color:#e6db74">&#34;dplyr&#34;</span>,<span style="color:#e6db74">&#34;ggplot2&#34;</span>,<span style="color:#e6db74">&#34;tidyverse&#34;</span>,
            <span style="color:#e6db74">&#34;ISLR&#34;</span>,<span style="color:#e6db74">&#34;caret&#34;</span>,<span style="color:#e6db74">&#34;MASS&#34;</span>, <span style="color:#e6db74">&#34;gridExtra&#34;</span>,
            <span style="color:#e6db74">&#34;pls&#34;</span>,<span style="color:#e6db74">&#34;latex2exp&#34;</span>,<span style="color:#e6db74">&#34;data.table&#34;</span>)
<span style="color:#a6e22e">invisible</span>(<span style="color:#a6e22e">lapply</span>(libsUsed, library, character.only <span style="color:#f92672">=</span> <span style="color:#66d9ef">TRUE</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;dplyr&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:stats&#39;:
##
##     filter, lag</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:base&#39;:
##
##     intersect, setdiff, setequal, union</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## ✔ tibble  2.1.3     ✔ purrr   0.3.3
## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.4.0</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Loading required package: lattice</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;caret&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:purrr&#39;:
##
##     lift</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;MASS&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:dplyr&#39;:
##
##     select</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;gridExtra&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:dplyr&#39;:
##
##     combine</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;pls&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:caret&#39;:
##
##     R2</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:stats&#39;:
##
##     loadings</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;data.table&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:purrr&#39;:
##
##     transpose</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:dplyr&#39;:
##
##     between, first, last</code></pre></div>
<h2 id="question-7.6---page-299">Question 7.6 - Page 299</h2>

<p>In this exercise, you will further analyze the <code>Wage</code> data set
considered throughout this chapter.</p>

<p><strong>(a)</strong> Perform polynomial regression to predict <code>wage</code> using <code>age</code>. Use
cross-validation to select the optimal degree <em>d</em> for the polynomial.
What degree was chosen, and how does this compare to the results of
hypothesis testing using ANOVA? Make a plot of the resulting polynomial
fit to the data.</p>

<p><strong>(b)</strong> Fit a step function to predict <code>wage</code> using <code>age</code>, and perform
cross-validation to choose the optimal number of cuts. Make a plot of
the fit obtained. In this exercise, we will generate simulated data, and
will then use this data to perform best subset selection.</p>

<h3 id="answer">Answer</h3>

<p>Lets get the data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">1984</span>)
wageDat<span style="color:#f92672">&lt;-</span>ISLR<span style="color:#f92672">::</span>Wage
wageDat <span style="color:#f92672">%&gt;%</span> str <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    3000 obs. of  11 variables:
##  $ year      : int  2006 2004 2003 2003 2005 2008 2009 2008 2006 2004 ...
##  $ age       : int  18 24 45 43 50 54 44 30 41 52 ...
##  $ maritl    : Factor w/ 5 levels &#34;1. Never Married&#34;,..: 1 1 2 2 4 2 2 1 1 2 ...
##  $ race      : Factor w/ 4 levels &#34;1. White&#34;,&#34;2. Black&#34;,..: 1 1 1 3 1 1 4 3 2 1 ...
##  $ education : Factor w/ 5 levels &#34;1. &lt; HS Grad&#34;,..: 1 4 3 4 2 4 3 3 3 2 ...
##  $ region    : Factor w/ 9 levels &#34;1. New England&#34;,..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ jobclass  : Factor w/ 2 levels &#34;1. Industrial&#34;,..: 1 2 1 2 2 2 1 2 2 2 ...
##  $ health    : Factor w/ 2 levels &#34;1. &lt;=Good&#34;,&#34;2. &gt;=Very Good&#34;: 1 2 1 2 1 2 2 1 2 2 ...
##  $ health_ins: Factor w/ 2 levels &#34;1. Yes&#34;,&#34;2. No&#34;: 2 2 1 1 1 1 1 1 1 1 ...
##  $ logwage   : num  4.32 4.26 4.88 5.04 4.32 ...
##  $ wage      : num  75 70.5 131 154.7 75 ...
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">wageDat <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       year           age                     maritl           race
##  Min.   :2003   Min.   :18.00   1. Never Married: 648   1. White:2480
##  1st Qu.:2004   1st Qu.:33.75   2. Married      :2074   2. Black: 293
##  Median :2006   Median :42.00   3. Widowed      :  19   3. Asian: 190
##  Mean   :2006   Mean   :42.41   4. Divorced     : 204   4. Other:  37
##  3rd Qu.:2008   3rd Qu.:51.00   5. Separated    :  55
##  Max.   :2009   Max.   :80.00
##
##               education                     region               jobclass
##  1. &lt; HS Grad      :268   2. Middle Atlantic   :3000   1. Industrial :1544
##  2. HS Grad        :971   1. New England       :   0   2. Information:1456
##  3. Some College   :650   3. East North Central:   0
##  4. College Grad   :685   4. West North Central:   0
##  5. Advanced Degree:426   5. South Atlantic    :   0
##                           6. East South Central:   0
##                           (Other)              :   0
##             health      health_ins      logwage           wage
##  1. &lt;=Good     : 858   1. Yes:2083   Min.   :3.000   Min.   : 20.09
##  2. &gt;=Very Good:2142   2. No : 917   1st Qu.:4.447   1st Qu.: 85.38
##                                      Median :4.653   Median :104.92
##                                      Mean   :4.654   Mean   :111.70
##                                      3rd Qu.:4.857   3rd Qu.:128.68
##                                      Max.   :5.763   Max.   :318.34
##</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">wageDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(unique) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(length) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       year        age     maritl       race  education     region   jobclass
##          7         61          5          4          5          1          2
##     health health_ins    logwage       wage
##          2          2        508        508</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(boot)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;boot&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:lattice&#39;:
##
##     melanoma</code></pre></div>
<h4 id="a-polynomial-regression">a) Polynomial regression</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">all.deltas <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NA</span>, <span style="color:#ae81ff">10</span>)
<span style="color:#a6e22e">for </span>(i in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>) {
  glm.fit <span style="color:#f92672">=</span> <span style="color:#a6e22e">glm</span>(wage<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(age, i), data<span style="color:#f92672">=</span>Wage)
  all.deltas[i] <span style="color:#f92672">=</span> <span style="color:#a6e22e">cv.glm</span>(Wage, glm.fit, K<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)<span style="color:#f92672">$</span>delta[2]
}
<span style="color:#a6e22e">plot</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>, all.deltas, xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Degree&#34;</span>, ylab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CV error&#34;</span>, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;l&#34;</span>, pch<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, lwd<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, ylim<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">1590</span>, <span style="color:#ae81ff">1700</span>))
min.point <span style="color:#f92672">=</span> <span style="color:#a6e22e">min</span>(all.deltas)
sd.points <span style="color:#f92672">=</span> <span style="color:#a6e22e">sd</span>(all.deltas)
<span style="color:#a6e22e">abline</span>(h<span style="color:#f92672">=</span>min.point <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.2</span> <span style="color:#f92672">*</span> sd.points, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>, lty<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dashed&#34;</span>)
<span style="color:#a6e22e">abline</span>(h<span style="color:#f92672">=</span>min.point <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.2</span> <span style="color:#f92672">*</span> sd.points, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>, lty<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dashed&#34;</span>)
<span style="color:#a6e22e">legend</span>(<span style="color:#e6db74">&#34;topright&#34;</span>, <span style="color:#e6db74">&#34;0.2-standard deviation lines&#34;</span>, lty<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dashed&#34;</span>, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-4-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># ANOVA</span>
fits<span style="color:#f92672">=</span><span style="color:#a6e22e">list</span>()
<span style="color:#a6e22e">for </span>(i in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>) {
  fits[[i]]<span style="color:#f92672">=</span><span style="color:#a6e22e">glm</span>(wage<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(age,i),data<span style="color:#f92672">=</span>wageDat)
}
<span style="color:#a6e22e">anova</span>(fits[[1]],fits[[2]],fits[[3]],fits[[4]],fits[[5]],
  fits[[6]],fits[[7]],fits[[8]],fits[[9]],fits[[10]])</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Analysis of Deviance Table
##
## Model  1: wage ~ poly(age, i)
## Model  2: wage ~ poly(age, i)
## Model  3: wage ~ poly(age, i)
## Model  4: wage ~ poly(age, i)
## Model  5: wage ~ poly(age, i)
## Model  6: wage ~ poly(age, i)
## Model  7: wage ~ poly(age, i)
## Model  8: wage ~ poly(age, i)
## Model  9: wage ~ poly(age, i)
## Model 10: wage ~ poly(age, i)
##    Resid. Df Resid. Dev Df Deviance
## 1       2998    5022216
## 2       2997    4793430  1   228786
## 3       2996    4777674  1    15756
## 4       2995    4771604  1     6070
## 5       2994    4770322  1     1283
## 6       2993    4766389  1     3932
## 7       2992    4763834  1     2555
## 8       2991    4763707  1      127
## 9       2990    4756703  1     7004
## 10      2989    4756701  1        3</code></pre></div>
<ul>
<li>The 4th degree looks the best at the moment</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># 3rd or 4th degrees look best based on ANOVA test</span>
<span style="color:#75715e"># let&#39;s go with 4th degree fit</span>
<span style="color:#a6e22e">plot</span>(wage<span style="color:#f92672">~</span>age, data<span style="color:#f92672">=</span>wageDat, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;darkgrey&#34;</span>)
agelims <span style="color:#f92672">=</span> <span style="color:#a6e22e">range</span>(wageDat<span style="color:#f92672">$</span>age)
age.grid <span style="color:#f92672">=</span> <span style="color:#a6e22e">seq</span>(from<span style="color:#f92672">=</span>agelims[1], to<span style="color:#f92672">=</span>agelims[2])
lm.fit <span style="color:#f92672">=</span> <span style="color:#a6e22e">lm</span>(wage<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(age, <span style="color:#ae81ff">4</span>), data<span style="color:#f92672">=</span>wageDat)
lm.pred <span style="color:#f92672">=</span> <span style="color:#a6e22e">predict</span>(lm.fit, <span style="color:#a6e22e">data.frame</span>(age<span style="color:#f92672">=</span>age.grid))
<span style="color:#a6e22e">lines</span>(age.grid, lm.pred, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;blue&#34;</span>, lwd<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-6-1.png"   />

        
    </figure>



<h4 id="b-step-function-and-cross-validation">b) Step function and cross-validation</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># cross-validation</span>
cv.error <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rep</span>(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">9</span>)
<span style="color:#a6e22e">for </span>(i in <span style="color:#ae81ff">2</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>) {
  wageDat<span style="color:#f92672">$</span>age.cut <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cut</span>(wageDat<span style="color:#f92672">$</span>age,i)
  glm.fit <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">glm</span>(wage<span style="color:#f92672">~</span>age.cut, data<span style="color:#f92672">=</span>wageDat)
  cv.error[i<span style="color:#ae81ff">-1</span>] <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cv.glm</span>(wageDat, glm.fit, K<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)<span style="color:#f92672">$</span>delta[1]  <span style="color:#75715e"># [1]:std, [2]:bias-corrected</span>
}
cv.error</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 1732.337 1682.978 1636.736 1635.600 1624.174 1610.688 1604.081 1612.005
## [9] 1607.022</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">cv.error</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 1732.337 1682.978 1636.736 1635.600 1624.174 1610.688 1604.081 1612.005
## [9] 1607.022</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(<span style="color:#ae81ff">2</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>, cv.error, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-7-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">cut.fit <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">glm</span>(wage<span style="color:#f92672">~</span><span style="color:#a6e22e">cut</span>(age,<span style="color:#ae81ff">8</span>), data<span style="color:#f92672">=</span>wageDat)
preds <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(cut.fit, newdata<span style="color:#f92672">=</span><span style="color:#a6e22e">list</span>(age<span style="color:#f92672">=</span>age.grid), se<span style="color:#f92672">=</span><span style="color:#66d9ef">TRUE</span>)
se.bands <span style="color:#f92672">&lt;-</span> preds<span style="color:#f92672">$</span>fit <span style="color:#f92672">+</span> <span style="color:#a6e22e">cbind</span>(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>preds<span style="color:#f92672">$</span>se.fit, <span style="color:#ae81ff">-2</span><span style="color:#f92672">*</span>preds<span style="color:#f92672">$</span>se.fit)
<span style="color:#a6e22e">plot</span>(wageDat<span style="color:#f92672">$</span>age, wageDat<span style="color:#f92672">$</span>wage, xlim<span style="color:#f92672">=</span>agelims, cex<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;darkgrey&#34;</span>)
<span style="color:#a6e22e">title</span>(<span style="color:#e6db74">&#34;Fit with 8 Age Bands&#34;</span>)
<span style="color:#a6e22e">lines</span>(age.grid, preds<span style="color:#f92672">$</span>fit, lwd<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;blue&#34;</span>)
<span style="color:#a6e22e">matlines</span>(age.grid, se.bands, lwd<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;blue&#34;</span>, lty<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-8-1.png"   />

        
    </figure>



<h2 id="question-7.8---page-299">Question 7.8 - Page 299</h2>

<p>Fit some of the non-linear models investigated in this chapter to the
<code>Auto</code> data set. Is there evidence for non-linear relationships in this
data set? Create some informative plots to justify your answer.</p>

<h3 id="answer-1">Answer</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">autoDat<span style="color:#f92672">&lt;-</span>ISLR<span style="color:#f92672">::</span>Auto</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">autoDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">pivot_longer</span>(<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(mpg,name),names_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Params&#34;</span>,values_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Value&#34;</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>mpg,y<span style="color:#f92672">=</span>Value)) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_point</span>() <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">facet_wrap</span>(<span style="color:#f92672">~</span> Params, scales <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;free_y&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-10-1.png"   />

        
    </figure>



<p>Very clearly there is a lot of non-linearity in the <code>mpg</code> data,
especially for <code>acceleration</code>, <code>weight</code>, <code>displacement</code>, <code>horsepower</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">rss <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NA</span>, <span style="color:#ae81ff">10</span>)
fits <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>()
<span style="color:#a6e22e">for </span>(d in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>) {
    fits[[d]] <span style="color:#f92672">=</span> <span style="color:#a6e22e">lm</span>(mpg <span style="color:#f92672">~</span> <span style="color:#a6e22e">poly</span>(displacement, d), data <span style="color:#f92672">=</span> autoDat)
    rss[d] <span style="color:#f92672">=</span> <span style="color:#a6e22e">deviance</span>(fits[[d]])
}
rss <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##  [1] 8378.822 7412.263 7392.322 7391.722 7380.838 7270.746 7089.716 6917.401
##  [9] 6737.801 6610.190</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">anova</span>(fits[[1]],fits[[2]],fits[[3]],fits[[4]],fits[[5]],
  fits[[6]],fits[[7]],fits[[8]],fits[[9]],fits[[10]])</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Analysis of Variance Table
##
## Model  1: mpg ~ poly(displacement, d)
## Model  2: mpg ~ poly(displacement, d)
## Model  3: mpg ~ poly(displacement, d)
## Model  4: mpg ~ poly(displacement, d)
## Model  5: mpg ~ poly(displacement, d)
## Model  6: mpg ~ poly(displacement, d)
## Model  7: mpg ~ poly(displacement, d)
## Model  8: mpg ~ poly(displacement, d)
## Model  9: mpg ~ poly(displacement, d)
## Model 10: mpg ~ poly(displacement, d)
##    Res.Df    RSS Df Sum of Sq       F    Pr(&gt;F)
## 1     390 8378.8
## 2     389 7412.3  1    966.56 55.7108 5.756e-13 ***
## 3     388 7392.3  1     19.94  1.1494  0.284364
## 4     387 7391.7  1      0.60  0.0346  0.852549
## 5     386 7380.8  1     10.88  0.6273  0.428823
## 6     385 7270.7  1    110.09  6.3455  0.012177 *
## 7     384 7089.7  1    181.03 10.4343  0.001344 **
## 8     383 6917.4  1    172.31  9.9319  0.001753 **
## 9     382 6737.8  1    179.60 10.3518  0.001404 **
## 10    381 6610.2  1    127.61  7.3553  0.006990 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre></div>
<p>Confirming our visual indications, we see that the second degree models
work well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(glmnet)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Loading required package: Matrix</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;Matrix&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:tidyr&#39;:
##
##     expand, pack, unpack</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Loaded glmnet 3.0-2</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(boot)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">cv.errs <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NA</span>, <span style="color:#ae81ff">15</span>)
<span style="color:#a6e22e">for </span>(d in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">15</span>) {
    fit <span style="color:#f92672">=</span> <span style="color:#a6e22e">glm</span>(mpg <span style="color:#f92672">~</span> <span style="color:#a6e22e">poly</span>(displacement, d), data <span style="color:#f92672">=</span> Auto)
    cv.errs[d] <span style="color:#f92672">=</span> <span style="color:#a6e22e">cv.glm</span>(Auto, fit, K <span style="color:#f92672">=</span> <span style="color:#ae81ff">15</span>)<span style="color:#f92672">$</span>delta[2]
}
<span style="color:#a6e22e">which.min</span>(cv.errs)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 10</code></pre></div>
<p>Strangely, we seem to have ended up with a ten variable model here.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># Step functions</span>
cv.errs <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NA</span>, <span style="color:#ae81ff">10</span>)
<span style="color:#a6e22e">for </span>(c in <span style="color:#ae81ff">2</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>) {
    Auto<span style="color:#f92672">$</span>dis.cut <span style="color:#f92672">=</span> <span style="color:#a6e22e">cut</span>(Auto<span style="color:#f92672">$</span>displacement, c)
    fit <span style="color:#f92672">=</span> <span style="color:#a6e22e">glm</span>(mpg <span style="color:#f92672">~</span> dis.cut, data <span style="color:#f92672">=</span> Auto)
    cv.errs[c] <span style="color:#f92672">=</span> <span style="color:#a6e22e">cv.glm</span>(Auto, fit, K <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>)<span style="color:#f92672">$</span>delta[2]
}
<span style="color:#a6e22e">which.min</span>(cv.errs) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 9</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(splines)
cv.errs <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NA</span>, <span style="color:#ae81ff">10</span>)
<span style="color:#a6e22e">for </span>(df in <span style="color:#ae81ff">3</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>) {
    fit <span style="color:#f92672">=</span> <span style="color:#a6e22e">glm</span>(mpg <span style="color:#f92672">~</span> <span style="color:#a6e22e">ns</span>(displacement, df <span style="color:#f92672">=</span> df), data <span style="color:#f92672">=</span> Auto)
    cv.errs[df] <span style="color:#f92672">=</span> <span style="color:#a6e22e">cv.glm</span>(Auto, fit, K <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>)<span style="color:#f92672">$</span>delta[2]
}
<span style="color:#a6e22e">which.min</span>(cv.errs) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 10</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(gam)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Loading required package: foreach</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;foreach&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:purrr&#39;:
##
##     accumulate, when</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Loaded gam 1.16.1</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># GAMs</span>
fit <span style="color:#f92672">=</span> <span style="color:#a6e22e">gam</span>(mpg <span style="color:#f92672">~</span> <span style="color:#a6e22e">s</span>(displacement, <span style="color:#ae81ff">4</span>) <span style="color:#f92672">+</span> <span style="color:#a6e22e">s</span>(horsepower, <span style="color:#ae81ff">4</span>), data <span style="color:#f92672">=</span> Auto)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument
## ignored</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(fit)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call: gam(formula = mpg ~ s(displacement, 4) + s(horsepower, 4), data = Auto)
## Deviance Residuals:
##      Min       1Q   Median       3Q      Max
## -11.2982  -2.1592  -0.4394   2.1247  17.0946
##
## (Dispersion Parameter for gaussian family taken to be 15.3543)
##
##     Null Deviance: 23818.99 on 391 degrees of freedom
## Residual Deviance: 5880.697 on 382.9999 degrees of freedom
## AIC: 2194.05
##
## Number of Local Scoring Iterations: 2
##
## Anova for Parametric Effects
##                     Df  Sum Sq Mean Sq F value  Pr(&gt;F)
## s(displacement, 4)   1 15254.9 15254.9 993.524 &lt; 2e-16 ***
## s(horsepower, 4)     1  1038.4  1038.4  67.632 3.1e-15 ***
## Residuals          383  5880.7    15.4
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Anova for Nonparametric Effects
##                    Npar Df Npar F     Pr(F)
## (Intercept)
## s(displacement, 4)       3 13.613 1.863e-08 ***
## s(horsepower, 4)         3 15.606 1.349e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre></div>
<h2 id="question-7.9---pages-299-300">Question 7.9 - Pages 299-300</h2>

<p>This question uses the variables <code>dis</code> (the weighted mean of distances
to five <code>Boston</code> employment centers) and <code>nox</code> (nitrogen oxides
concentration in parts per 10 million) from the <code>Boston</code> data. We will
treat <code>dis</code> as the predictor and <code>nox</code> as the response.</p>

<p><strong>(a)</strong> Use the <code>poly()</code> function to fit a cubic polynomial regression to
predict <code>nox</code> using <code>dis</code>. Report the regression output, and plot the
resulting data and polynomial fits.</p>

<p><strong>(b)</strong> Plot the polynomial fits for a range of different polynomial
degrees (say, from 1 to 10), and report the associated residual sum of
squares.</p>

<p><strong>&copy;</strong> Perform cross-validation or another approach to select the optimal
degree for the polynomial, and explain your results.</p>

<p><strong>(d)</strong> Use the <code>bs()</code> function to fit a regression spline to predict nox
using <code>dis</code>. Report the output for the fit using four degrees of
freedom. How did you choose the knots? Plot the resulting fit.</p>

<p><strong>(e)</strong> Now fit a regression spline for a range of degrees of freedom, and
plot the resulting fits and report the resulting RSS. Describe the
results obtained.</p>

<p><strong>(f)</strong> Perform cross-validation or another approach in order to select
the best degrees of freedom for a regression spline on this data.
Describe your results.</p>

<h3 id="answer-2">Answer</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">&lt;-</span>MASS<span style="color:#f92672">::</span>Boston
boston <span style="color:#f92672">%&gt;%</span> str <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    506 obs. of  14 variables:
##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
##  $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...
##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
##  $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...
##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
##  $ black  : num  397 397 393 395 397 ...
##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       crim                zn             indus            chas
##  Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000
##  1st Qu.: 0.08204   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000
##  Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000
##  Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917
##  3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000
##  Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000
##       nox               rm             age              dis
##  Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130
##  1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100
##  Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207
##  Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795
##  3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188
##  Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127
##       rad              tax           ptratio          black
##  Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32
##  1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38
##  Median : 5.000   Median :330.0   Median :19.05   Median :391.44
##  Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67
##  3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23
##  Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90
##      lstat            medv
##  Min.   : 1.73   Min.   : 5.00
##  1st Qu.: 6.95   1st Qu.:17.02
##  Median :11.36   Median :21.20
##  Mean   :12.65   Mean   :22.53
##  3rd Qu.:16.95   3rd Qu.:25.00
##  Max.   :37.97   Max.   :50.00</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(unique) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(length) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##    crim      zn   indus    chas     nox      rm     age     dis     rad     tax
##     504      26      76       2      81     446     356     412       9      66
## ptratio   black   lstat    medv
##      46     357     455     229</code></pre></div>
<h4 id="a-polynomial">a) Polynomial</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">fit.03 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(nox<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(dis,<span style="color:#ae81ff">3</span>), data<span style="color:#f92672">=</span>boston)
dislims <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">range</span>(boston<span style="color:#f92672">$</span>dis)
dis.grid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">seq</span>(dislims[1], dislims[2], <span style="color:#ae81ff">0.1</span>)
preds <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(fit.03, newdata<span style="color:#f92672">=</span><span style="color:#a6e22e">list</span>(dis<span style="color:#f92672">=</span>dis.grid), se<span style="color:#f92672">=</span><span style="color:#66d9ef">TRUE</span>)
se.bands <span style="color:#f92672">&lt;-</span> preds<span style="color:#f92672">$</span>fit <span style="color:#f92672">+</span> <span style="color:#a6e22e">cbind</span>(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>preds<span style="color:#f92672">$</span>se.fit, <span style="color:#ae81ff">-2</span><span style="color:#f92672">*</span>preds<span style="color:#f92672">$</span>se.fit)
<span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>), mar<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">4.5</span>,<span style="color:#ae81ff">4.5</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>), oma<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">0</span>))
<span style="color:#a6e22e">plot</span>(boston<span style="color:#f92672">$</span>dis, boston<span style="color:#f92672">$</span>nox, xlim<span style="color:#f92672">=</span>dislims, cex<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;darkgrey&#34;</span>)
<span style="color:#a6e22e">title</span>(<span style="color:#e6db74">&#34;Degree 3 Polynomial Fit&#34;</span>)
<span style="color:#a6e22e">lines</span>(dis.grid, preds<span style="color:#f92672">$</span>fit, lwd<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;blue&#34;</span>)
<span style="color:#a6e22e">matlines</span>(dis.grid, se.bands, lwd<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;blue&#34;</span>, lty<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-20-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(fit.03)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = nox ~ poly(dis, 3), data = boston)
##
## Residuals:
##       Min        1Q    Median        3Q       Max
## -0.121130 -0.040619 -0.009738  0.023385  0.194904
##
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    0.554695   0.002759 201.021  &lt; 2e-16 ***
## poly(dis, 3)1 -2.003096   0.062071 -32.271  &lt; 2e-16 ***
## poly(dis, 3)2  0.856330   0.062071  13.796  &lt; 2e-16 ***
## poly(dis, 3)3 -0.318049   0.062071  -5.124 4.27e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 0.06207 on 502 degrees of freedom
## Multiple R-squared:  0.7148, Adjusted R-squared:  0.7131
## F-statistic: 419.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16</code></pre></div>
<h4 id="b-multiple-polynomials">b) Multiple Polynomials</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">rss.error <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rep</span>(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">10</span>)
<span style="color:#a6e22e">for </span>(i in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>) {
  lm.fit <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(nox<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(dis,i), data<span style="color:#f92672">=</span>boston)
  rss.error[i] <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">sum</span>(lm.fit<span style="color:#f92672">$</span>residuals^2)
}
rss.error</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##  [1] 2.768563 2.035262 1.934107 1.932981 1.915290 1.878257 1.849484 1.835630
##  [9] 1.833331 1.832171</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(rss.error, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-21-1.png"   />

        
    </figure>



<h4 id="c-cross-validation-and-polynomial-selection">c) Cross validation and polynomial selection</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">require</span>(boot)
<span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">1</span>)
cv.error <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rep</span>(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">10</span>)
<span style="color:#a6e22e">for </span>(i in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>) {
  glm.fit <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">glm</span>(nox<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(dis,i), data<span style="color:#f92672">=</span>boston)
  cv.error[i] <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cv.glm</span>(boston, glm.fit, K<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)<span style="color:#f92672">$</span>delta[1]  <span style="color:#75715e"># [1]:std, [2]:bias-corrected</span>
}
cv.error</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##  [1] 0.005558263 0.004085706 0.003876521 0.003863342 0.004237452 0.005686862
##  [7] 0.010278897 0.006810868 0.033308607 0.004075599</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(cv.error, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-22-1.png"   />

        
    </figure>



<ul>
<li>I feel like the second degree fit would be the most reasonable, though
the fourth degree seems to be doing well.</li>
</ul>

<h4 id="d-regression-spline">d) Regression spline</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">fit.sp <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(nox<span style="color:#f92672">~</span><span style="color:#a6e22e">bs</span>(dis, df<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>), data<span style="color:#f92672">=</span>boston)
pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(fit.sp, newdata<span style="color:#f92672">=</span><span style="color:#a6e22e">list</span>(dis<span style="color:#f92672">=</span>dis.grid), se<span style="color:#f92672">=</span>T)
<span style="color:#a6e22e">plot</span>(boston<span style="color:#f92672">$</span>dis, boston<span style="color:#f92672">$</span>nox, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gray&#34;</span>)
<span style="color:#a6e22e">lines</span>(dis.grid, pred<span style="color:#f92672">$</span>fit, lwd<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
<span style="color:#a6e22e">lines</span>(dis.grid, pred<span style="color:#f92672">$</span>fit<span style="color:#ae81ff">+2</span><span style="color:#f92672">*</span>pred<span style="color:#f92672">$</span>se, lty<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dashed&#34;</span>)
<span style="color:#a6e22e">lines</span>(dis.grid, pred<span style="color:#f92672">$</span>fit<span style="color:#ae81ff">-2</span><span style="color:#f92672">*</span>pred<span style="color:#f92672">$</span>se, lty<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dashed&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-23-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># set df to select knots at uniform quantiles of `dis`</span>
<span style="color:#a6e22e">attr</span>(<span style="color:#a6e22e">bs</span>(boston<span style="color:#f92672">$</span>dis,df<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>),<span style="color:#e6db74">&#34;knots&#34;</span>)  <span style="color:#75715e"># only 1 knot at 50th percentile</span></code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##     50%
## 3.20745</code></pre></div>
<h4 id="e-range-of-regression-splines">e) Range of regression splines</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">rss.error <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rep</span>(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">7</span>)
<span style="color:#a6e22e">for </span>(i in <span style="color:#ae81ff">4</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>) {
  fit.sp <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(nox<span style="color:#f92672">~</span><span style="color:#a6e22e">bs</span>(dis, df<span style="color:#f92672">=</span>i), data<span style="color:#f92672">=</span>boston)
  rss.error[i<span style="color:#ae81ff">-3</span>] <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">sum</span>(fit.sp<span style="color:#f92672">$</span>residuals^2)
}
rss.error</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 1.922775 1.840173 1.833966 1.829884 1.816995 1.825653 1.792535</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(<span style="color:#ae81ff">4</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>, rss.error, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-24-1.png"   />

        
    </figure>



<ul>
<li>As the model gains more degrees of freedom, it tends to over fit to
the training data better</li>
</ul>

<h4 id="f-cross-validation-for-best-spline">f) Cross validation for best spline</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">cv.error <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rep</span>(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">7</span>)
<span style="color:#a6e22e">for </span>(i in <span style="color:#ae81ff">4</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>) {
  glm.fit <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">glm</span>(nox<span style="color:#f92672">~</span><span style="color:#a6e22e">bs</span>(dis, df<span style="color:#f92672">=</span>i), data<span style="color:#f92672">=</span>boston)
  cv.error[i<span style="color:#ae81ff">-3</span>] <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cv.glm</span>(boston, glm.fit, K<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)<span style="color:#f92672">$</span>delta[1]
}</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.1523), Boundary.knots =
## c(1.1296, : some &#39;x&#39; values beyond boundary knots may cause ill-conditioned
## bases

## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.1523), Boundary.knots =
## c(1.1296, : some &#39;x&#39; values beyond boundary knots may cause ill-conditioned
## bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.2157), Boundary.knots =
## c(1.137, : some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.2157), Boundary.knots =
## c(1.137, : some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.35953333333333, : some
## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.35953333333333, : some
## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.38403333333333, : some
## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.38403333333333, : some
## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.07945, `50%` = 3.1323, :
## some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.07945, `50%` = 3.1323, :
## some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.1103, `50%` = 3.2797, : some
## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.1103, `50%` = 3.2797, : some
## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.9682, `40%` = 2.7147, : some
## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.9682, `40%` = 2.7147, : some
## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.95434, `40%` = 2.59666, :
## some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.95434, `40%` = 2.59666, :
## some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.82203333333333, : some
## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.82203333333333, : some
## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.8226, `33.33333%` =
## 2.3817, : some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.8226, `33.33333%` =
## 2.3817, : some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`14.28571%` = 1.7936, `28.57143%`
## = 2.16972857142857, : some &#39;x&#39; values beyond boundary knots may cause ill-
## conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`14.28571%` = 1.7936, `28.57143%`
## = 2.16972857142857, : some &#39;x&#39; values beyond boundary knots may cause ill-
## conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.754625, `25%` = 2.10215, :
## some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.754625, `25%` = 2.10215, :
## some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.751575, `25%` = 2.08755, :
## some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases

## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.751575, `25%` = 2.08755, :
## some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">cv.error</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 0.003898810 0.003694675 0.003732665 0.003766202 0.003716389 0.003723126
## [7] 0.003727358</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(<span style="color:#ae81ff">4</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>, cv.error, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-25-1.png"   />

        
    </figure>



<ul>
<li>A fifth degree polynomial is clearly indicated</li>
</ul>

<h2 id="question-10---page-300">Question 10 - Page 300</h2>

<p>This question relates to the <code>College</code> data set.</p>

<p><strong>(a)</strong> Split the data into a training set and a test set. Using
out-of-state tuition as the response and the other variables as the
predictors, perform forward stepwise selection on the training set in
order to identify a satisfactory model that uses just a subset of the
predictors.</p>

<p><strong>(b)</strong> Fit a GAM on the training data, using out-of-state tuition as the
response and the features selected in the previous step as the
predictors. Plot the results, and explain your ﬁndings.</p>

<p><strong>&copy;</strong> Evaluate the model obtained on the test set, and explain the
results obtained.</p>

<p><strong>(d)</strong> For which variables, if any, is there evidence of a non-linear
relationship with the response?</p>

<h3 id="answer-3">Answer</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">colDat<span style="color:#f92672">&lt;-</span>ISLR<span style="color:#f92672">::</span>College
colDat <span style="color:#f92672">%&gt;%</span> str <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    777 obs. of  18 variables:
##  $ Private    : Factor w/ 2 levels &#34;No&#34;,&#34;Yes&#34;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ Apps       : num  1660 2186 1428 417 193 ...
##  $ Accept     : num  1232 1924 1097 349 146 ...
##  $ Enroll     : num  721 512 336 137 55 158 103 489 227 172 ...
##  $ Top10perc  : num  23 16 22 60 16 38 17 37 30 21 ...
##  $ Top25perc  : num  52 29 50 89 44 62 45 68 63 44 ...
##  $ F.Undergrad: num  2885 2683 1036 510 249 ...
##  $ P.Undergrad: num  537 1227 99 63 869 ...
##  $ Outstate   : num  7440 12280 11250 12960 7560 ...
##  $ Room.Board : num  3300 6450 3750 5450 4120 ...
##  $ Books      : num  450 750 400 450 800 500 500 450 300 660 ...
##  $ Personal   : num  2200 1500 1165 875 1500 ...
##  $ PhD        : num  70 29 53 92 76 67 90 89 79 40 ...
##  $ Terminal   : num  78 30 66 97 72 73 93 100 84 41 ...
##  $ S.F.Ratio  : num  18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ...
##  $ perc.alumni: num  12 16 30 37 2 11 26 37 23 15 ...
##  $ Expend     : num  7041 10527 8735 19016 10922 ...
##  $ Grad.Rate  : num  60 56 54 59 15 55 63 73 80 52 ...
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">colDat <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##  Private        Apps           Accept          Enroll       Top10perc
##  No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00
##  Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00
##            Median : 1558   Median : 1110   Median : 434   Median :23.00
##            Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56
##            3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00
##            Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00
##    Top25perc      F.Undergrad     P.Undergrad         Outstate
##  Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340
##  1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320
##  Median : 54.0   Median : 1707   Median :  353.0   Median : 9990
##  Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441
##  3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925
##  Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700
##    Room.Board       Books           Personal         PhD
##  Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00
##  1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00
##  Median :4200   Median : 500.0   Median :1200   Median : 75.00
##  Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66
##  3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00
##  Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00
##     Terminal       S.F.Ratio      perc.alumni        Expend
##  Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186
##  1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751
##  Median : 82.0   Median :13.60   Median :21.00   Median : 8377
##  Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660
##  3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830
##  Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233
##    Grad.Rate
##  Min.   : 10.00
##  1st Qu.: 53.00
##  Median : 65.00
##  Mean   : 65.46
##  3rd Qu.: 78.00
##  Max.   :118.00</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">colDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(unique) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(length) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##     Private        Apps      Accept      Enroll   Top10perc   Top25perc
##           2         711         693         581          82          89
## F.Undergrad P.Undergrad    Outstate  Room.Board       Books    Personal
##         714         566         640         553         122         294
##         PhD    Terminal   S.F.Ratio perc.alumni      Expend   Grad.Rate
##          78          65         173          61         744          81</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">plotLEAP<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(leapObj){
  <span style="color:#a6e22e">par</span>(mfrow <span style="color:#f92672">=</span> <span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
  bar2<span style="color:#f92672">=</span><span style="color:#a6e22e">which.max</span>(leapObj<span style="color:#f92672">$</span>adjr2)
  bbic<span style="color:#f92672">=</span><span style="color:#a6e22e">which.min</span>(leapObj<span style="color:#f92672">$</span>bic)
  bcp<span style="color:#f92672">=</span><span style="color:#a6e22e">which.min</span>(leapObj<span style="color:#f92672">$</span>cp)
  <span style="color:#a6e22e">plot</span>(leapObj<span style="color:#f92672">$</span>rss,xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Number of variables&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;RSS&#34;</span>,type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)
  <span style="color:#a6e22e">plot</span>(leapObj<span style="color:#f92672">$</span>adjr2,xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Number of variables&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#a6e22e">TeX</span>(<span style="color:#e6db74">&#34;Adjusted R^2&#34;</span>),type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)
  <span style="color:#a6e22e">points</span>(bar2,leapObj<span style="color:#f92672">$</span>adjr2[bar2],col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;green&#34;</span>,cex<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,pch<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
  <span style="color:#a6e22e">plot</span>(leapObj<span style="color:#f92672">$</span>bic,xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Number of variables&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#a6e22e">TeX</span>(<span style="color:#e6db74">&#34;BIC&#34;</span>),type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)
  <span style="color:#a6e22e">points</span>(bbic,leapObj<span style="color:#f92672">$</span>bic[bbic],col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;blue&#34;</span>,cex<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,pch<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
  <span style="color:#a6e22e">plot</span>(leapObj<span style="color:#f92672">$</span>cp,xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Number of variables&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#a6e22e">TeX</span>(<span style="color:#e6db74">&#34;C_p&#34;</span>),type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)
  <span style="color:#a6e22e">points</span>(bcp,leapObj<span style="color:#f92672">$</span>cp[bcp],col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>,cex<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,pch<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
}</code></pre></div>
<h4 id="a-train-test">a) Train test</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">train_ind <span style="color:#f92672">=</span> <span style="color:#a6e22e">sample</span>(colDat <span style="color:#f92672">%&gt;%</span> nrow,<span style="color:#ae81ff">100</span>)
test_ind <span style="color:#f92672">=</span> <span style="color:#a6e22e">setdiff</span>(<span style="color:#a6e22e">seq_len</span>(colDat <span style="color:#f92672">%&gt;%</span> nrow), train_ind)</code></pre></div>
<h4 id="best-subset-selection">Best subset selection</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">train_set<span style="color:#f92672">&lt;-</span>colDat[train_ind,]
test_set<span style="color:#f92672">&lt;-</span>colDat[<span style="color:#f92672">-</span>train_ind,]</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(leaps)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">modelFit<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">regsubsets</span>(Outstate<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>colDat,nvmax<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
modelFit <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Subset selection object
## Call: regsubsets.formula(Outstate ~ ., data = colDat, nvmax = 20)
## 17 Variables  (and intercept)
##             Forced in Forced out
## PrivateYes      FALSE      FALSE
## Apps            FALSE      FALSE
## Accept          FALSE      FALSE
## Enroll          FALSE      FALSE
## Top10perc       FALSE      FALSE
## Top25perc       FALSE      FALSE
## F.Undergrad     FALSE      FALSE
## P.Undergrad     FALSE      FALSE
## Room.Board      FALSE      FALSE
## Books           FALSE      FALSE
## Personal        FALSE      FALSE
## PhD             FALSE      FALSE
## Terminal        FALSE      FALSE
## S.F.Ratio       FALSE      FALSE
## perc.alumni     FALSE      FALSE
## Expend          FALSE      FALSE
## Grad.Rate       FALSE      FALSE
## 1 subsets of each size up to 17
## Selection Algorithm: exhaustive
##           PrivateYes Apps Accept Enroll Top10perc Top25perc F.Undergrad
## 1  ( 1 )  &#34; &#34;        &#34; &#34;  &#34; &#34;    &#34; &#34;    &#34; &#34;       &#34; &#34;       &#34; &#34;
## 2  ( 1 )  &#34;*&#34;        &#34; &#34;  &#34; &#34;    &#34; &#34;    &#34; &#34;       &#34; &#34;       &#34; &#34;
## 3  ( 1 )  &#34;*&#34;        &#34; &#34;  &#34; &#34;    &#34; &#34;    &#34; &#34;       &#34; &#34;       &#34; &#34;
## 4  ( 1 )  &#34;*&#34;        &#34; &#34;  &#34; &#34;    &#34; &#34;    &#34; &#34;       &#34; &#34;       &#34; &#34;
## 5  ( 1 )  &#34;*&#34;        &#34; &#34;  &#34; &#34;    &#34; &#34;    &#34; &#34;       &#34; &#34;       &#34; &#34;
## 6  ( 1 )  &#34;*&#34;        &#34; &#34;  &#34; &#34;    &#34; &#34;    &#34; &#34;       &#34; &#34;       &#34; &#34;
## 7  ( 1 )  &#34;*&#34;        &#34; &#34;  &#34; &#34;    &#34; &#34;    &#34; &#34;       &#34; &#34;       &#34; &#34;
## 8  ( 1 )  &#34;*&#34;        &#34; &#34;  &#34;*&#34;    &#34; &#34;    &#34; &#34;       &#34; &#34;       &#34;*&#34;
## 9  ( 1 )  &#34;*&#34;        &#34;*&#34;  &#34;*&#34;    &#34; &#34;    &#34; &#34;       &#34; &#34;       &#34;*&#34;
## 10  ( 1 ) &#34;*&#34;        &#34;*&#34;  &#34;*&#34;    &#34; &#34;    &#34;*&#34;       &#34; &#34;       &#34;*&#34;
## 11  ( 1 ) &#34;*&#34;        &#34;*&#34;  &#34;*&#34;    &#34; &#34;    &#34;*&#34;       &#34; &#34;       &#34;*&#34;
## 12  ( 1 ) &#34;*&#34;        &#34;*&#34;  &#34;*&#34;    &#34; &#34;    &#34;*&#34;       &#34; &#34;       &#34;*&#34;
## 13  ( 1 ) &#34;*&#34;        &#34;*&#34;  &#34;*&#34;    &#34;*&#34;    &#34;*&#34;       &#34; &#34;       &#34;*&#34;
## 14  ( 1 ) &#34;*&#34;        &#34;*&#34;  &#34;*&#34;    &#34;*&#34;    &#34;*&#34;       &#34; &#34;       &#34;*&#34;
## 15  ( 1 ) &#34;*&#34;        &#34;*&#34;  &#34;*&#34;    &#34;*&#34;    &#34;*&#34;       &#34; &#34;       &#34;*&#34;
## 16  ( 1 ) &#34;*&#34;        &#34;*&#34;  &#34;*&#34;    &#34;*&#34;    &#34;*&#34;       &#34;*&#34;       &#34;*&#34;
## 17  ( 1 ) &#34;*&#34;        &#34;*&#34;  &#34;*&#34;    &#34;*&#34;    &#34;*&#34;       &#34;*&#34;       &#34;*&#34;
##           P.Undergrad Room.Board Books Personal PhD Terminal S.F.Ratio
## 1  ( 1 )  &#34; &#34;         &#34; &#34;        &#34; &#34;   &#34; &#34;      &#34; &#34; &#34; &#34;      &#34; &#34;
## 2  ( 1 )  &#34; &#34;         &#34; &#34;        &#34; &#34;   &#34; &#34;      &#34; &#34; &#34; &#34;      &#34; &#34;
## 3  ( 1 )  &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34; &#34;      &#34; &#34; &#34; &#34;      &#34; &#34;
## 4  ( 1 )  &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34; &#34;      &#34; &#34; &#34; &#34;      &#34; &#34;
## 5  ( 1 )  &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34; &#34;      &#34;*&#34; &#34; &#34;      &#34; &#34;
## 6  ( 1 )  &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34; &#34;      &#34; &#34; &#34;*&#34;      &#34; &#34;
## 7  ( 1 )  &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34;*&#34;      &#34; &#34; &#34;*&#34;      &#34; &#34;
## 8  ( 1 )  &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34; &#34;      &#34; &#34; &#34;*&#34;      &#34; &#34;
## 9  ( 1 )  &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34; &#34;      &#34; &#34; &#34;*&#34;      &#34; &#34;
## 10  ( 1 ) &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34; &#34;      &#34; &#34; &#34;*&#34;      &#34; &#34;
## 11  ( 1 ) &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34;*&#34;      &#34; &#34; &#34;*&#34;      &#34; &#34;
## 12  ( 1 ) &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34;*&#34;      &#34; &#34; &#34;*&#34;      &#34;*&#34;
## 13  ( 1 ) &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34;*&#34;      &#34; &#34; &#34;*&#34;      &#34;*&#34;
## 14  ( 1 ) &#34; &#34;         &#34;*&#34;        &#34; &#34;   &#34;*&#34;      &#34;*&#34; &#34;*&#34;      &#34;*&#34;
## 15  ( 1 ) &#34; &#34;         &#34;*&#34;        &#34;*&#34;   &#34;*&#34;      &#34;*&#34; &#34;*&#34;      &#34;*&#34;
## 16  ( 1 ) &#34; &#34;         &#34;*&#34;        &#34;*&#34;   &#34;*&#34;      &#34;*&#34; &#34;*&#34;      &#34;*&#34;
## 17  ( 1 ) &#34;*&#34;         &#34;*&#34;        &#34;*&#34;   &#34;*&#34;      &#34;*&#34; &#34;*&#34;      &#34;*&#34;
##           perc.alumni Expend Grad.Rate
## 1  ( 1 )  &#34; &#34;         &#34;*&#34;    &#34; &#34;
## 2  ( 1 )  &#34; &#34;         &#34;*&#34;    &#34; &#34;
## 3  ( 1 )  &#34; &#34;         &#34;*&#34;    &#34; &#34;
## 4  ( 1 )  &#34;*&#34;         &#34;*&#34;    &#34; &#34;
## 5  ( 1 )  &#34;*&#34;         &#34;*&#34;    &#34; &#34;
## 6  ( 1 )  &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 7  ( 1 )  &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 8  ( 1 )  &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 9  ( 1 )  &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 10  ( 1 ) &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 11  ( 1 ) &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 12  ( 1 ) &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 13  ( 1 ) &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 14  ( 1 ) &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 15  ( 1 ) &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 16  ( 1 ) &#34;*&#34;         &#34;*&#34;    &#34;*&#34;
## 17  ( 1 ) &#34;*&#34;         &#34;*&#34;    &#34;*&#34;</code></pre></div>
<p>We might want to take a look at these.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(modelFit)
<span style="color:#a6e22e">plot</span>(modelFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Cp&#39;</span>)
<span style="color:#a6e22e">plot</span>(modelFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r2&#39;</span>)
<span style="color:#a6e22e">plot</span>(modelFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adjr2&#39;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-32-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plotLEAP</span>(modelFit <span style="color:#f92672">%&gt;%</span> summary)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-33-1.png"   />

        
    </figure>



<ul>
<li>So we like 14 variables, namely</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coefficients</span>(modelFit,id<span style="color:#f92672">=</span><span style="color:#ae81ff">14</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##   (Intercept)    PrivateYes          Apps        Accept        Enroll
## -1.817040e+03  2.256946e+03 -2.999022e-01  8.023519e-01 -5.372545e-01
##     Top10perc   F.Undergrad    Room.Board      Personal           PhD
##  2.365529e+01 -9.569936e-02  8.741819e-01 -2.478418e-01  1.269506e+01
##      Terminal     S.F.Ratio   perc.alumni        Expend     Grad.Rate
##  2.297296e+01 -4.700560e+01  4.195006e+01  2.003912e-01  2.383197e+01</code></pre></div>
<ul>
<li>But five seems like a better bet.</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coefficients</span>(modelFit,id<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##   (Intercept)    PrivateYes    Room.Board           PhD   perc.alumni
## -2864.6325619  2936.7416766     1.0677573    40.5334088    61.3147684
##        Expend
##     0.2253945</code></pre></div>
<h4 id="b-gam">b) GAM</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(gam)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">fit <span style="color:#f92672">=</span> <span style="color:#a6e22e">gam</span>(Outstate <span style="color:#f92672">~</span> Private<span style="color:#f92672">+</span><span style="color:#a6e22e">s</span>(Apps,<span style="color:#ae81ff">3</span>)<span style="color:#f92672">+</span>Accept<span style="color:#f92672">+</span>Enroll<span style="color:#f92672">+</span>
            Top10perc<span style="color:#f92672">+</span>F.Undergrad<span style="color:#f92672">+</span>Room.Board<span style="color:#f92672">+</span>
            Personal<span style="color:#f92672">+</span>PhD<span style="color:#f92672">+</span>Terminal<span style="color:#f92672">+</span>S.F.Ratio<span style="color:#f92672">+</span>
            perc.alumni<span style="color:#f92672">+</span>Expend<span style="color:#f92672">+</span>Grad.Rate
        , data <span style="color:#f92672">=</span> colDat)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument
## ignored</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">fit2 <span style="color:#f92672">=</span> <span style="color:#a6e22e">gam</span>(Outstate <span style="color:#f92672">~</span> Private<span style="color:#f92672">+</span><span style="color:#a6e22e">s</span>(Room.Board,<span style="color:#ae81ff">2</span>)<span style="color:#f92672">+</span><span style="color:#a6e22e">s</span>(PhD,<span style="color:#ae81ff">3</span>)<span style="color:#f92672">+</span><span style="color:#a6e22e">s</span>(perc.alumni)<span style="color:#f92672">+</span>Expend
        , data <span style="color:#f92672">=</span> colDat)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument
## ignored</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(fit,se<span style="color:#f92672">=</span><span style="color:#66d9ef">TRUE</span>)</code></pre></div>
<p><img src="/islr/sol5/unnamed-chunk-39-1.png" alt="" />
<img src="/islr/sol5/unnamed-chunk-39-2.png" alt="" />
<img src="/islr/sol5/unnamed-chunk-39-3.png" alt="" />
<img src="/islr/sol5/unnamed-chunk-39-4.png" alt="" /></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(fit2,se<span style="color:#f92672">=</span><span style="color:#66d9ef">TRUE</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol5/unnamed-chunk-40-2.png"   />

        
    </figure>



<h4 id="c-evaluate">c) Evaluate</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(fit, test_set)
mse.error <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">mean</span>((test_set<span style="color:#f92672">$</span>Outstate <span style="color:#f92672">-</span> pred)^2)
mse.error <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 3691891</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">gam.tss <span style="color:#f92672">=</span> <span style="color:#a6e22e">mean</span>((test_set<span style="color:#f92672">$</span>Outstate <span style="color:#f92672">-</span> <span style="color:#a6e22e">mean</span>(test_set<span style="color:#f92672">$</span>Outstate))^2)
test.rss <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> mse.error<span style="color:#f92672">/</span>gam.tss
test.rss <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 0.7731239</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">pred2 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">predict</span>(fit2, test_set)
mse.error2 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">mean</span>((test_set<span style="color:#f92672">$</span>Outstate <span style="color:#f92672">-</span> pred2)^2)
mse.error2 <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 4121902</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">gam.tss2 <span style="color:#f92672">=</span> <span style="color:#a6e22e">mean</span>((test_set<span style="color:#f92672">$</span>Outstate <span style="color:#f92672">-</span> <span style="color:#a6e22e">mean</span>(test_set<span style="color:#f92672">$</span>Outstate))^2)
test.rss2 <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> mse.error2<span style="color:#f92672">/</span>gam.tss2
test.rss2 <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 0.7466987</code></pre></div>
<p>This is pretty good model, all told.</p>

<h4 id="d-summary">d) Summary</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(fit) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call: gam(formula = Outstate ~ Private + s(Apps, 3) + Accept + Enroll +
##     Top10perc + F.Undergrad + Room.Board + Personal + PhD + Terminal +
##     S.F.Ratio + perc.alumni + Expend + Grad.Rate, data = colDat)
## Deviance Residuals:
##       Min        1Q    Median        3Q       Max
## -6641.083 -1262.806    -5.698  1270.911  9965.901
##
## (Dispersion Parameter for gaussian family taken to be 3749048)
##
##     Null Deviance: 12559297426 on 776 degrees of freedom
## Residual Deviance: 2849276343 on 760 degrees of freedom
## AIC: 13985.3
##
## Number of Local Scoring Iterations: 2
##
## Anova for Parametric Effects
##              Df     Sum Sq    Mean Sq  F value    Pr(&gt;F)
## Private       1 4034912907 4034912907 1076.250 &lt; 2.2e-16 ***
## s(Apps, 3)    1 1344548030 1344548030  358.637 &lt; 2.2e-16 ***
## Accept        1   90544274   90544274   24.151 1.091e-06 ***
## Enroll        1  144471570  144471570   38.535 8.838e-10 ***
## Top10perc     1 1802244831 1802244831  480.721 &lt; 2.2e-16 ***
## F.Undergrad   1   45230645   45230645   12.065 0.0005430 ***
## Room.Board    1 1110285773 1110285773  296.151 &lt; 2.2e-16 ***
## Personal      1   47886988   47886988   12.773 0.0003738 ***
## PhD           1  220249039  220249039   58.748 5.476e-14 ***
## Terminal      1   66366007   66366007   17.702 2.892e-05 ***
## S.F.Ratio     1  190811028  190811028   50.896 2.274e-12 ***
## perc.alumni   1  225293653  225293653   60.094 2.904e-14 ***
## Expend        1  258162295  258162295   68.861 4.805e-16 ***
## Grad.Rate     1   57947219   57947219   15.457 9.214e-05 ***
## Residuals   760 2849276343    3749048
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Anova for Nonparametric Effects
##             Npar Df Npar F     Pr(F)
## (Intercept)
## Private
## s(Apps, 3)        2  8.571 0.0002085 ***
## Accept
## Enroll
## Top10perc
## F.Undergrad
## Room.Board
## Personal
## PhD
## Terminal
## S.F.Ratio
## perc.alumni
## Expend
## Grad.Rate
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(fit2) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call: gam(formula = Outstate ~ Private + s(Room.Board, 2) + s(PhD,
##     3) + s(perc.alumni) + Expend, data = colDat)
## Deviance Residuals:
##       Min        1Q    Median        3Q       Max
## -8676.030 -1345.678    -8.409  1265.524  9590.459
##
## (Dispersion Parameter for gaussian family taken to be 4175193)
##
##     Null Deviance: 12559297426 on 776 degrees of freedom
## Residual Deviance: 3194023899 on 765.0002 degrees of freedom
## AIC: 14064.05
##
## Number of Local Scoring Iterations: 2
##
## Anova for Parametric Effects
##                   Df     Sum Sq    Mean Sq F value    Pr(&gt;F)
## Private            1 3751107814 3751107814  898.43 &lt; 2.2e-16 ***
## s(Room.Board, 2)   1 2913770756 2913770756  697.88 &lt; 2.2e-16 ***
## s(PhD, 3)          1 1149711330 1149711330  275.37 &lt; 2.2e-16 ***
## s(perc.alumni)     1  556759894  556759894  133.35 &lt; 2.2e-16 ***
## Expend             1  554812125  554812125  132.88 &lt; 2.2e-16 ***
## Residuals        765 3194023899    4175193
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Anova for Nonparametric Effects
##                  Npar Df Npar F     Pr(F)
## (Intercept)
## Private
## s(Room.Board, 2)       1 4.9853 0.0258517 *
## s(PhD, 3)              2 9.1614 0.0001171 ***
## s(perc.alumni)         3 0.8726 0.4548496
## Expend
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre></div>]]></content>
        </item>
        
        <item>
            <title>ISLR :: Linear Model Selection and Regularization</title>
            <link>https://rgoswami.me/posts/islr-ch6/</link>
            <pubDate>Wed, 19 Feb 2020 07:00:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/islr-ch6/</guid>
            <description>Chapter VI - Linear Model Selection and Regularization All the questions are as per the ISL seventh printing1.
Common Instead of using the standard functions, we will leverage the mlr3 package2.
#install.packages(&amp;#34;mlr3&amp;#34;,&amp;#34;data.table&amp;#34;,&amp;#34;mlr3viz&amp;#34;,&amp;#34;mlr3learners&amp;#34;) Actually for R version 3.6.2, the steps to get it working were a bit more involved.
Load ISLR and other libraries.
libsUsed&amp;lt;-c(&amp;#34;dplyr&amp;#34;,&amp;#34;ggplot2&amp;#34;,&amp;#34;tidyverse&amp;#34;, &amp;#34;ISLR&amp;#34;,&amp;#34;caret&amp;#34;,&amp;#34;MASS&amp;#34;, &amp;#34;gridExtra&amp;#34;, &amp;#34;pls&amp;#34;,&amp;#34;latex2exp&amp;#34;,&amp;#34;data.table&amp;#34;) invisible(lapply(libsUsed, library, character.only = TRUE))## ## Attaching package: &amp;#39;dplyr&amp;#39;## The following objects are masked from &amp;#39;package:stats&amp;#39;: ## ## filter, lag## The following objects are masked from &amp;#39;package:base&amp;#39;: ## ## intersect, setdiff, setequal, union## ── Attaching packages ─────────────────────────────────────── tidyverse 1.</description>
            <content type="html"><![CDATA[

<h2 id="chapter-vi---linear-model-selection-and-regularization">Chapter VI - Linear Model Selection and Regularization</h2>

<p>All the questions are as per the
<a href="https://faculty.marshall.usc.edu/gareth-james/ISL/" target="_blank">ISL seventh
printing</a><sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup>.</p>

<h3 id="common">Common</h3>

<p>Instead of using the standard functions, we will leverage the <code>mlr3</code>
package<sup class="footnote-ref" id="fnref:fn-2"><a href="#fn:fn-2">2</a></sup>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e">#install.packages(&#34;mlr3&#34;,&#34;data.table&#34;,&#34;mlr3viz&#34;,&#34;mlr3learners&#34;)</span></code></pre></div>
<p>Actually for <code>R</code> version <code>3.6.2</code>, the steps to get it working were a bit
more involved.</p>

<p>Load <code>ISLR</code> and other libraries.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">libsUsed<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">c</span>(<span style="color:#e6db74">&#34;dplyr&#34;</span>,<span style="color:#e6db74">&#34;ggplot2&#34;</span>,<span style="color:#e6db74">&#34;tidyverse&#34;</span>,
            <span style="color:#e6db74">&#34;ISLR&#34;</span>,<span style="color:#e6db74">&#34;caret&#34;</span>,<span style="color:#e6db74">&#34;MASS&#34;</span>, <span style="color:#e6db74">&#34;gridExtra&#34;</span>,
            <span style="color:#e6db74">&#34;pls&#34;</span>,<span style="color:#e6db74">&#34;latex2exp&#34;</span>,<span style="color:#e6db74">&#34;data.table&#34;</span>)
<span style="color:#a6e22e">invisible</span>(<span style="color:#a6e22e">lapply</span>(libsUsed, library, character.only <span style="color:#f92672">=</span> <span style="color:#66d9ef">TRUE</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;dplyr&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:stats&#39;:
##
##     filter, lag</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:base&#39;:
##
##     intersect, setdiff, setequal, union</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## ✔ tibble  2.1.3     ✔ purrr   0.3.3
## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.4.0</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Loading required package: lattice</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;caret&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:purrr&#39;:
##
##     lift</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;MASS&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:dplyr&#39;:
##
##     select</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;gridExtra&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:dplyr&#39;:
##
##     combine</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;pls&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:caret&#39;:
##
##     R2</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:stats&#39;:
##
##     loadings</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;data.table&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:purrr&#39;:
##
##     transpose</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:dplyr&#39;:
##
##     between, first, last</code></pre></div>
<h2 id="question-6.8---page-262">Question 6.8 - Page 262</h2>

<p>In this exercise, we will generate simulated data, and will then use
this data to perform best subset selection.</p>

<p><strong>(a)</strong> Use the =rnorm()=function to generate a predictor \(X\) of length
\(n = 100\), as well as a noise vector \(\eta\) of length \(n = 100\).</p>

<p><strong>(b)</strong> Generate a response vector \(Y\) of length \(n = 100\) according to
the model \[Y = \beta_0 + \beta_1X + \beta2X^2 + \beta_3X^3 + \eta\],
where \(\beta_{0}\) , \(\beta_{1}\), \(\beta_{2}\), and \(\beta_{3}\) are
constants of your choice.</p>

<p><strong>&copy;</strong> Use the <code>regsubsets()</code> function to perform best subset selection
in order to choose the best model containing the predictors \(X\),
\(X^{2}\), &hellip;, \(X^{10}\). What is the best model obtained according to
\(C_p\) , BIC, and adjusted \(R^2\) ? Show some plots to provide evidence
for your answer, and report the coefficients of the best model obtained.
Note you will need to use the <code>data.frame()</code> function to create a single
data set containing both \(X\) and \(Y\).</p>

<p><strong>(d)</strong> Repeat &copy;, using forward stepwise selection and also using
backwards stepwise selection. How does your answer compare to the
results in &copy;?</p>

<p><strong>(e)</strong> Now fit a lasso model to the simulated data, again using \(X\),
\(X^{2}\), &hellip;, \(X^{10}\) as predictors. Use cross-validation to select the
optimal value of \(\lambda\). Create plots of the cross-validation error
as a function of \(\lambda\). Report the resulting coefficient estimates,
and discuss the results obtained.</p>

<p><strong>(f)</strong> Now generate a response vector Y according to the model
\[Y = \beta_{0} + \beta_{7}X^{7} + \eta,\] and perform best subset
selection and the lasso. Discuss the results obtained.</p>

<h3 id="answer">Answer</h3>

<h4 id="a-generate-model">a) Generate model</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">1984</span>)
x<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">rnorm</span>(<span style="color:#ae81ff">100</span>)
noise<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">rnorm</span>(<span style="color:#ae81ff">100</span>)</code></pre></div>
<h4 id="b-response-vector">b) Response vector</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">beta<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">43</span>,<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">6</span>)
y<span style="color:#f92672">&lt;-</span>beta[1] <span style="color:#f92672">+</span> beta[2]<span style="color:#f92672">*</span>x <span style="color:#f92672">+</span> beta[3]<span style="color:#f92672">*</span>x^2 <span style="color:#f92672">+</span> beta[4]<span style="color:#f92672">*</span>x^3 <span style="color:#f92672">+</span> noise
<span style="color:#a6e22e">qplot</span>(x,y)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-4-1.png"   />

        
    </figure>



<h4 id="c-subset-selection">c) Subset selection</h4>

<p>Since the question requires it, we will be using the <code>leaps</code> libraries.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(leaps)
df<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">data.frame</span>(y<span style="color:#f92672">=</span>y,x<span style="color:#f92672">=</span>x)
sets<span style="color:#f92672">=</span><span style="color:#a6e22e">regsubsets</span>(y<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(x,<span style="color:#ae81ff">10</span>,raw<span style="color:#f92672">=</span>T),data<span style="color:#f92672">=</span>df,nvmax<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
sets <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Subset selection object
## Call: regsubsets.formula(y ~ poly(x, 10, raw = T), data = df, nvmax = 10)
## 10 Variables  (and intercept)
##                        Forced in Forced out
## poly(x, 10, raw = T)1      FALSE      FALSE
## poly(x, 10, raw = T)2      FALSE      FALSE
## poly(x, 10, raw = T)3      FALSE      FALSE
## poly(x, 10, raw = T)4      FALSE      FALSE
## poly(x, 10, raw = T)5      FALSE      FALSE
## poly(x, 10, raw = T)6      FALSE      FALSE
## poly(x, 10, raw = T)7      FALSE      FALSE
## poly(x, 10, raw = T)8      FALSE      FALSE
## poly(x, 10, raw = T)9      FALSE      FALSE
## poly(x, 10, raw = T)10     FALSE      FALSE
## 1 subsets of each size up to 10
## Selection Algorithm: exhaustive
##           poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 poly(x, 10, raw = T)3
## 1  ( 1 )  &#34; &#34;                   &#34; &#34;                   &#34;*&#34;
## 2  ( 1 )  &#34; &#34;                   &#34;*&#34;                   &#34;*&#34;
## 3  ( 1 )  &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
## 4  ( 1 )  &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
## 5  ( 1 )  &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
## 6  ( 1 )  &#34;*&#34;                   &#34; &#34;                   &#34;*&#34;
## 7  ( 1 )  &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
## 8  ( 1 )  &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
## 9  ( 1 )  &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
## 10  ( 1 ) &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
##           poly(x, 10, raw = T)4 poly(x, 10, raw = T)5 poly(x, 10, raw = T)6
## 1  ( 1 )  &#34; &#34;                   &#34; &#34;                   &#34; &#34;
## 2  ( 1 )  &#34; &#34;                   &#34; &#34;                   &#34; &#34;
## 3  ( 1 )  &#34; &#34;                   &#34; &#34;                   &#34; &#34;
## 4  ( 1 )  &#34;*&#34;                   &#34; &#34;                   &#34; &#34;
## 5  ( 1 )  &#34; &#34;                   &#34; &#34;                   &#34; &#34;
## 6  ( 1 )  &#34;*&#34;                   &#34; &#34;                   &#34;*&#34;
## 7  ( 1 )  &#34;*&#34;                   &#34; &#34;                   &#34;*&#34;
## 8  ( 1 )  &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
## 9  ( 1 )  &#34;*&#34;                   &#34; &#34;                   &#34;*&#34;
## 10  ( 1 ) &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
##           poly(x, 10, raw = T)7 poly(x, 10, raw = T)8 poly(x, 10, raw = T)9
## 1  ( 1 )  &#34; &#34;                   &#34; &#34;                   &#34; &#34;
## 2  ( 1 )  &#34; &#34;                   &#34; &#34;                   &#34; &#34;
## 3  ( 1 )  &#34; &#34;                   &#34; &#34;                   &#34; &#34;
## 4  ( 1 )  &#34; &#34;                   &#34; &#34;                   &#34; &#34;
## 5  ( 1 )  &#34; &#34;                   &#34; &#34;                   &#34;*&#34;
## 6  ( 1 )  &#34; &#34;                   &#34;*&#34;                   &#34; &#34;
## 7  ( 1 )  &#34; &#34;                   &#34;*&#34;                   &#34; &#34;
## 8  ( 1 )  &#34; &#34;                   &#34;*&#34;                   &#34; &#34;
## 9  ( 1 )  &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
## 10  ( 1 ) &#34;*&#34;                   &#34;*&#34;                   &#34;*&#34;
##           poly(x, 10, raw = T)10
## 1  ( 1 )  &#34; &#34;
## 2  ( 1 )  &#34; &#34;
## 3  ( 1 )  &#34; &#34;
## 4  ( 1 )  &#34; &#34;
## 5  ( 1 )  &#34;*&#34;
## 6  ( 1 )  &#34;*&#34;
## 7  ( 1 )  &#34;*&#34;
## 8  ( 1 )  &#34;*&#34;
## 9  ( 1 )  &#34;*&#34;
## 10  ( 1 ) &#34;*&#34;</code></pre></div>
<p>We also want the best parameters.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">summarySet<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">summary</span>(sets)
<span style="color:#a6e22e">which.min</span>(summarySet<span style="color:#f92672">$</span>cp) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 3</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">which.min</span>(summarySet<span style="color:#f92672">$</span>bic) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 3</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">which.max</span>(summarySet<span style="color:#f92672">$</span>adjr2) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 7</code></pre></div>
<p>We might want to see this as a plot.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(summarySet<span style="color:#f92672">$</span>cp, xlab <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Subset Size&#34;</span>, ylab <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Cp&#34;</span>, pch <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>, type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;l&#34;</span>)
<span style="color:#a6e22e">points</span>(<span style="color:#ae81ff">3</span>,summarySet<span style="color:#f92672">$</span>cp[3],pch<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,col<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>,lwd<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-7-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(summarySet<span style="color:#f92672">$</span>bic, xlab <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Subset Size&#34;</span>, ylab <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;BIC&#34;</span>, pch <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>, type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;l&#34;</span>)
<span style="color:#a6e22e">points</span>(<span style="color:#ae81ff">3</span>,summarySet<span style="color:#f92672">$</span>bic[3],pch<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,col<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>,lwd<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-8-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(summarySet<span style="color:#f92672">$</span>adjr2, xlab <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Subset Size&#34;</span>, ylab <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Adjusted R2&#34;</span>, pch <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>, type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;l&#34;</span>)
<span style="color:#a6e22e">points</span>(<span style="color:#ae81ff">3</span>,summarySet<span style="color:#f92672">$</span>adjr2[3],pch<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,col<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>,lwd<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-9-1.png"   />

        
    </figure>



<p>Lets check the coefficients.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coefficients</span>(sets,id<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##           (Intercept) poly(x, 10, raw = T)1 poly(x, 10, raw = T)2
##             42.895657              5.108094              3.034408
## poly(x, 10, raw = T)3
##              5.989367</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">beta <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 43  5  3  6</code></pre></div>
<p>We see that we actually have a pretty good set of coefficients.</p>

<h4 id="d-forward-and-backward-stepwise-models">d) Forward and backward stepwise models</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">modelX<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">poly</span>(x,<span style="color:#ae81ff">10</span>,raw<span style="color:#f92672">=</span>T)
forwardFit<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">regsubsets</span>(y<span style="color:#f92672">~</span>modelX,data<span style="color:#f92672">=</span>df,nvmax<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;forward&#34;</span>)
forwardFit <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Subset selection object
## Call: regsubsets.formula(y ~ modelX, data = df, nvmax = 10, method = &#34;forward&#34;)
## 10 Variables  (and intercept)
##          Forced in Forced out
## modelX1      FALSE      FALSE
## modelX2      FALSE      FALSE
## modelX3      FALSE      FALSE
## modelX4      FALSE      FALSE
## modelX5      FALSE      FALSE
## modelX6      FALSE      FALSE
## modelX7      FALSE      FALSE
## modelX8      FALSE      FALSE
## modelX9      FALSE      FALSE
## modelX10     FALSE      FALSE
## 1 subsets of each size up to 10
## Selection Algorithm: forward
##           modelX1 modelX2 modelX3 modelX4 modelX5 modelX6 modelX7 modelX8
## 1  ( 1 )  &#34; &#34;     &#34; &#34;     &#34;*&#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;
## 2  ( 1 )  &#34; &#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;
## 3  ( 1 )  &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;
## 4  ( 1 )  &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;
## 5  ( 1 )  &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;
## 6  ( 1 )  &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;
## 7  ( 1 )  &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34; &#34;
## 8  ( 1 )  &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34;*&#34;
## 9  ( 1 )  &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;
## 10  ( 1 ) &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;
##           modelX9 modelX10
## 1  ( 1 )  &#34; &#34;     &#34; &#34;
## 2  ( 1 )  &#34; &#34;     &#34; &#34;
## 3  ( 1 )  &#34; &#34;     &#34; &#34;
## 4  ( 1 )  &#34; &#34;     &#34; &#34;
## 5  ( 1 )  &#34; &#34;     &#34; &#34;
## 6  ( 1 )  &#34; &#34;     &#34;*&#34;
## 7  ( 1 )  &#34; &#34;     &#34;*&#34;
## 8  ( 1 )  &#34; &#34;     &#34;*&#34;
## 9  ( 1 )  &#34; &#34;     &#34;*&#34;
## 10  ( 1 ) &#34;*&#34;     &#34;*&#34;</code></pre></div>
<p>We might want to take a look at these.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(forwardFit)
<span style="color:#a6e22e">plot</span>(forwardFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Cp&#39;</span>)
<span style="color:#a6e22e">plot</span>(forwardFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r2&#39;</span>)
<span style="color:#a6e22e">plot</span>(forwardFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adjr2&#39;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-12-1.png"   />

        
    </figure>



<p>I find these not as fun to look at, so we will do better.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">plotLEAP<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(leapObj){
  <span style="color:#a6e22e">par</span>(mfrow <span style="color:#f92672">=</span> <span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
  bar2<span style="color:#f92672">=</span><span style="color:#a6e22e">which.max</span>(leapObj<span style="color:#f92672">$</span>adjr2)
  bbic<span style="color:#f92672">=</span><span style="color:#a6e22e">which.min</span>(leapObj<span style="color:#f92672">$</span>bic)
  bcp<span style="color:#f92672">=</span><span style="color:#a6e22e">which.min</span>(leapObj<span style="color:#f92672">$</span>cp)
  <span style="color:#a6e22e">plot</span>(leapObj<span style="color:#f92672">$</span>rss,xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Number of variables&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;RSS&#34;</span>,type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)
  <span style="color:#a6e22e">plot</span>(leapObj<span style="color:#f92672">$</span>adjr2,xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Number of variables&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#a6e22e">TeX</span>(<span style="color:#e6db74">&#34;Adjusted R^2&#34;</span>),type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)
  <span style="color:#a6e22e">points</span>(bar2,leapObj<span style="color:#f92672">$</span>adjr2[bar2],col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;green&#34;</span>,cex<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,pch<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
  <span style="color:#a6e22e">plot</span>(leapObj<span style="color:#f92672">$</span>bic,xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Number of variables&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#a6e22e">TeX</span>(<span style="color:#e6db74">&#34;BIC&#34;</span>),type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)
  <span style="color:#a6e22e">points</span>(bbic,leapObj<span style="color:#f92672">$</span>bic[bbic],col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;blue&#34;</span>,cex<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,pch<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
  <span style="color:#a6e22e">plot</span>(leapObj<span style="color:#f92672">$</span>cp,xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Number of variables&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#a6e22e">TeX</span>(<span style="color:#e6db74">&#34;C_p&#34;</span>),type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;b&#34;</span>)
  <span style="color:#a6e22e">points</span>(bcp,leapObj<span style="color:#f92672">$</span>cp[bcp],col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>,cex<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,pch<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
}</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plotLEAP</span>(forwardFit <span style="color:#f92672">%&gt;%</span> summary)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-14-1.png"   />

        
    </figure>



<p>Lets check the backward selection as well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">modelX<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">poly</span>(x,<span style="color:#ae81ff">10</span>,raw<span style="color:#f92672">=</span>T)
backwardFit<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">regsubsets</span>(y<span style="color:#f92672">~</span>modelX,data<span style="color:#f92672">=</span>df,nvmax<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;backward&#34;</span>)
backwardFit <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Subset selection object
## Call: regsubsets.formula(y ~ modelX, data = df, nvmax = 10, method = &#34;backward&#34;)
## 10 Variables  (and intercept)
##          Forced in Forced out
## modelX1      FALSE      FALSE
## modelX2      FALSE      FALSE
## modelX3      FALSE      FALSE
## modelX4      FALSE      FALSE
## modelX5      FALSE      FALSE
## modelX6      FALSE      FALSE
## modelX7      FALSE      FALSE
## modelX8      FALSE      FALSE
## modelX9      FALSE      FALSE
## modelX10     FALSE      FALSE
## 1 subsets of each size up to 10
## Selection Algorithm: backward
##           modelX1 modelX2 modelX3 modelX4 modelX5 modelX6 modelX7 modelX8
## 1  ( 1 )  &#34; &#34;     &#34; &#34;     &#34;*&#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;
## 2  ( 1 )  &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;
## 3  ( 1 )  &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;     &#34; &#34;
## 4  ( 1 )  &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34; &#34;     &#34; &#34;
## 5  ( 1 )  &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;
## 6  ( 1 )  &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;
## 7  ( 1 )  &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;
## 8  ( 1 )  &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;
## 9  ( 1 )  &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34; &#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;
## 10  ( 1 ) &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;     &#34;*&#34;
##           modelX9 modelX10
## 1  ( 1 )  &#34; &#34;     &#34; &#34;
## 2  ( 1 )  &#34; &#34;     &#34; &#34;
## 3  ( 1 )  &#34; &#34;     &#34; &#34;
## 4  ( 1 )  &#34; &#34;     &#34; &#34;
## 5  ( 1 )  &#34; &#34;     &#34; &#34;
## 6  ( 1 )  &#34; &#34;     &#34;*&#34;
## 7  ( 1 )  &#34; &#34;     &#34;*&#34;
## 8  ( 1 )  &#34; &#34;     &#34;*&#34;
## 9  ( 1 )  &#34;*&#34;     &#34;*&#34;
## 10  ( 1 ) &#34;*&#34;     &#34;*&#34;</code></pre></div>
<p>We might want to take a look at these.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(backwardFit)
<span style="color:#a6e22e">plot</span>(backwardFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Cp&#39;</span>)
<span style="color:#a6e22e">plot</span>(backwardFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r2&#39;</span>)
<span style="color:#a6e22e">plot</span>(backwardFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adjr2&#39;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-16-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plotLEAP</span>(backwardFit <span style="color:#f92672">%&gt;%</span> summary)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-17-1.png"   />

        
    </figure>



<p>In spite of some slight variations, overall all methods converge to the
same <code>best</code> set of parameters, that of the third model.</p>

<h4 id="e-lasso-and-cross-validation">e) LASSO and Cross Validation</h4>

<p>For this, instead of using <code>glmnet</code> directly, we will use <code>caret</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">df<span style="color:#f92672">&lt;-</span>df <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">mutate</span>(x2<span style="color:#f92672">=</span>x^2,x3<span style="color:#f92672">=</span>x^3,
                  x4<span style="color:#f92672">=</span>x^4,x5<span style="color:#f92672">=</span>x^5,
                  x6<span style="color:#f92672">=</span>x^6,x7<span style="color:#f92672">=</span>x^7,
                  x8<span style="color:#f92672">=</span>x^8,x9<span style="color:#f92672">=</span>x^9,
                  x10<span style="color:#f92672">=</span>x^10)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lambda<span style="color:#f92672">&lt;-</span><span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(<span style="color:#ae81ff">-3</span>, <span style="color:#ae81ff">3</span>, length <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>)
lassoCaret<span style="color:#f92672">=</span> <span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>df,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;glmnet&#34;</span>,tuneGrid<span style="color:#f92672">=</span><span style="color:#a6e22e">expand.grid</span>(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,lambda<span style="color:#f92672">=</span>lambda))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCaret <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## glmnet
##
## 100 samples
##  10 predictor
##
## No pre-processing
## Resampling: Bootstrapped (25 reps)
## Summary of sample sizes: 100, 100, 100, 100, 100, 100, ...
## Resampling results across tuning parameters:
##
##   lambda        RMSE       Rsquared   MAE
##   1.000000e-03   1.009696  0.9965632   0.8051425
##   1.149757e-03   1.009696  0.9965632   0.8051425
##   1.321941e-03   1.009696  0.9965632   0.8051425
##   1.519911e-03   1.009696  0.9965632   0.8051425
##   1.747528e-03   1.009696  0.9965632   0.8051425
##   2.009233e-03   1.009696  0.9965632   0.8051425
##   2.310130e-03   1.009696  0.9965632   0.8051425
##   2.656088e-03   1.009696  0.9965632   0.8051425
##   3.053856e-03   1.009696  0.9965632   0.8051425
##   3.511192e-03   1.009696  0.9965632   0.8051425
##   4.037017e-03   1.009696  0.9965632   0.8051425
##   4.641589e-03   1.009696  0.9965632   0.8051425
##   5.336699e-03   1.009696  0.9965632   0.8051425
##   6.135907e-03   1.009696  0.9965632   0.8051425
##   7.054802e-03   1.009696  0.9965632   0.8051425
##   8.111308e-03   1.009696  0.9965632   0.8051425
##   9.326033e-03   1.009696  0.9965632   0.8051425
##   1.072267e-02   1.009696  0.9965632   0.8051425
##   1.232847e-02   1.009696  0.9965632   0.8051425
##   1.417474e-02   1.009696  0.9965632   0.8051425
##   1.629751e-02   1.009696  0.9965632   0.8051425
##   1.873817e-02   1.009696  0.9965632   0.8051425
##   2.154435e-02   1.009696  0.9965632   0.8051425
##   2.477076e-02   1.009696  0.9965632   0.8051425
##   2.848036e-02   1.009696  0.9965632   0.8051425
##   3.274549e-02   1.009696  0.9965632   0.8051425
##   3.764936e-02   1.009696  0.9965632   0.8051425
##   4.328761e-02   1.009696  0.9965632   0.8051425
##   4.977024e-02   1.009696  0.9965632   0.8051425
##   5.722368e-02   1.009696  0.9965632   0.8051425
##   6.579332e-02   1.009696  0.9965632   0.8051425
##   7.564633e-02   1.009637  0.9965632   0.8050666
##   8.697490e-02   1.009216  0.9965637   0.8047862
##   1.000000e-01   1.008901  0.9965636   0.8046468
##   1.149757e-01   1.009470  0.9965616   0.8054790
##   1.321941e-01   1.011206  0.9965561   0.8074253
##   1.519911e-01   1.014475  0.9965476   0.8104930
##   1.747528e-01   1.019202  0.9965383   0.8147296
##   2.009233e-01   1.025943  0.9965259   0.8203974
##   2.310130e-01   1.035374  0.9965094   0.8284187
##   2.656088e-01   1.048294  0.9964878   0.8393282
##   3.053856e-01   1.065717  0.9964592   0.8530952
##   3.511192e-01   1.088903  0.9964215   0.8701072
##   4.037017e-01   1.119433  0.9963715   0.8918217
##   4.641589e-01   1.158919  0.9963053   0.9193677
##   5.336699e-01   1.209841  0.9962136   0.9532842
##   6.135907e-01   1.275467  0.9960778   0.9957151
##   7.054802e-01   1.357247  0.9958966   1.0471169
##   8.111308e-01   1.457886  0.9956561   1.1087362
##   9.326033e-01   1.580743  0.9953362   1.1818188
##   1.072267e+00   1.729330  0.9949070   1.2696235
##   1.232847e+00   1.907599  0.9943306   1.3758463
##   1.417474e+00   2.120178  0.9935518   1.5059031
##   1.629751e+00   2.369642  0.9924954   1.6673393
##   1.873817e+00   2.662906  0.9910539   1.8621728
##   2.154435e+00   3.007271  0.9890638   2.0978907
##   2.477076e+00   3.409377  0.9863097   2.3788439
##   2.848036e+00   3.864727  0.9825900   2.7053428
##   3.274549e+00   4.350785  0.9778541   3.0659309
##   3.764936e+00   4.847045  0.9724311   3.4403210
##   4.328761e+00   5.369017  0.9668240   3.8351441
##   4.977024e+00   5.919492  0.9626812   4.2512694
##   5.722368e+00   6.562134  0.9580843   4.7389049
##   6.579332e+00   7.307112  0.9534537   5.2945905
##   7.564633e+00   8.132296  0.9500300   5.8774541
##   8.697490e+00   9.067321  0.9486589   6.4760997
##   1.000000e+01  10.167822  0.9483195   7.1226569
##   1.149757e+01  11.473284  0.9482975   7.8556639
##   1.321941e+01  13.002703  0.9482975   8.6990451
##   1.519911e+01  14.727852  0.9454119   9.6414650
##   1.747528e+01  16.325210  0.9426796  10.5303097
##   2.009233e+01  17.740599  0.9357286  11.3560865
##   2.310130e+01  18.585795  0.9227167  11.8799668
##   2.656088e+01  18.939596  0.9080584  12.1336575
##   3.053856e+01  19.123568  0.9109065  12.2733471
##   3.511192e+01  19.197966        NaN  12.3308613
##   4.037017e+01  19.197966        NaN  12.3308613
##   4.641589e+01  19.197966        NaN  12.3308613
##   5.336699e+01  19.197966        NaN  12.3308613
##   6.135907e+01  19.197966        NaN  12.3308613
##   7.054802e+01  19.197966        NaN  12.3308613
##   8.111308e+01  19.197966        NaN  12.3308613
##   9.326033e+01  19.197966        NaN  12.3308613
##   1.072267e+02  19.197966        NaN  12.3308613
##   1.232847e+02  19.197966        NaN  12.3308613
##   1.417474e+02  19.197966        NaN  12.3308613
##   1.629751e+02  19.197966        NaN  12.3308613
##   1.873817e+02  19.197966        NaN  12.3308613
##   2.154435e+02  19.197966        NaN  12.3308613
##   2.477076e+02  19.197966        NaN  12.3308613
##   2.848036e+02  19.197966        NaN  12.3308613
##   3.274549e+02  19.197966        NaN  12.3308613
##   3.764936e+02  19.197966        NaN  12.3308613
##   4.328761e+02  19.197966        NaN  12.3308613
##   4.977024e+02  19.197966        NaN  12.3308613
##   5.722368e+02  19.197966        NaN  12.3308613
##   6.579332e+02  19.197966        NaN  12.3308613
##   7.564633e+02  19.197966        NaN  12.3308613
##   8.697490e+02  19.197966        NaN  12.3308613
##   1.000000e+03  19.197966        NaN  12.3308613
##
## Tuning parameter &#39;alpha&#39; was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.1.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCaret  <span style="color:#f92672">%&gt;%</span> ggplot</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-20-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCaret <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-21-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(glmnet)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Loading required package: Matrix</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;Matrix&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:tidyr&#39;:
##
##     expand, pack, unpack</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Loaded glmnet 3.0-2</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(boot)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;boot&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:lattice&#39;:
##
##     melanoma</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lasso.mod <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cv.glmnet</span>(<span style="color:#a6e22e">as.matrix</span>(df[<span style="color:#ae81ff">-1</span>]), y, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
lambda <span style="color:#f92672">&lt;-</span> lasso.mod<span style="color:#f92672">$</span>lambda.min
<span style="color:#a6e22e">plot</span>(lasso.mod)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-22-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">predict</span>(lasso.mod, s<span style="color:#f92672">=</span>lambda, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;coefficients&#34;</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## 11 x 1 sparse Matrix of class &#34;dgCMatrix&#34;
##                     1
## (Intercept) 42.975240
## x            5.005023
## x2           2.947540
## x3           5.989105
## x4           .
## x5           .
## x6           .
## x7           .
## x8           .
## x9           .
## x10          .</code></pre></div>
<p>Clearly, the only important variables are \(x\), \(x^2\) and \(x^3\).</p>

<h4 id="f-new-model">f) New model</h4>

<p>Our new model requires a newly expanded set of betas as well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">y2<span style="color:#f92672">&lt;-</span>beta[1]<span style="color:#ae81ff">+23</span><span style="color:#f92672">*</span>x^7<span style="color:#f92672">+</span>noise</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">modelX<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">poly</span>(x,<span style="color:#ae81ff">10</span>,raw<span style="color:#f92672">=</span>T)
newDF<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">data.frame</span>(x<span style="color:#f92672">=</span><span style="color:#a6e22e">as.matrix</span>(modelX),y<span style="color:#f92672">=</span>y2)
newSub<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">regsubsets</span>(y2<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>newDF,nvmax<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
newSub <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Subset selection object
## Call: regsubsets.formula(y2 ~ ., data = newDF, nvmax = 10)
## 11 Variables  (and intercept)
##      Forced in Forced out
## x.1      FALSE      FALSE
## x.2      FALSE      FALSE
## x.3      FALSE      FALSE
## x.4      FALSE      FALSE
## x.5      FALSE      FALSE
## x.6      FALSE      FALSE
## x.7      FALSE      FALSE
## x.8      FALSE      FALSE
## x.9      FALSE      FALSE
## x.10     FALSE      FALSE
## y        FALSE      FALSE
## 1 subsets of each size up to 10
## Selection Algorithm: exhaustive
##           x.1 x.2 x.3 x.4 x.5 x.6 x.7 x.8 x.9 x.10 y
## 1  ( 1 )  &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34;*&#34;
## 2  ( 1 )  &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34;*&#34;
## 3  ( 1 )  &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34;*&#34;
## 4  ( 1 )  &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34;*&#34;
## 5  ( 1 )  &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;
## 6  ( 1 )  &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34;*&#34;
## 7  ( 1 )  &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34;*&#34;
## 8  ( 1 )  &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34;*&#34;
## 9  ( 1 )  &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34; &#34;  &#34;*&#34;
## 10  ( 1 ) &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plotLEAP</span>(newSub <span style="color:#f92672">%&gt;%</span> summary)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-25-1.png"   />

        
    </figure>



<p>Or in its more native look,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(newSub)
<span style="color:#a6e22e">plot</span>(newSub,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Cp&#39;</span>)
<span style="color:#a6e22e">plot</span>(newSub,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r2&#39;</span>)
<span style="color:#a6e22e">plot</span>(newSub,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adjr2&#39;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-26-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(glmnet)
<span style="color:#a6e22e">library</span>(boot)
lasso.mod2 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">cv.glmnet</span>(<span style="color:#a6e22e">as.matrix</span>(newDF[<span style="color:#ae81ff">-1</span>]), y, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
lambda2 <span style="color:#f92672">&lt;-</span> lasso.mod2<span style="color:#f92672">$</span>lambda.min
<span style="color:#a6e22e">plot</span>(lasso.mod2)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-27-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">predict</span>(lasso.mod2, s<span style="color:#f92672">=</span>lambda, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;coefficients&#34;</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## 11 x 1 sparse Matrix of class &#34;dgCMatrix&#34;
##                       1
## (Intercept) 42.67982691
## x.2          3.22521396
## x.3          8.56699146
## x.4          .
## x.5         -0.10229572
## x.6          .
## x.7         -0.03184905
## x.8          .
## x.9          .
## x.10         .
## y            .</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lambda<span style="color:#f92672">&lt;-</span><span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(<span style="color:#ae81ff">-3</span>, <span style="color:#ae81ff">3</span>, length <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>)
lassocaret2<span style="color:#f92672">=</span> <span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>newDF,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;glmnet&#34;</span>,tuneGrid<span style="color:#f92672">=</span><span style="color:#a6e22e">expand.grid</span>(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,lambda<span style="color:#f92672">=</span>lambda))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassocaret2 <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## glmnet
##
## 100 samples
##  10 predictor
##
## No pre-processing
## Resampling: Bootstrapped (25 reps)
## Summary of sample sizes: 100, 100, 100, 100, 100, 100, ...
## Resampling results across tuning parameters:
##
##   lambda        RMSE        Rsquared   MAE
##   1.000000e-03    40.03231  0.9999955   14.48774
##   1.149757e-03    40.03231  0.9999955   14.48774
##   1.321941e-03    40.03231  0.9999955   14.48774
##   1.519911e-03    40.03231  0.9999955   14.48774
##   1.747528e-03    40.03231  0.9999955   14.48774
##   2.009233e-03    40.03231  0.9999955   14.48774
##   2.310130e-03    40.03231  0.9999955   14.48774
##   2.656088e-03    40.03231  0.9999955   14.48774
##   3.053856e-03    40.03231  0.9999955   14.48774
##   3.511192e-03    40.03231  0.9999955   14.48774
##   4.037017e-03    40.03231  0.9999955   14.48774
##   4.641589e-03    40.03231  0.9999955   14.48774
##   5.336699e-03    40.03231  0.9999955   14.48774
##   6.135907e-03    40.03231  0.9999955   14.48774
##   7.054802e-03    40.03231  0.9999955   14.48774
##   8.111308e-03    40.03231  0.9999955   14.48774
##   9.326033e-03    40.03231  0.9999955   14.48774
##   1.072267e-02    40.03231  0.9999955   14.48774
##   1.232847e-02    40.03231  0.9999955   14.48774
##   1.417474e-02    40.03231  0.9999955   14.48774
##   1.629751e-02    40.03231  0.9999955   14.48774
##   1.873817e-02    40.03231  0.9999955   14.48774
##   2.154435e-02    40.03231  0.9999955   14.48774
##   2.477076e-02    40.03231  0.9999955   14.48774
##   2.848036e-02    40.03231  0.9999955   14.48774
##   3.274549e-02    40.03231  0.9999955   14.48774
##   3.764936e-02    40.03231  0.9999955   14.48774
##   4.328761e-02    40.03231  0.9999955   14.48774
##   4.977024e-02    40.03231  0.9999955   14.48774
##   5.722368e-02    40.03231  0.9999955   14.48774
##   6.579332e-02    40.03231  0.9999955   14.48774
##   7.564633e-02    40.03231  0.9999955   14.48774
##   8.697490e-02    40.03231  0.9999955   14.48774
##   1.000000e-01    40.03231  0.9999955   14.48774
##   1.149757e-01    40.03231  0.9999955   14.48774
##   1.321941e-01    40.03231  0.9999955   14.48774
##   1.519911e-01    40.03231  0.9999955   14.48774
##   1.747528e-01    40.03231  0.9999955   14.48774
##   2.009233e-01    40.03231  0.9999955   14.48774
##   2.310130e-01    40.03231  0.9999955   14.48774
##   2.656088e-01    40.03231  0.9999955   14.48774
##   3.053856e-01    40.03231  0.9999955   14.48774
##   3.511192e-01    40.03231  0.9999955   14.48774
##   4.037017e-01    40.03231  0.9999955   14.48774
##   4.641589e-01    40.03231  0.9999955   14.48774
##   5.336699e-01    40.03231  0.9999955   14.48774
##   6.135907e-01    40.03231  0.9999955   14.48774
##   7.054802e-01    40.03231  0.9999955   14.48774
##   8.111308e-01    40.03231  0.9999955   14.48774
##   9.326033e-01    40.03231  0.9999955   14.48774
##   1.072267e+00    40.03231  0.9999955   14.48774
##   1.232847e+00    40.03231  0.9999955   14.48774
##   1.417474e+00    40.03231  0.9999955   14.48774
##   1.629751e+00    40.03231  0.9999955   14.48774
##   1.873817e+00    40.03231  0.9999955   14.48774
##   2.154435e+00    40.03231  0.9999955   14.48774
##   2.477076e+00    40.03231  0.9999955   14.48774
##   2.848036e+00    40.03231  0.9999955   14.48774
##   3.274549e+00    40.03231  0.9999955   14.48774
##   3.764936e+00    40.03231  0.9999955   14.48774
##   4.328761e+00    40.03231  0.9999955   14.48774
##   4.977024e+00    40.03231  0.9999955   14.48774
##   5.722368e+00    40.03231  0.9999955   14.48774
##   6.579332e+00    40.03231  0.9999955   14.48774
##   7.564633e+00    40.43005  0.9999955   14.59881
##   8.697490e+00    41.25214  0.9999955   14.81913
##   1.000000e+01    42.30446  0.9999955   15.09937
##   1.149757e+01    43.59429  0.9999955   15.44307
##   1.321941e+01    45.43633  0.9999955   15.93255
##   1.519911e+01    47.55425  0.9999955   16.49605
##   1.747528e+01    49.98935  0.9999955   17.14447
##   2.009233e+01    52.90533  0.9999955   17.91650
##   2.310130e+01    57.57589  0.9999955   19.10125
##   2.656088e+01    63.25484  0.9999955   20.53147
##   3.053856e+01    70.51580  0.9999955   22.36400
##   3.511192e+01    78.93391  0.9999955   24.49105
##   4.037017e+01    88.61274  0.9999955   26.93830
##   4.641589e+01    99.97831  0.9999955   29.83601
##   5.336699e+01   113.48225  0.9999955   33.39320
##   6.135907e+01   129.17536  0.9999955   37.58303
##   7.054802e+01   147.76452  0.9999957   42.74333
##   8.111308e+01   169.60027  0.9999961   48.98043
##   9.326033e+01   194.94266  0.9999965   56.29001
##   1.072267e+02   224.07631  0.9999969   64.70026
##   1.232847e+02   257.56092  0.9999971   74.36989
##   1.417474e+02   296.13382  0.9999971   85.51504
##   1.629751e+02   340.49129  0.9999971   98.33212
##   1.873817e+02   391.49185  0.9999971  113.06864
##   2.154435e+02   450.13031  0.9999971  130.01206
##   2.477076e+02   509.28329  0.9999970  147.15405
##   2.848036e+02   564.17558  0.9999969  163.34475
##   3.274549e+02   618.84080  0.9999969  179.85589
##   3.764936e+02   681.69265  0.9999969  198.83969
##   4.328761e+02   741.14452  0.9999967  217.28049
##   4.977024e+02   807.25385  0.9999967  237.88938
##   5.722368e+02   883.26360  0.9999967  261.58461
##   6.579332e+02   970.65640  0.9999967  288.82836
##   7.564633e+02  1037.84801  0.9999960  312.54099
##   8.697490e+02  1088.92551  0.9999960  334.04769
##   1.000000e+03  1131.46176  0.9999955  354.62317
##
## Tuning parameter &#39;alpha&#39; was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 6.579332.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassocaret2  <span style="color:#f92672">%&gt;%</span> ggplot</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-29-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassocaret2 <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-30-1.png"   />

        
    </figure>



<p>Clearly, the LASSO model has correctly reduced the model down to the
correct single variable form, though best subset seems to suggest using
more predictors, their coefficients are low enough to recognize that
they are noise.</p>

<h2 id="question-6.9---page-263">Question 6.9 - Page 263</h2>

<p>In this exercise, we will predict the number of applications received
using the other variables in the <code>College</code> data set.</p>

<p><strong>(a)</strong> Split the data set into a training set and a test set.</p>

<p><strong>(b)</strong> Fit a linear model using least squares on the training set, and
report the test error obtained.</p>

<p><strong>&copy;</strong> Fit a ridge regression model on the training set, with \(\lambda\)
chosen by cross-validation. Report the test error obtained.</p>

<p><strong>(d)</strong> Fit a lasso model on the training set, with \(\lambda\) chosen by
crossvalidation. Report the test error obtained, along with the number
of non-zero coefficient estimates.</p>

<p><strong>(e)</strong> Fit a PCR model on the training set, with \(M\) chosen by
crossvalidation. Report the test error obtained, along with the value of
\(M\) selected by cross-validation.</p>

<p><strong>(f)</strong> Fit a PLS model on the training set, with M chosen by
crossvalidation. Report the test error obtained, along with the value of
M selected by cross-validation.</p>

<p><strong>(g)</strong> Comment on the results obtained. How accurately can we predict the
number of college applications received? Is there much difference among
the test errors resulting from these five approaches?</p>

<h3 id="answer-1">Answer</h3>

<p>We will use the <code>caret</code> package, since at the moment, <code>mlr3</code> does not
have learners for <code>PCR</code> and <code>PLS</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">colDat<span style="color:#f92672">&lt;-</span>ISLR<span style="color:#f92672">::</span>College
colDat <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##  Private        Apps           Accept          Enroll       Top10perc
##  No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00
##  Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00
##            Median : 1558   Median : 1110   Median : 434   Median :23.00
##            Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56
##            3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00
##            Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00
##    Top25perc      F.Undergrad     P.Undergrad         Outstate
##  Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340
##  1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320
##  Median : 54.0   Median : 1707   Median :  353.0   Median : 9990
##  Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441
##  3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925
##  Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700
##    Room.Board       Books           Personal         PhD
##  Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00
##  1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00
##  Median :4200   Median : 500.0   Median :1200   Median : 75.00
##  Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66
##  3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00
##  Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00
##     Terminal       S.F.Ratio      perc.alumni        Expend
##  Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186
##  1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751
##  Median : 82.0   Median :13.60   Median :21.00   Median : 8377
##  Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660
##  3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830
##  Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233
##    Grad.Rate
##  Min.   : 10.00
##  1st Qu.: 53.00
##  Median : 65.00
##  Mean   : 65.46
##  3rd Qu.: 78.00
##  Max.   :118.00</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">colDat <span style="color:#f92672">%&gt;%</span> str <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    777 obs. of  18 variables:
##  $ Private    : Factor w/ 2 levels &#34;No&#34;,&#34;Yes&#34;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ Apps       : num  1660 2186 1428 417 193 ...
##  $ Accept     : num  1232 1924 1097 349 146 ...
##  $ Enroll     : num  721 512 336 137 55 158 103 489 227 172 ...
##  $ Top10perc  : num  23 16 22 60 16 38 17 37 30 21 ...
##  $ Top25perc  : num  52 29 50 89 44 62 45 68 63 44 ...
##  $ F.Undergrad: num  2885 2683 1036 510 249 ...
##  $ P.Undergrad: num  537 1227 99 63 869 ...
##  $ Outstate   : num  7440 12280 11250 12960 7560 ...
##  $ Room.Board : num  3300 6450 3750 5450 4120 ...
##  $ Books      : num  450 750 400 450 800 500 500 450 300 660 ...
##  $ Personal   : num  2200 1500 1165 875 1500 ...
##  $ PhD        : num  70 29 53 92 76 67 90 89 79 40 ...
##  $ Terminal   : num  78 30 66 97 72 73 93 100 84 41 ...
##  $ S.F.Ratio  : num  18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ...
##  $ perc.alumni: num  12 16 30 37 2 11 26 37 23 15 ...
##  $ Expend     : num  7041 10527 8735 19016 10922 ...
##  $ Grad.Rate  : num  60 56 54 59 15 55 63 73 80 52 ...
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">colDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(unique) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(length) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##     Private        Apps      Accept      Enroll   Top10perc   Top25perc
##           2         711         693         581          82          89
## F.Undergrad P.Undergrad    Outstate  Room.Board       Books    Personal
##         714         566         640         553         122         294
##         PhD    Terminal   S.F.Ratio perc.alumni      Expend   Grad.Rate
##          78          65         173          61         744          81</code></pre></div>
<p>Clearly, there are no psuedo-factors which might have been converted at
this stage.</p>

<h4 id="a-train-test-split">a) Train-Test split</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">train_ind<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">createDataPartition</span>(colDat<span style="color:#f92672">$</span>Apps,p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>,times<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,list<span style="color:#f92672">=</span><span style="color:#66d9ef">FALSE</span>)
train_set<span style="color:#f92672">&lt;-</span>colDat[train_ind,]
test_set<span style="color:#f92672">&lt;-</span>colDat[<span style="color:#f92672">-</span>train_ind,]</code></pre></div>
<h4 id="b-linear-least-squares">b) Linear least squares</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">linCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(Apps<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>)
linCol <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = .outcome ~ ., data = dat)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -5145.6  -414.8   -20.3   340.5  7526.8
##
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -2.918e+02  4.506e+02  -0.648 0.517486
## PrivateYes  -5.351e+02  1.532e+02  -3.494 0.000511 ***
## Accept       1.617e+00  4.258e-02  37.983  &lt; 2e-16 ***
## Enroll      -1.012e+00  1.959e-01  -5.165 3.26e-07 ***
## Top10perc    5.379e+01  6.221e+00   8.647  &lt; 2e-16 ***
## Top25perc   -1.632e+01  5.046e+00  -3.235 0.001282 **
## F.Undergrad  6.836e-02  3.457e-02   1.978 0.048410 *
## P.Undergrad  7.929e-02  3.367e-02   2.355 0.018854 *
## Outstate    -7.303e-02  2.098e-02  -3.481 0.000536 ***
## Room.Board   1.695e-01  5.367e-02   3.159 0.001663 **
## Books        9.998e-02  2.578e-01   0.388 0.698328
## Personal    -3.145e-03  6.880e-02  -0.046 0.963553
## PhD         -8.926e+00  5.041e+00  -1.771 0.077112 .
## Terminal    -2.298e+00  5.608e+00  -0.410 0.682152
## S.F.Ratio    6.038e+00  1.420e+01   0.425 0.670757
## perc.alumni -5.085e-01  4.560e+00  -0.112 0.911249
## Expend       4.668e-02  1.332e-02   3.505 0.000490 ***
## Grad.Rate    9.042e+00  3.379e+00   2.676 0.007653 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 1042 on 606 degrees of freedom
## Multiple R-squared:  0.9332, Adjusted R-squared:  0.9313
## F-statistic: 497.7 on 17 and 606 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">linPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(linCol,test_set)
linPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>Apps)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##         RMSE     Rsquared          MAE
## 1071.6360025    0.9017032  625.7827996</code></pre></div>
<p>Do note that the
<a href="https://topepo.github.io/caret/measuring-performance.html#measures-for-regression" target="_blank">metrics
are calculated</a> in a manner to ensure no negative values are obtained.</p>

<h4 id="c-ridge-regression-with-cv-for-λ">c) Ridge regression with CV for λ</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">L2Grid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,
                          lambda<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">-3</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(Apps<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;glmnet&#34;</span>,tuneGrid <span style="color:#f92672">=</span> L2Grid)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##             Length Class      Mode
## a0           100   -none-     numeric
## beta        1700   dgCMatrix  S4
## df           100   -none-     numeric
## dim            2   -none-     numeric
## lambda       100   -none-     numeric
## dev.ratio    100   -none-     numeric
## nulldev        1   -none-     numeric
## npasses        1   -none-     numeric
## jerr           1   -none-     numeric
## offset         1   -none-     logical
## call           5   -none-     call
## nobs           1   -none-     numeric
## lambdaOpt      1   -none-     numeric
## xNames        17   -none-     character
## problemType    1   -none-     character
## tuneValue      2   data.frame list
## obsLevels      1   -none-     logical
## param          0   -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coef</span>(ridgCol<span style="color:#f92672">$</span>finalModel, ridgCol<span style="color:#f92672">$</span>bestTune<span style="color:#f92672">$</span>lambda) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## 18 x 1 sparse Matrix of class &#34;dgCMatrix&#34;
##                         1
## (Intercept) -1.407775e+03
## PrivateYes  -5.854245e+02
## Accept       1.042778e+00
## Enroll       3.511219e-01
## Top10perc    2.780211e+01
## Top25perc    2.883536e-02
## F.Undergrad  6.825141e-02
## P.Undergrad  5.281320e-02
## Outstate    -2.011504e-02
## Room.Board   2.155224e-01
## Books        1.517585e-01
## Personal    -3.711406e-02
## PhD         -4.453155e+00
## Terminal    -3.783231e+00
## S.F.Ratio    6.897360e+00
## perc.alumni -9.301831e+00
## Expend       5.601144e-02
## Grad.Rate    1.259989e+01</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(ridgCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-38-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(ridgCol,test_set)
ridgPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>Apps)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##         RMSE     Rsquared          MAE
## 1047.7545250    0.9051726  644.4535063</code></pre></div>
<h4 id="d-lasso-with-cv-for-λ">d) LASSO with CV for λ</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">L1Grid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, <span style="color:#75715e"># for lasso</span>
                          lambda<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">-3</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(Apps<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;glmnet&#34;</span>,tuneGrid <span style="color:#f92672">=</span> L1Grid)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##             Length Class      Mode
## a0            81   -none-     numeric
## beta        1377   dgCMatrix  S4
## df            81   -none-     numeric
## dim            2   -none-     numeric
## lambda        81   -none-     numeric
## dev.ratio     81   -none-     numeric
## nulldev        1   -none-     numeric
## npasses        1   -none-     numeric
## jerr           1   -none-     numeric
## offset         1   -none-     logical
## call           5   -none-     call
## nobs           1   -none-     numeric
## lambdaOpt      1   -none-     numeric
## xNames        17   -none-     character
## problemType    1   -none-     character
## tuneValue      2   data.frame list
## obsLevels      1   -none-     logical
## param          0   -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coef</span>(lassoCol<span style="color:#f92672">$</span>finalModel, lassoCol<span style="color:#f92672">$</span>bestTune<span style="color:#f92672">$</span>lambda) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## 18 x 1 sparse Matrix of class &#34;dgCMatrix&#34;
##                         1
## (Intercept) -325.51554340
## PrivateYes  -532.28956305
## Accept         1.60370798
## Enroll        -0.90158328
## Top10perc     51.96610325
## Top25perc    -14.87886847
## F.Undergrad    0.05352324
## P.Undergrad    0.07832395
## Outstate      -0.07047302
## Room.Board     0.16783269
## Books          0.08836704
## Personal       .
## PhD           -8.67634519
## Terminal      -2.18494018
## S.F.Ratio      5.25050018
## perc.alumni   -0.67848535
## Expend         0.04597728
## Grad.Rate      8.67569015</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(lassoCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-43-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(lassoCol,test_set)
lassoPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>Apps)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##         RMSE     Rsquared          MAE
## 1068.9834769    0.9021268  622.7029418</code></pre></div>
<h4 id="e-pcr-with-cv-for-m">e) PCR with CV for M</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">mGrid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(ncomp<span style="color:#f92672">=</span><span style="color:#a6e22e">seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">pcrCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(Apps<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pcr&#34;</span>,tuneGrid <span style="color:#f92672">=</span> mGrid)
pcrCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Data:    X dimension: 624 17
##  Y dimension: 624 1
## Fit method: svdpc
## Number of components considered: 17
## TRAINING: % variance explained
##           1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X         48.2314    87.24    95.02    97.26    98.63    99.43    99.91
## .outcome   0.2419    76.54    77.88    80.19    91.27    91.34    91.34
##           8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X           99.96   100.00    100.00    100.00    100.00    100.00    100.00
## .outcome    91.65    91.66     92.26     92.65     92.66     92.67     92.76
##           15 comps  16 comps  17 comps
## X           100.00    100.00    100.00
## .outcome     93.17     93.18     93.32
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(pcrCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-47-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">pcrPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(pcrCol,test_set)
pcrPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>Apps)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##         RMSE     Rsquared          MAE
## 1071.6360025    0.9017032  625.7827996</code></pre></div>
<h4 id="f-pls-with-cv-for-m">f) PLS with CV for M</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">plsCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(Apps<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pls&#34;</span>,tuneGrid <span style="color:#f92672">=</span> mGrid)
plsCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Data:    X dimension: 624 17
##  Y dimension: 624 1
## Fit method: oscorespls
## Number of components considered: 17
## TRAINING: % variance explained
##           1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X           39.02     56.4    91.83    96.61    98.62    99.22    99.49
## .outcome    78.04     84.1    86.88    91.09    91.38    91.49    91.66
##           8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X           99.96    99.99    100.00    100.00    100.00    100.00    100.00
## .outcome    91.68    91.85     92.64     92.87     93.16     93.18     93.18
##           15 comps  16 comps  17 comps
## X           100.00    100.00    100.00
## .outcome     93.18     93.19     93.32
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(plsCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-50-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">plsPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(plsCol,test_set)
plsPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>Apps)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##         RMSE     Rsquared          MAE
## 1071.6360039    0.9017032  625.7827987</code></pre></div>
<h4 id="g-comments-and-comparison">g) Comments and Comparison</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">models <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">list</span>(ridge <span style="color:#f92672">=</span> ridgCol, lasso <span style="color:#f92672">=</span> lassoCol, pcr <span style="color:#f92672">=</span> pcrCol, pls<span style="color:#f92672">=</span>plsCol,linear<span style="color:#f92672">=</span>linCol)
<span style="color:#a6e22e">resamples</span>(models) <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## summary.resamples(object = .)
##
## Models: ridge, lasso, pcr, pls, linear
## Number of resamples: 25
##
## MAE
##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## ridge  536.9612 600.5398 623.2005 649.6713 707.4014 793.4972    0
## lasso  573.8563 616.3883 671.9453 655.8858 691.7620 732.2155    0
## pcr    576.1427 618.8694 650.0360 662.9040 714.8491 767.5535    0
## pls    553.3999 607.9757 637.1985 638.6619 668.5120 735.4479    0
## linear 556.5553 619.2395 654.1478 659.4635 686.7747 792.4912    0
##
## RMSE
##            Min.   1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## ridge  882.2646  920.5934 1000.519 1168.603 1163.377 1939.541    0
## lasso  801.9415  990.0724 1168.234 1184.329 1302.221 1584.712    0
## pcr    828.1370  942.2678 1131.207 1144.071 1284.178 1544.078    0
## pls    786.7989 1038.3265 1167.764 1157.026 1274.041 1461.434    0
## linear 798.3771 1063.3690 1134.291 1135.977 1215.115 1403.576    0
##
## Rsquared
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## ridge  0.8735756 0.8962010 0.9185736 0.9136429 0.9306819 0.9474913    0
## lasso  0.8851991 0.9132766 0.9217660 0.9191638 0.9284838 0.9398772    0
## pcr    0.8658504 0.9080179 0.9235117 0.9146884 0.9281892 0.9471991    0
## pls    0.8881249 0.9080786 0.9183968 0.9173632 0.9258994 0.9420894    0
## linear 0.8840049 0.8986452 0.9222319 0.9160913 0.9296275 0.9492198    0</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">resamples</span>(models) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">bwplot</span>(scales<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;free&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-53-1.png"   />

        
    </figure>



<ul>
<li>Given the tighter spread of <code>PLS</code>, it seems more reliable than <code>PCR</code></li>
<li><code>Ridge</code> is just poor in every way</li>
<li><code>OLS</code> does well, but it also has a worrying outlier</li>
<li><code>LASSO</code> appears to be doing alright as well We also have kept track of
the performance on the <code>test_set</code></li>
</ul>

<p>We might want to see the variable significance values as well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lgp<span style="color:#f92672">&lt;-</span>linCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;OLS Variable Importance&#34;</span>)
rgp<span style="color:#f92672">&lt;-</span>ridgCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;Ridge Variable Importance&#34;</span>)
lsgp<span style="color:#f92672">&lt;-</span>lassoCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;Lasso Variable Importance&#34;</span>)
pcgp<span style="color:#f92672">&lt;-</span>pcrCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;PCR Variable Importance&#34;</span>)
plgp<span style="color:#f92672">&lt;-</span>plsCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;PLS Variable Importance&#34;</span>)
<span style="color:#a6e22e">grid.arrange</span>(lgp,rgp,lsgp,pcgp,plgp,ncol<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-54-1.png"   />

        
    </figure>



<h2 id="question-6.10---pages-263-264">Question 6.10 - Pages 263-264</h2>

<p>We have seen that as the number of features used in a model increases,
the training error will necessarily decrease, but the test error may
not. We will now explore this in a simulated data set.</p>

<p><strong>(a)</strong> Generate a data set with \(p = 20\) features, \(n = 1,000\)
observations, and an associated quantitative response vector generated
according to the model \[Y = X\beta + \eta,\] where \(\beta\) has some
elements that are exactly equal to zero.</p>

<p><strong>(b)</strong> Split your data set into a training set containing \(100\)
observations and a test set containing \(900\) observations.</p>

<p><strong>&copy;</strong> Perform best subset selection on the training set, and plot the
training set MSE associated with the best model of each size.</p>

<p><strong>(d)</strong> Plot the test set MSE associated with the best model of each size.</p>

<p><strong>(e)</strong> For which model size does the test set MSE take on its minimum
value? Comment on your results. If it takes on its minimum value for a
model containing only an intercept or a model containing all of the
features, then play around with the way that you are generating the data
in (a) until you come up with a scenario in which the test set MSE is
minimized for an intermediate model size.</p>

<p><strong>(f)</strong> How does the model at which the test set MSE is minimized compare
to the true model used to generate the data? Comment on the coefficient
values.</p>

<p><strong>(g)</strong> Create a plot displaying
\(\sqrt{\Sum_{j=1}^{p}(\beta_{j}-\hat{\beta}_{j}^{r})^{2}}\) for a range
of values of \(r\), where \(\hat{\beta}_{j}^{r}\) is the $j$th coefficient
estimate for the best model containing \(r\) coefficients. Comment on what
you observe. How does this compare to the test MSE plot from (d)?</p>

<h3 id="answer-2">Answer</h3>

<h4 id="model-creation">Model creation</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">p<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>
n<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>
noise<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">rnorm</span>(n)
xmat<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">matrix</span>(<span style="color:#a6e22e">rnorm</span>(n<span style="color:#f92672">*</span>p),nrow<span style="color:#f92672">=</span>n,ncol<span style="color:#f92672">=</span>p)
beta<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">sample</span>(<span style="color:#ae81ff">-10</span><span style="color:#f92672">:</span><span style="color:#ae81ff">34</span>,<span style="color:#ae81ff">20</span>)
beta<span style="color:#a6e22e">[sample</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">20</span>,<span style="color:#ae81ff">4</span>)]<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
myY<span style="color:#f92672">&lt;-</span>xmat <span style="color:#f92672">%*%</span> beta <span style="color:#f92672">+</span> noise
modelDat<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">data.frame</span>(x<span style="color:#f92672">=</span><span style="color:#a6e22e">as.matrix</span>(xmat),y<span style="color:#f92672">=</span>myY)</code></pre></div>
<ul>
<li>As always we will want to take a peak</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">modelDat <span style="color:#f92672">%&gt;%</span> str <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    1000 obs. of  21 variables:
##  $ x.1 : num  -0.406 -1.375 0.858 -0.231 -0.601 ...
##  $ x.2 : num  -0.129 -0.218 -0.17 0.573 -0.513 ...
##  $ x.3 : num  0.127 -0.224 1.014 0.896 0.159 ...
##  $ x.4 : num  0.499 -0.151 -0.488 -0.959 2.187 ...
##  $ x.5 : num  -0.235 -0.345 -0.773 -0.346 0.773 ...
##  $ x.6 : num  0.26 -0.429 -1.183 -1.159 0.959 ...
##  $ x.7 : num  0.567 1.647 0.149 -0.593 -0.902 ...
##  $ x.8 : num  -0.092 0.8391 -1.4835 0.0229 -0.1353 ...
##  $ x.9 : num  -0.998 -1.043 -0.563 -0.377 0.324 ...
##  $ x.10: num  -0.4401 -0.195 -0.5139 -0.0156 -0.9543 ...
##  $ x.11: num  -0.147 0.829 0.165 0.101 -0.105 ...
##  $ x.12: num  -0.0118 1.02 1.0794 1.3184 -2.2844 ...
##  $ x.13: num  -1.683 0.487 -1.142 -0.744 -0.175 ...
##  $ x.14: num  0.228 -1.031 -2.798 -0.646 0.56 ...
##  $ x.15: num  -0.718 0.508 0.637 -0.556 0.585 ...
##  $ x.16: num  -1.6378 0.581 -0.9939 0.0537 -0.5854 ...
##  $ x.17: num  1.758 -0.616 1.377 -0.876 -1.174 ...
##  $ x.18: num  -1.438 0.373 1.364 0.399 0.949 ...
##  $ x.19: num  -0.715 -0.731 1.142 0.149 0.916 ...
##  $ x.20: num  2.774 -2.024 1.316 0.138 0.187 ...
##  $ y   : num  77.5 -82.8 -38.9 -79.7 64.9 ...
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">modelDat <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       x.1                x.2                x.3                x.4
##  Min.   :-2.79766   Min.   :-3.13281   Min.   :-2.71232   Min.   :-4.29604
##  1st Qu.:-0.60516   1st Qu.:-0.66759   1st Qu.:-0.60561   1st Qu.:-0.66598
##  Median : 0.04323   Median : 0.03681   Median : 0.06556   Median : 0.06589
##  Mean   : 0.06879   Mean   : 0.01004   Mean   : 0.06443   Mean   : 0.02244
##  3rd Qu.: 0.74049   3rd Qu.: 0.68234   3rd Qu.: 0.70521   3rd Qu.: 0.71174
##  Max.   : 3.50354   Max.   : 3.47268   Max.   : 3.02817   Max.   : 3.27326
##       x.5                 x.6                x.7                x.8
##  Min.   :-3.228376   Min.   :-4.24014   Min.   :-2.98577   Min.   :-3.27770
##  1st Qu.:-0.698220   1st Qu.:-0.69448   1st Qu.:-0.59092   1st Qu.:-0.52939
##  Median :-0.058778   Median :-0.01141   Median : 0.01732   Median : 0.05703
##  Mean   : 0.000126   Mean   :-0.05158   Mean   : 0.04767   Mean   : 0.08231
##  3rd Qu.: 0.663570   3rd Qu.: 0.64217   3rd Qu.: 0.67438   3rd Qu.: 0.72849
##  Max.   : 3.036307   Max.   : 3.27572   Max.   : 2.72163   Max.   : 3.33409
##       x.9                x.10               x.11               x.12
##  Min.   :-3.08957   Min.   :-3.21268   Min.   :-3.00572   Min.   :-3.72016
##  1st Qu.:-0.65456   1st Qu.:-0.69401   1st Qu.:-0.68226   1st Qu.:-0.63043
##  Median :-0.04242   Median :-0.03069   Median :-0.04777   Median : 0.07079
##  Mean   : 0.02049   Mean   :-0.02400   Mean   :-0.03729   Mean   : 0.03769
##  3rd Qu.: 0.71209   3rd Qu.: 0.61540   3rd Qu.: 0.64873   3rd Qu.: 0.67155
##  Max.   : 3.23110   Max.   : 2.76059   Max.   : 2.87306   Max.   : 3.48569
##       x.13               x.14               x.15                x.16
##  Min.   :-3.20126   Min.   :-3.55432   Min.   :-2.857575   Min.   :-3.5383
##  1st Qu.:-0.68535   1st Qu.:-0.66752   1st Qu.:-0.658708   1st Qu.:-0.7813
##  Median :-0.01329   Median :-0.03302   Median : 0.020581   Median :-0.0740
##  Mean   : 0.01094   Mean   : 0.02113   Mean   : 0.007976   Mean   :-0.0883
##  3rd Qu.: 0.64877   3rd Qu.: 0.74919   3rd Qu.: 0.670464   3rd Qu.: 0.5568
##  Max.   : 2.78973   Max.   : 3.47923   Max.   : 2.891527   Max.   : 3.0938
##       x.17               x.18               x.19              x.20
##  Min.   :-3.28570   Min.   :-4.06416   Min.   :-3.0443   Min.   :-4.06307
##  1st Qu.:-0.72302   1st Qu.:-0.72507   1st Qu.:-0.6684   1st Qu.:-0.70518
##  Median :-0.02439   Median :-0.04941   Median :-0.0610   Median :-0.07697
##  Mean   :-0.01459   Mean   :-0.03164   Mean   :-0.0414   Mean   :-0.05302
##  3rd Qu.: 0.62692   3rd Qu.: 0.68115   3rd Qu.: 0.6381   3rd Qu.: 0.58597
##  Max.   : 2.86446   Max.   : 3.32958   Max.   : 3.1722   Max.   : 3.01358
##        y
##  Min.   :-199.268
##  1st Qu.: -54.758
##  Median :  -1.607
##  Mean   :  -1.710
##  3rd Qu.:  49.367
##  Max.   : 278.244</code></pre></div>
<h4 id="b-train-test-split">b) Train Test Split</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">train_ind <span style="color:#f92672">=</span> <span style="color:#a6e22e">sample</span>(modelDat <span style="color:#f92672">%&gt;%</span> nrow,<span style="color:#ae81ff">100</span>)
test_ind <span style="color:#f92672">=</span> <span style="color:#a6e22e">setdiff</span>(<span style="color:#a6e22e">seq_len</span>(modelDat <span style="color:#f92672">%&gt;%</span> nrow), train_set)</code></pre></div>
<h4 id="best-subset-selection">Best subset selection</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">train_set<span style="color:#f92672">&lt;-</span>modelDat[train_ind,]
test_set<span style="color:#f92672">&lt;-</span>modelDat[<span style="color:#f92672">-</span>train_ind,]</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">linCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>)
linCol <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = .outcome ~ ., data = dat)
##
## Residuals:
##      Min       1Q   Median       3Q      Max
## -2.12474 -0.53970 -0.00944  0.42398  2.21086
##
## Coefficients:
##             Estimate Std. Error  t value Pr(&gt;|t|)
## (Intercept) -0.06052    0.09604   -0.630    0.530
## x.1         -0.02265    0.09198   -0.246    0.806
## x.2         28.91650    0.09879  292.719   &lt;2e-16 ***
## x.3         14.16532    0.09343  151.610   &lt;2e-16 ***
## x.4         28.16256    0.09828  286.564   &lt;2e-16 ***
## x.5          0.13742    0.09658    1.423    0.159
## x.6         27.01497    0.08540  316.341   &lt;2e-16 ***
## x.7         31.15917    0.09003  346.092   &lt;2e-16 ***
## x.8         -9.66308    0.11095  -87.094   &lt;2e-16 ***
## x.9          0.11641    0.10768    1.081    0.283
## x.10        19.06687    0.09662  197.344   &lt;2e-16 ***
## x.11        -9.09956    0.08627 -105.472   &lt;2e-16 ***
## x.12        -8.01933    0.10198  -78.633   &lt;2e-16 ***
## x.13         4.26852    0.09888   43.170   &lt;2e-16 ***
## x.14        20.22366    0.09853  205.247   &lt;2e-16 ***
## x.15        -0.16607    0.10466   -1.587    0.117
## x.16         7.95594    0.11250   70.721   &lt;2e-16 ***
## x.17        10.89851    0.11157   97.684   &lt;2e-16 ***
## x.18        -1.09760    0.09391  -11.688   &lt;2e-16 ***
## x.19        22.05197    0.08697  253.553   &lt;2e-16 ***
## x.20        20.88796    0.09274  225.221   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 0.8583 on 79 degrees of freedom
## Multiple R-squared:  0.9999, Adjusted R-squared:  0.9999
## F-statistic: 4.71e+04 on 20 and 79 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">linPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(linCol,test_set)
linPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>y)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      RMSE  Rsquared       MAE
## 1.2151815 0.9997265 0.9638378</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">L2Grid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,
                          lambda<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">-3</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;glmnet&#34;</span>,tuneGrid <span style="color:#f92672">=</span> L2Grid)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##             Length Class      Mode
## a0           100   -none-     numeric
## beta        2000   dgCMatrix  S4
## df           100   -none-     numeric
## dim            2   -none-     numeric
## lambda       100   -none-     numeric
## dev.ratio    100   -none-     numeric
## nulldev        1   -none-     numeric
## npasses        1   -none-     numeric
## jerr           1   -none-     numeric
## offset         1   -none-     logical
## call           5   -none-     call
## nobs           1   -none-     numeric
## lambdaOpt      1   -none-     numeric
## xNames        20   -none-     character
## problemType    1   -none-     character
## tuneValue      2   data.frame list
## obsLevels      1   -none-     logical
## param          0   -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coef</span>(ridgCol<span style="color:#f92672">$</span>finalModel, ridgCol<span style="color:#f92672">$</span>bestTune<span style="color:#f92672">$</span>lambda) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## 21 x 1 sparse Matrix of class &#34;dgCMatrix&#34;
##                       1
## (Intercept)  0.03898376
## x.1         -0.12140945
## x.2         27.63771674
## x.3         13.46853844
## x.4         26.54402352
## x.5         -0.13838118
## x.6         25.87706885
## x.7         29.90687677
## x.8         -9.42088971
## x.9         -0.08983349
## x.10        17.45444598
## x.11        -8.33991071
## x.12        -7.23653865
## x.13         3.35145521
## x.14        19.42178898
## x.15        -0.02794731
## x.16         7.63951382
## x.17        11.08083907
## x.18        -1.36872894
## x.19        20.90257005
## x.20        20.07494414</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(ridgCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-64-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(ridgCol,test_set)
ridgPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>y)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      RMSE  Rsquared       MAE
## 3.7554417 0.9994231 3.0184859</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">L1Grid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, <span style="color:#75715e"># for lasso</span>
                          lambda<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">-3</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;glmnet&#34;</span>,tuneGrid <span style="color:#f92672">=</span> L1Grid)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##             Length Class      Mode
## a0           47    -none-     numeric
## beta        940    dgCMatrix  S4
## df           47    -none-     numeric
## dim           2    -none-     numeric
## lambda       47    -none-     numeric
## dev.ratio    47    -none-     numeric
## nulldev       1    -none-     numeric
## npasses       1    -none-     numeric
## jerr          1    -none-     numeric
## offset        1    -none-     logical
## call          5    -none-     call
## nobs          1    -none-     numeric
## lambdaOpt     1    -none-     numeric
## xNames       20    -none-     character
## problemType   1    -none-     character
## tuneValue     2    data.frame list
## obsLevels     1    -none-     logical
## param         0    -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coef</span>(lassoCol<span style="color:#f92672">$</span>finalModel, lassoCol<span style="color:#f92672">$</span>bestTune<span style="color:#f92672">$</span>lambda) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## 21 x 1 sparse Matrix of class &#34;dgCMatrix&#34;
##                      1
## (Intercept)  0.1158884
## x.1          .
## x.2         28.5897869
## x.3         13.3637110
## x.4         27.2558797
## x.5          .
## x.6         26.6625588
## x.7         30.6841774
## x.8         -9.1388677
## x.9          .
## x.10        17.9220939
## x.11        -8.2461257
## x.12        -7.0603651
## x.13         3.2052101
## x.14        19.7219890
## x.15         .
## x.16         7.2082509
## x.17        10.4137411
## x.18        -0.6693664
## x.19        21.5357460
## x.20        20.5226071</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(lassoCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-69-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(lassoCol,test_set)
lassoPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>y)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      RMSE  Rsquared       MAE
## 2.7289452 0.9992454 2.2029482</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">mGrid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(ncomp<span style="color:#f92672">=</span><span style="color:#a6e22e">seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">pcrCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pcr&#34;</span>,tuneGrid <span style="color:#f92672">=</span> mGrid)
pcrCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Data:    X dimension: 100 20
##  Y dimension: 100 1
## Fit method: svdpc
## Number of components considered: 20
## TRAINING: % variance explained
##           1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X          10.040    18.46    26.62    34.56    41.87    48.54    54.56
## .outcome    8.425    34.90    41.09    43.12    45.06    48.09    66.44
##           8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X           60.33    65.37     70.01     74.46     78.72     82.32     85.67
## .outcome    85.76    88.81     89.93     91.66     91.91     92.04     92.08
##           15 comps  16 comps  17 comps  18 comps  19 comps  20 comps
## X            88.85     91.94     94.59     96.73     98.51    100.00
## .outcome     92.15     94.96     99.51     99.57     99.76     99.99
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(pcrCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-73-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">pcrPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(pcrCol,test_set)
pcrPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>y)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      RMSE  Rsquared       MAE
## 1.2151815 0.9997265 0.9638378</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">plsCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pls&#34;</span>,tuneGrid <span style="color:#f92672">=</span> mGrid)
plsCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Data:    X dimension: 100 20
##  Y dimension: 100 1
## Fit method: oscorespls
## Number of components considered: 20
## TRAINING: % variance explained
##           1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps
## X           7.762    14.79    21.01    26.89    31.55    36.13    41.12
## .outcome   92.765    98.81    99.75    99.96    99.98    99.99    99.99
##           8 comps  9 comps  10 comps  11 comps  12 comps  13 comps  14 comps
## X           46.35    51.21     56.12     60.34     65.63     71.57     76.16
## .outcome    99.99    99.99     99.99     99.99     99.99     99.99     99.99
##           15 comps  16 comps  17 comps  18 comps  19 comps  20 comps
## X            80.72     84.69     88.98     92.69     96.71    100.00
## .outcome     99.99     99.99     99.99     99.99     99.99     99.99
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(plsCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-76-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">plsPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(plsCol,test_set)
plsPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>y)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      RMSE  Rsquared       MAE
## 1.2151815 0.9997265 0.9638378</code></pre></div>
<h4 id="d-test-mse-for-best-models">d) Test MSE for best models</h4>

<ul>
<li>All the models have the same R² but Ridge does the worst followed by
LASSO</li>
</ul>

<p>For the rest of the question, we will consider the <code>OLS</code> model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">modelFit<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">regsubsets</span>(y<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>modelDat,nvmax<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
modelFit <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Subset selection object
## Call: regsubsets.formula(y ~ ., data = modelDat, nvmax = 20)
## 20 Variables  (and intercept)
##      Forced in Forced out
## x.1      FALSE      FALSE
## x.2      FALSE      FALSE
## x.3      FALSE      FALSE
## x.4      FALSE      FALSE
## x.5      FALSE      FALSE
## x.6      FALSE      FALSE
## x.7      FALSE      FALSE
## x.8      FALSE      FALSE
## x.9      FALSE      FALSE
## x.10     FALSE      FALSE
## x.11     FALSE      FALSE
## x.12     FALSE      FALSE
## x.13     FALSE      FALSE
## x.14     FALSE      FALSE
## x.15     FALSE      FALSE
## x.16     FALSE      FALSE
## x.17     FALSE      FALSE
## x.18     FALSE      FALSE
## x.19     FALSE      FALSE
## x.20     FALSE      FALSE
## 1 subsets of each size up to 20
## Selection Algorithm: exhaustive
##           x.1 x.2 x.3 x.4 x.5 x.6 x.7 x.8 x.9 x.10 x.11 x.12 x.13 x.14 x.15
## 1  ( 1 )  &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;
## 2  ( 1 )  &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;
## 3  ( 1 )  &#34; &#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;
## 4  ( 1 )  &#34; &#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;
## 5  ( 1 )  &#34; &#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34; &#34;
## 6  ( 1 )  &#34; &#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;
## 7  ( 1 )  &#34; &#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34; &#34;
## 8  ( 1 )  &#34; &#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34;*&#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34; &#34;
## 9  ( 1 )  &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34;*&#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34; &#34;
## 10  ( 1 ) &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34;*&#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34; &#34;
## 11  ( 1 ) &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34; &#34;
## 12  ( 1 ) &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34; &#34;
## 13  ( 1 ) &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34; &#34;  &#34;*&#34;  &#34; &#34;
## 14  ( 1 ) &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34; &#34;  &#34;*&#34;  &#34; &#34;
## 15  ( 1 ) &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34; &#34;
## 16  ( 1 ) &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34; &#34;
## 17  ( 1 ) &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34; &#34;
## 18  ( 1 ) &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;
## 19  ( 1 ) &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;
## 20  ( 1 ) &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;
##           x.16 x.17 x.18 x.19 x.20
## 1  ( 1 )  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;
## 2  ( 1 )  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;
## 3  ( 1 )  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;
## 4  ( 1 )  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;
## 5  ( 1 )  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34; &#34;
## 6  ( 1 )  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34;*&#34;
## 7  ( 1 )  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34;*&#34;
## 8  ( 1 )  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34;*&#34;
## 9  ( 1 )  &#34; &#34;  &#34; &#34;  &#34; &#34;  &#34;*&#34;  &#34;*&#34;
## 10  ( 1 ) &#34; &#34;  &#34;*&#34;  &#34; &#34;  &#34;*&#34;  &#34;*&#34;
## 11  ( 1 ) &#34; &#34;  &#34;*&#34;  &#34; &#34;  &#34;*&#34;  &#34;*&#34;
## 12  ( 1 ) &#34; &#34;  &#34;*&#34;  &#34; &#34;  &#34;*&#34;  &#34;*&#34;
## 13  ( 1 ) &#34; &#34;  &#34;*&#34;  &#34; &#34;  &#34;*&#34;  &#34;*&#34;
## 14  ( 1 ) &#34;*&#34;  &#34;*&#34;  &#34; &#34;  &#34;*&#34;  &#34;*&#34;
## 15  ( 1 ) &#34;*&#34;  &#34;*&#34;  &#34; &#34;  &#34;*&#34;  &#34;*&#34;
## 16  ( 1 ) &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;
## 17  ( 1 ) &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;
## 18  ( 1 ) &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;
## 19  ( 1 ) &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;
## 20  ( 1 ) &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;  &#34;*&#34;</code></pre></div>
<p>We might want to take a look at these.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(modelFit)
<span style="color:#a6e22e">plot</span>(modelFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Cp&#39;</span>)
<span style="color:#a6e22e">plot</span>(modelFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r2&#39;</span>)
<span style="color:#a6e22e">plot</span>(modelFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adjr2&#39;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-79-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plotLEAP</span>(modelFit <span style="color:#f92672">%&gt;%</span> summary)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-80-1.png"   />

        
    </figure>



<p>It would appear that 16 variables would be a good bet. We note that the
lasso model did void out 4 parameters, namely x₁,x₃,x₁₃ and x₁₇.</p>

<p>Lets take a quick look at the various model variable significance
values.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lgp<span style="color:#f92672">&lt;-</span>linCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;OLS Variable Importance&#34;</span>)
rgp<span style="color:#f92672">&lt;-</span>ridgCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;Ridge Variable Importance&#34;</span>)
lsgp<span style="color:#f92672">&lt;-</span>lassoCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;Lasso Variable Importance&#34;</span>)
pcgp<span style="color:#f92672">&lt;-</span>pcrCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;PCR Variable Importance&#34;</span>)
plgp<span style="color:#f92672">&lt;-</span>plsCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;PLS Variable Importance&#34;</span>)
<span style="color:#a6e22e">grid.arrange</span>(lgp,rgp,lsgp,pcgp,plgp,ncol<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,bottom<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Effective Importance, scaled&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-81-1.png"   />

        
    </figure>



<h4 id="e-model-size">e) Model size</h4>

<p>The test set numeric minimum RMSE is a tie between OLS and PCR, and this
was achieved for the (effective) 16 variable OLS model, as well as the
18 variable PCR model.</p>

<h4 id="f-best-model">f) Best model</h4>

<p>We will consider the OLS and PCR models and its parameters.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">linCol<span style="color:#f92672">$</span>finalModel <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = .outcome ~ ., data = dat)
##
## Coefficients:
## (Intercept)          x.1          x.2          x.3          x.4          x.5
##    -0.06052     -0.02265     28.91650     14.16532     28.16256      0.13742
##         x.6          x.7          x.8          x.9         x.10         x.11
##    27.01497     31.15917     -9.66308      0.11641     19.06687     -9.09956
##        x.12         x.13         x.14         x.15         x.16         x.17
##    -8.01933      4.26852     20.22366     -0.16607      7.95594     10.89851
##        x.18         x.19         x.20
##    -1.09760     22.05197     20.88796</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">pcrCol<span style="color:#f92672">$</span>bestTune <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##    ncomp
## 10    20</code></pre></div>
<p>Now to compare this to the original.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">beta <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##  [1]   0  29  14  28   0  27  31 -10   0  19  -9  -8   4  20   0   8  11  -1  22
## [20]  21</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">t<span style="color:#f92672">=</span><span style="color:#a6e22e">data.frame</span>(linCol<span style="color:#f92672">$</span>finalModel<span style="color:#f92672">$</span>coefficients[<span style="color:#ae81ff">-1</span>]) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">rename</span>(<span style="color:#e6db74">&#34;Model_Coeffs&#34;</span><span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">add_column</span>(beta) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">rename</span>(<span style="color:#e6db74">&#34;Original_Coeffs&#34;</span><span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
<span style="color:#a6e22e">print</span>(t)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      Model_Coeffs Original_Coeffs
## x.1   -0.02265289               0
## x.2   28.91649699              29
## x.3   14.16532050              14
## x.4   28.16255937              28
## x.5    0.13741621               0
## x.6   27.01497459              27
## x.7   31.15917172              31
## x.8   -9.66308362             -10
## x.9    0.11641282               0
## x.10  19.06687041              19
## x.11  -9.09955826              -9
## x.12  -8.01932598              -8
## x.13   4.26852334               4
## x.14  20.22366153              20
## x.15  -0.16606531               0
## x.16   7.95593559               8
## x.17  10.89851353              11
## x.18  -1.09759687              -1
## x.19  22.05196537              22
## x.20  20.88795623              21</code></pre></div>
<p>We see that the coefficients are pretty similar.</p>

<h4 id="g-plotting-differences">g) Plotting differences</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">val.errors <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NaN</span>, p)
a <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NaN</span>, p)
b <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NaN</span>, p)
x_cols <span style="color:#f92672">=</span> <span style="color:#a6e22e">colnames</span>(xmat, do.NULL <span style="color:#f92672">=</span> <span style="color:#66d9ef">FALSE</span>, prefix <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;x.&#34;</span>)
<span style="color:#a6e22e">for </span>(i in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>p) {
    coefi <span style="color:#f92672">=</span> <span style="color:#a6e22e">coef</span>(modelFit, id <span style="color:#f92672">=</span> i)
    a[i] <span style="color:#f92672">=</span> <span style="color:#a6e22e">length</span>(coefi) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> <span style="color:#75715e">## Not counting the intercept</span>
    b[i] <span style="color:#f92672">=</span> <span style="color:#a6e22e">sqrt</span>(<span style="color:#a6e22e">sum</span>((beta[x_cols <span style="color:#f92672">%in%</span> <span style="color:#a6e22e">names</span>(coefi)] <span style="color:#f92672">-</span> coefi<span style="color:#a6e22e">[names</span>(coefi) <span style="color:#f92672">%in%</span> x_cols])^2) <span style="color:#f92672">+</span>
        <span style="color:#a6e22e">sum</span>(beta[<span style="color:#f92672">!</span>(x_cols <span style="color:#f92672">%in%</span> <span style="color:#a6e22e">names</span>(coefi))])^2) <span style="color:#75715e">## Handling the intercept</span>
}
<span style="color:#a6e22e">plot</span>(x <span style="color:#f92672">=</span> a, y <span style="color:#f92672">=</span> b, xlab <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Number of Coefficients&#34;</span>, ylab <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Relative Error&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-84-1.png"   />

        
    </figure>



<h2 id="question-6.11---page-264">Question 6.11 - Page 264</h2>

<p>We will now try to predict per capita crime rate in the Boston data set.</p>

<p><strong>(a)</strong> Try out some of the regression methods explored in this chapter,
such as best subset selection, the lasso, ridge regression, and PCR.
Present and discuss results for the approaches that you consider.</p>

<p><strong>(b)</strong> Propose a model (or set of models) that seem to perform well on
this data set, and justify your answer. Make sure that you are
evaluating model performance using validation set error,
crossvalidation, or some other reasonable alternative, as opposed to
using training error.</p>

<p><strong>&copy;</strong> Does your chosen model involve all of the features in the data
set? Why or why not?</p>

<h3 id="answer-3">Answer</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">&lt;-</span>MASS<span style="color:#f92672">::</span>Boston</code></pre></div>
<ul>
<li>Summarize</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> str <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    506 obs. of  14 variables:
##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
##  $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...
##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
##  $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...
##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
##  $ black  : num  397 397 393 395 397 ...
##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       crim                zn             indus            chas
##  Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000
##  1st Qu.: 0.08204   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000
##  Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000
##  Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917
##  3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000
##  Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000
##       nox               rm             age              dis
##  Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130
##  1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100
##  Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207
##  Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795
##  3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188
##  Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127
##       rad              tax           ptratio          black
##  Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32
##  1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38
##  Median : 5.000   Median :330.0   Median :19.05   Median :391.44
##  Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67
##  3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23
##  Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90
##      lstat            medv
##  Min.   : 1.73   Min.   : 5.00
##  1st Qu.: 6.95   1st Qu.:17.02
##  Median :11.36   Median :21.20
##  Mean   :12.65   Mean   :22.53
##  3rd Qu.:16.95   3rd Qu.:25.00
##  Max.   :37.97   Max.   :50.00</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(unique) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(length) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##    crim      zn   indus    chas     nox      rm     age     dis     rad     tax
##     504      26      76       2      81     446     356     412       9      66
## ptratio   black   lstat    medv
##      46     357     455     229</code></pre></div>
<h4 id="a-test-regression-models">a) Test regression models</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">train_ind <span style="color:#f92672">=</span> <span style="color:#a6e22e">sample</span>(boston <span style="color:#f92672">%&gt;%</span> nrow,<span style="color:#ae81ff">100</span>)
test_ind <span style="color:#f92672">=</span> <span style="color:#a6e22e">setdiff</span>(<span style="color:#a6e22e">seq_len</span>(boston <span style="color:#f92672">%&gt;%</span> nrow), train_set)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">train_set<span style="color:#f92672">&lt;-</span>boston[train_ind,]
test_set<span style="color:#f92672">&lt;-</span>boston[<span style="color:#f92672">-</span>train_ind,]</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">linCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(crim<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>)
linCol <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = .outcome ~ ., data = dat)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -6.2431 -1.0344 -0.0563  0.8187  8.1318
##
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.9339246  7.6508393   0.122   0.9031
## zn           0.0046819  0.0157375   0.297   0.7668
## indus        0.0276209  0.0875254   0.316   0.7531
## chas        -1.1602278  1.2386869  -0.937   0.3516
## nox         -7.5024503  5.0207818  -1.494   0.1388
## rm           1.1240874  0.7462340   1.506   0.1356
## age          0.0020182  0.0137404   0.147   0.8836
## dis         -0.3934753  0.2454365  -1.603   0.1126
## rad          0.4540613  0.0791580   5.736 1.41e-07 ***
## tax          0.0008469  0.0052593   0.161   0.8724
## ptratio     -0.2978204  0.1637629  -1.819   0.0725 .
## black        0.0030642  0.0045281   0.677   0.5004
## lstat        0.1322779  0.0626485   2.111   0.0376 *
## medv        -0.0841382  0.0590700  -1.424   0.1580
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 2.362 on 86 degrees of freedom
## Multiple R-squared:  0.775,  Adjusted R-squared:  0.741
## F-statistic: 22.78 on 13 and 86 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">linPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(linCol,test_set)
linPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>crim)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      RMSE  Rsquared       MAE
## 7.3794735 0.4056002 2.6774969</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">L2Grid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,
                          lambda<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">-3</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(crim<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;glmnet&#34;</span>,tuneGrid <span style="color:#f92672">=</span> L2Grid)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##             Length Class      Mode
## a0           100   -none-     numeric
## beta        1300   dgCMatrix  S4
## df           100   -none-     numeric
## dim            2   -none-     numeric
## lambda       100   -none-     numeric
## dev.ratio    100   -none-     numeric
## nulldev        1   -none-     numeric
## npasses        1   -none-     numeric
## jerr           1   -none-     numeric
## offset         1   -none-     logical
## call           5   -none-     call
## nobs           1   -none-     numeric
## lambdaOpt      1   -none-     numeric
## xNames        13   -none-     character
## problemType    1   -none-     character
## tuneValue      2   data.frame list
## obsLevels      1   -none-     logical
## param          0   -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coef</span>(ridgCol<span style="color:#f92672">$</span>finalModel, ridgCol<span style="color:#f92672">$</span>bestTune<span style="color:#f92672">$</span>lambda) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## 14 x 1 sparse Matrix of class &#34;dgCMatrix&#34;
##                        1
## (Intercept) -3.881166065
## zn           0.002597790
## indus       -0.005103517
## chas        -0.674764337
## nox         -0.053645732
## rm           0.600064844
## age          0.001153570
## dis         -0.179295384
## rad          0.267082956
## tax          0.006447932
## ptratio     -0.075885753
## black       -0.001650403
## lstat        0.086462700
## medv        -0.027519270</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(ridgCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-94-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(ridgCol,test_set)
ridgPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>crim)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      RMSE  Rsquared       MAE
## 7.5065916 0.4017056 2.4777547</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">L1Grid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, <span style="color:#75715e"># for lasso</span>
                          lambda<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">-3</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(crim<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;glmnet&#34;</span>,tuneGrid <span style="color:#f92672">=</span> L1Grid)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##             Length Class      Mode
## a0            78   -none-     numeric
## beta        1014   dgCMatrix  S4
## df            78   -none-     numeric
## dim            2   -none-     numeric
## lambda        78   -none-     numeric
## dev.ratio     78   -none-     numeric
## nulldev        1   -none-     numeric
## npasses        1   -none-     numeric
## jerr           1   -none-     numeric
## offset         1   -none-     logical
## call           5   -none-     call
## nobs           1   -none-     numeric
## lambdaOpt      1   -none-     numeric
## xNames        13   -none-     character
## problemType    1   -none-     character
## tuneValue      2   data.frame list
## obsLevels      1   -none-     logical
## param          0   -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coef</span>(lassoCol<span style="color:#f92672">$</span>finalModel, lassoCol<span style="color:#f92672">$</span>bestTune<span style="color:#f92672">$</span>lambda) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## 14 x 1 sparse Matrix of class &#34;dgCMatrix&#34;
##                        1
## (Intercept) -2.024006430
## zn           .
## indus        .
## chas         .
## nox          .
## rm           .
## age          .
## dis         -0.008506188
## rad          0.386379255
## tax          0.001779579
## ptratio      .
## black        .
## lstat        0.040788606
## medv         .</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(lassoCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-99-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(lassoCol,test_set)
lassoPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>crim)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      RMSE  Rsquared       MAE
## 7.5868293 0.3859121 2.4892258</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">mGrid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(ncomp<span style="color:#f92672">=</span><span style="color:#a6e22e">seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>))</code></pre></div>
<ul>
<li>All the models have the same R² but Ridge does the worst followed by
LASSO</li>
</ul>

<p>For the rest of the question, we will consider the <code>OLS</code> model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">modelFit<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">regsubsets</span>(crim<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>boston,nvmax<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
modelFit <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Subset selection object
## Call: regsubsets.formula(crim ~ ., data = boston, nvmax = 20)
## 13 Variables  (and intercept)
##         Forced in Forced out
## zn          FALSE      FALSE
## indus       FALSE      FALSE
## chas        FALSE      FALSE
## nox         FALSE      FALSE
## rm          FALSE      FALSE
## age         FALSE      FALSE
## dis         FALSE      FALSE
## rad         FALSE      FALSE
## tax         FALSE      FALSE
## ptratio     FALSE      FALSE
## black       FALSE      FALSE
## lstat       FALSE      FALSE
## medv        FALSE      FALSE
## 1 subsets of each size up to 13
## Selection Algorithm: exhaustive
##           zn  indus chas nox rm  age dis rad tax ptratio black lstat medv
## 1  ( 1 )  &#34; &#34; &#34; &#34;   &#34; &#34;  &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34; &#34;     &#34; &#34;   &#34; &#34;   &#34; &#34;
## 2  ( 1 )  &#34; &#34; &#34; &#34;   &#34; &#34;  &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34; &#34;     &#34; &#34;   &#34;*&#34;   &#34; &#34;
## 3  ( 1 )  &#34; &#34; &#34; &#34;   &#34; &#34;  &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34; &#34; &#34; &#34;     &#34;*&#34;   &#34;*&#34;   &#34; &#34;
## 4  ( 1 )  &#34;*&#34; &#34; &#34;   &#34; &#34;  &#34; &#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34;     &#34; &#34;   &#34; &#34;   &#34;*&#34;
## 5  ( 1 )  &#34;*&#34; &#34; &#34;   &#34; &#34;  &#34; &#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34;     &#34;*&#34;   &#34; &#34;   &#34;*&#34;
## 6  ( 1 )  &#34;*&#34; &#34; &#34;   &#34; &#34;  &#34;*&#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34; &#34;     &#34;*&#34;   &#34; &#34;   &#34;*&#34;
## 7  ( 1 )  &#34;*&#34; &#34; &#34;   &#34; &#34;  &#34;*&#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;     &#34;*&#34;   &#34; &#34;   &#34;*&#34;
## 8  ( 1 )  &#34;*&#34; &#34; &#34;   &#34; &#34;  &#34;*&#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;     &#34;*&#34;   &#34;*&#34;   &#34;*&#34;
## 9  ( 1 )  &#34;*&#34; &#34;*&#34;   &#34; &#34;  &#34;*&#34; &#34; &#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;     &#34;*&#34;   &#34;*&#34;   &#34;*&#34;
## 10  ( 1 ) &#34;*&#34; &#34;*&#34;   &#34; &#34;  &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34;     &#34;*&#34;   &#34;*&#34;   &#34;*&#34;
## 11  ( 1 ) &#34;*&#34; &#34;*&#34;   &#34; &#34;  &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34;     &#34;*&#34;   &#34;*&#34;   &#34;*&#34;
## 12  ( 1 ) &#34;*&#34; &#34;*&#34;   &#34;*&#34;  &#34;*&#34; &#34;*&#34; &#34; &#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34;     &#34;*&#34;   &#34;*&#34;   &#34;*&#34;
## 13  ( 1 ) &#34;*&#34; &#34;*&#34;   &#34;*&#34;  &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34; &#34;*&#34;     &#34;*&#34;   &#34;*&#34;   &#34;*&#34;</code></pre></div>
<p>We might want to take a look at these.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(modelFit)
<span style="color:#a6e22e">plot</span>(modelFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Cp&#39;</span>)
<span style="color:#a6e22e">plot</span>(modelFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r2&#39;</span>)
<span style="color:#a6e22e">plot</span>(modelFit,scale<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adjr2&#39;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-103-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plotLEAP</span>(modelFit <span style="color:#f92672">%&gt;%</span> summary)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-104-1.png"   />

        
    </figure>



<p>It would appear that 16 variables would be a good bet. We note that the
lasso model did void out 4 parameters, namely x₁,x₃,x₁₃ and x₁₇.</p>

<p>Lets take a quick look at the various model variable significance
values.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lgp<span style="color:#f92672">&lt;-</span>linCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;OLS Variable Importance&#34;</span>)
rgp<span style="color:#f92672">&lt;-</span>ridgCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;Ridge Variable Importance&#34;</span>)
lsgp<span style="color:#f92672">&lt;-</span>lassoCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;Lasso Variable Importance&#34;</span>)
<span style="color:#a6e22e">grid.arrange</span>(lgp,rgp,lsgp,ncol<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,bottom<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Effective Importance, scaled&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-105-1.png"   />

        
    </figure>



<h4 id="b-propose-a-model">b) Propose a model</h4>

<ul>
<li><p>Given the data and plots, I would probably end up using the Ridge
regression model</p></li>

<li><p>Clearly, LASSO is not working very well since it seems to have taken
mainly 3 variables, one of which is largely categorical (9 levels)</p></li>
</ul>

<h4 id="c-model-properties">c) Model properties</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> str <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    506 obs. of  14 variables:
##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
##  $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...
##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
##  $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...
##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
##  $ black  : num  397 397 393 395 397 ...
##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...
## NULL</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       crim                zn             indus            chas
##  Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000
##  1st Qu.: 0.08204   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000
##  Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000
##  Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917
##  3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000
##  Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000
##       nox               rm             age              dis
##  Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130
##  1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100
##  Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207
##  Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795
##  3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188
##  Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127
##       rad              tax           ptratio          black
##  Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32
##  1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38
##  Median : 5.000   Median :330.0   Median :19.05   Median :391.44
##  Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67
##  3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23
##  Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90
##      lstat            medv
##  Min.   : 1.73   Min.   : 5.00
##  1st Qu.: 6.95   1st Qu.:17.02
##  Median :11.36   Median :21.20
##  Mean   :12.65   Mean   :22.53
##  3rd Qu.:16.95   3rd Qu.:25.00
##  Max.   :37.97   Max.   :50.00</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(unique) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(length) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##    crim      zn   indus    chas     nox      rm     age     dis     rad     tax
##     504      26      76       2      81     446     356     412       9      66
## ptratio   black   lstat    medv
##      46     357     455     229</code></pre></div>
<ul>
<li>A good idea would be removing <code>rad</code> and <code>chas</code> from the regression</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">&lt;-</span>boston <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(rad,chas))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">train_ind <span style="color:#f92672">=</span> <span style="color:#a6e22e">sample</span>(boston <span style="color:#f92672">%&gt;%</span> nrow,<span style="color:#ae81ff">100</span>)
test_ind <span style="color:#f92672">=</span> <span style="color:#a6e22e">setdiff</span>(<span style="color:#a6e22e">seq_len</span>(boston <span style="color:#f92672">%&gt;%</span> nrow), train_set)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">train_set<span style="color:#f92672">&lt;-</span>boston[train_ind,]
test_set<span style="color:#f92672">&lt;-</span>boston[<span style="color:#f92672">-</span>train_ind,]</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">L2Grid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,
                          lambda<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">-3</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(crim<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;glmnet&#34;</span>,tuneGrid <span style="color:#f92672">=</span> L2Grid)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##             Length Class      Mode
## a0           100   -none-     numeric
## beta        1100   dgCMatrix  S4
## df           100   -none-     numeric
## dim            2   -none-     numeric
## lambda       100   -none-     numeric
## dev.ratio    100   -none-     numeric
## nulldev        1   -none-     numeric
## npasses        1   -none-     numeric
## jerr           1   -none-     numeric
## offset         1   -none-     logical
## call           5   -none-     call
## nobs           1   -none-     numeric
## lambdaOpt      1   -none-     numeric
## xNames        11   -none-     character
## problemType    1   -none-     character
## tuneValue      2   data.frame list
## obsLevels      1   -none-     logical
## param          0   -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coef</span>(ridgCol<span style="color:#f92672">$</span>finalModel, ridgCol<span style="color:#f92672">$</span>bestTune<span style="color:#f92672">$</span>lambda) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## 12 x 1 sparse Matrix of class &#34;dgCMatrix&#34;
##                       1
## (Intercept)  0.23015398
## zn           0.01595378
## indus       -0.01006683
## nox          3.42104450
## rm          -0.05904911
## age          0.01419585
## dis         -0.16724715
## tax          0.01021790
## ptratio      0.05741508
## black       -0.01414707
## lstat        0.15519923
## medv        -0.04483670</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(ridgCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-113-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ridgPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(ridgCol,test_set)
ridgPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>crim)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      RMSE  Rsquared       MAE
## 6.9365371 0.3619974 2.8570012</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">L1Grid <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">expand.grid</span>(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, <span style="color:#75715e"># for lasso</span>
                          lambda<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#a6e22e">^seq</span>(from<span style="color:#f92672">=</span><span style="color:#ae81ff">-3</span>,to<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,length<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCol<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">train</span>(crim<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>train_set,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;glmnet&#34;</span>,tuneGrid <span style="color:#f92672">=</span> L1Grid)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoCol <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##             Length Class      Mode
## a0           74    -none-     numeric
## beta        814    dgCMatrix  S4
## df           74    -none-     numeric
## dim           2    -none-     numeric
## lambda       74    -none-     numeric
## dev.ratio    74    -none-     numeric
## nulldev       1    -none-     numeric
## npasses       1    -none-     numeric
## jerr          1    -none-     numeric
## offset        1    -none-     logical
## call          5    -none-     call
## nobs          1    -none-     numeric
## lambdaOpt     1    -none-     numeric
## xNames       11    -none-     character
## problemType   1    -none-     character
## tuneValue     2    data.frame list
## obsLevels     1    -none-     logical
## param         0    -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">coef</span>(lassoCol<span style="color:#f92672">$</span>finalModel, lassoCol<span style="color:#f92672">$</span>bestTune<span style="color:#f92672">$</span>lambda) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## 12 x 1 sparse Matrix of class &#34;dgCMatrix&#34;
##                       1
## (Intercept)  0.86635621
## zn           .
## indus        .
## nox          .
## rm           .
## age          .
## dis          .
## tax          0.01402174
## ptratio      .
## black       -0.01454496
## lstat        0.15290845
## medv         .</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggplot</span>(lassoCol)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-118-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lassoPred<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(lassoCol,test_set)
lassoPred <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">postResample</span>(obs <span style="color:#f92672">=</span> test_set<span style="color:#f92672">$</span>crim)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      RMSE  Rsquared       MAE
## 6.9630616 0.3693364 2.6767742</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">rgp<span style="color:#f92672">&lt;-</span>ridgCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;Ridge Variable Importance&#34;</span>)
lsgp<span style="color:#f92672">&lt;-</span>lassoCol <span style="color:#f92672">%&gt;%</span> varImp <span style="color:#f92672">%&gt;%</span> ggplot <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;Lasso Variable Importance&#34;</span>)
<span style="color:#a6e22e">grid.arrange</span>(rgp,lsgp,ncol<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,bottom<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Effective Importance, scaled&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol4/unnamed-chunk-120-1.png"   />

        
    </figure>



<p>None of these models are actually any good apparently, given that we
have an R² of 0.3619974 for the L2 regularization and 0.3693364 for the
L1.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1">James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science &amp; Business Media.
 <a class="footnote-return" href="#fnref:fn-1"><sup>[return]</sup></a></li>
<li id="fn:fn-2">Lang et al., (2019). mlr3: A modern object-oriented machine learning framework in R. Journal of Open Source Software, 4(44), 1903, <a href="https://doi.org/10.21105/joss.01903" target="_blank">https://doi.org/10.21105/joss.01903</a>
 <a class="footnote-return" href="#fnref:fn-2"><sup>[return]</sup></a></li>
</ol>
</div>
]]></content>
        </item>
        
        <item>
            <title>ISLR :: Resampling Methods</title>
            <link>https://rgoswami.me/posts/islr-ch5/</link>
            <pubDate>Tue, 18 Feb 2020 22:00:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/islr-ch5/</guid>
            <description>Chapter V - Resampling Methods All the questions are as per the ISL seventh printing1.
Common Instead of using the standard functions, we will leverage the mlr3 package2.
#install.packages(&amp;#34;mlr3&amp;#34;,&amp;#34;data.table&amp;#34;,&amp;#34;mlr3viz&amp;#34;,&amp;#34;mlr3learners&amp;#34;) Actually for R version 3.6.2, the steps to get it working were a bit more involved.
install.packages(&amp;#34;remotes&amp;#34;,&amp;#34;data.table&amp;#34;, &amp;#34;GGally&amp;#34;,&amp;#34;precerec&amp;#34;) # For plotslibrary(remotes) remotes::install_github(&amp;#34;mlr-org/mlr3&amp;#34;)## Skipping install of &amp;#39;mlr3&amp;#39; from a github remote, the SHA1 (fca21c10) has not changed since last install. ## Use `force = TRUE` to force installationremotes::install_github(&amp;#34;mlr-org/mlr3viz&amp;#34;)## Skipping install of &amp;#39;mlr3viz&amp;#39; from a github remote, the SHA1 (0b4ea273) has not changed since last install.</description>
            <content type="html"><![CDATA[

<h2 id="chapter-v---resampling-methods">Chapter V - Resampling Methods</h2>

<p>All the questions are as per the
<a href="https://faculty.marshall.usc.edu/gareth-james/ISL/" target="_blank">ISL seventh
printing</a><sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup>.</p>

<h3 id="common">Common</h3>

<p>Instead of using the standard functions, we will leverage the <code>mlr3</code>
package<sup class="footnote-ref" id="fnref:fn-2"><a href="#fn:fn-2">2</a></sup>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e">#install.packages(&#34;mlr3&#34;,&#34;data.table&#34;,&#34;mlr3viz&#34;,&#34;mlr3learners&#34;)</span></code></pre></div>
<p>Actually for <code>R</code> version <code>3.6.2</code>, the steps to get it working were a bit
more involved.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">install.packages</span>(<span style="color:#e6db74">&#34;remotes&#34;</span>,<span style="color:#e6db74">&#34;data.table&#34;</span>,
                 <span style="color:#e6db74">&#34;GGally&#34;</span>,<span style="color:#e6db74">&#34;precerec&#34;</span>) <span style="color:#75715e"># For plots</span></code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(remotes)
remotes<span style="color:#f92672">::</span><span style="color:#a6e22e">install_github</span>(<span style="color:#e6db74">&#34;mlr-org/mlr3&#34;</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Skipping install of &#39;mlr3&#39; from a github remote, the SHA1 (fca21c10) has not changed since last install.
##   Use `force = TRUE` to force installation</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">remotes<span style="color:#f92672">::</span><span style="color:#a6e22e">install_github</span>(<span style="color:#e6db74">&#34;mlr-org/mlr3viz&#34;</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Skipping install of &#39;mlr3viz&#39; from a github remote, the SHA1 (0b4ea273) has not changed since last install.
##   Use `force = TRUE` to force installation</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">remotes<span style="color:#f92672">::</span><span style="color:#a6e22e">install_github</span>(<span style="color:#e6db74">&#34;mlr-org/mlr3learners&#34;</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Skipping install of &#39;mlr3learners&#39; from a github remote, the SHA1 (64b275a0) has not changed since last install.
##   Use `force = TRUE` to force installation</code></pre></div>
<p>Load <code>ISLR</code> and other libraries.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">libsUsed<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">c</span>(<span style="color:#e6db74">&#34;dplyr&#34;</span>,<span style="color:#e6db74">&#34;ggplot2&#34;</span>,<span style="color:#e6db74">&#34;tidyverse&#34;</span>,
            <span style="color:#e6db74">&#34;ISLR&#34;</span>,<span style="color:#e6db74">&#34;caret&#34;</span>,<span style="color:#e6db74">&#34;MASS&#34;</span>,
            <span style="color:#e6db74">&#34;pROC&#34;</span>,<span style="color:#e6db74">&#34;mlr3&#34;</span>,<span style="color:#e6db74">&#34;data.table&#34;</span>,
            <span style="color:#e6db74">&#34;mlr3viz&#34;</span>,<span style="color:#e6db74">&#34;mlr3learners&#34;</span>)
<span style="color:#a6e22e">invisible</span>(<span style="color:#a6e22e">lapply</span>(libsUsed, library, character.only <span style="color:#f92672">=</span> <span style="color:#66d9ef">TRUE</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;dplyr&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:stats&#39;:
##
##     filter, lag</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:base&#39;:
##
##     intersect, setdiff, setequal, union</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## ✔ tibble  2.1.3     ✔ purrr   0.3.3
## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.4.0</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Loading required package: lattice</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;caret&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:purrr&#39;:
##
##     lift</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;MASS&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:dplyr&#39;:
##
##     select</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Type &#39;citation(&#34;pROC&#34;)&#39; for a citation.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;pROC&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:stats&#39;:
##
##     cov, smooth, var</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;data.table&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:purrr&#39;:
##
##     transpose</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:dplyr&#39;:
##
##     between, first, last</code></pre></div>
<h2 id="question-5.5---page-198">Question 5.5 - Page 198</h2>

<p>In Chapter 4, we used logistic regression to predict the probability of
<code>default</code> using <code>income</code> and <code>balance</code> on the <code>Default</code> data set. We
will now estimate the test error of this logistic regression model using
the validation set approach. Do not forget to set a random seed before
beginning your analysis.</p>

<p><strong>(a)</strong> Fit a logistic regression model that uses income and balance to
predict <code>default</code>.</p>

<p><strong>(b)</strong> Using the validation set approach, estimate the test error of this
model. In order to do this, you must perform the following steps:</p>

<ol>
<li><p>Split the sample set into a training set and a validation set.</p></li>

<li><p>Fit a multiple logistic regression model using only the training
observations.</p></li>

<li><p>Obtain a prediction of default status for each individual in the
validation set by computing the posterior probability of default for
that individual, and classifying the individual to the <code>default</code>
category if the posterior probability is greater than \(0.5\).</p></li>

<li><p>Compute the validation set error, which is the fraction of the
observations in the validation set that are misclassified.</p></li>
</ol>

<p><strong>&copy;</strong> Repeat the process in (b) three times, using three different
splits of the observations into a training set and a validation set.
Comment on the results obtained.</p>

<p><strong>(d)</strong> Now consider a logistic regression model that predicts the prob-
ability of <code>default</code> using <code>income</code> , <code>balance</code> , and a dummy variable
for <code>student</code>. Estimate the test error for this model using the
validation set approach. Comment on whether or not including a dummy
variable for <code>student</code> leads to a reduction in the test error rate.</p>

<h3 id="answer">Answer</h3>

<p>We will need our data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">defDat<span style="color:#f92672">&lt;-</span>ISLR<span style="color:#f92672">::</span>Default</code></pre></div>
<ul>
<li>Very quick peek</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">defDat <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##  default    student       balance           income
##  No :9667   No :7056   Min.   :   0.0   Min.   :  772
##  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340
##                        Median : 823.6   Median :34553
##                        Mean   : 835.4   Mean   :33517
##                        3rd Qu.:1166.3   3rd Qu.:43808
##                        Max.   :2654.3   Max.   :73554</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">defDat <span style="color:#f92672">%&gt;%</span> str</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    10000 obs. of  4 variables:
##  $ default: Factor w/ 2 levels &#34;No&#34;,&#34;Yes&#34;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ student: Factor w/ 2 levels &#34;No&#34;,&#34;Yes&#34;: 1 2 1 1 1 2 1 2 1 1 ...
##  $ balance: num  730 817 1074 529 786 ...
##  $ income : num  44362 12106 31767 35704 38463 ...</code></pre></div>
<h4 id="a-logistic-model-with-mlr3">a) Logistic Model with mlr3</h4>

<p>Following the <a href="https://mlr3book.mlr-org.com/tasks.html" target="_blank">new approach</a>
which leverages R6 features leads us to define a classification task
first. As far as I can tell, the data needs to be filtered to contain
only the things we need to predict with, in this case we are required to
use only income and balance so we will do so.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">1984</span>)
redDat<span style="color:#f92672">&lt;-</span>defDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(income,balance,default))
tskLogiFull<span style="color:#f92672">=</span>TaskClassif<span style="color:#f92672">$</span><span style="color:#a6e22e">new</span>(id<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;credit&#34;</span>,backend<span style="color:#f92672">=</span>redDat,target<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;default&#34;</span>)
<span style="color:#a6e22e">print</span>(tskLogiFull)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &lt;TaskClassif:credit&gt; (10000 x 3)
## * Target: default
## * Properties: twoclass
## * Features (2):
##   - dbl (2): balance, income</code></pre></div>
<p>This can be visualized neatly as well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">autoplot</span>(tskLogiFull)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol3/unnamed-chunk-8-1.png"   />

        
            <figcaption class="center" >Figure 1: MLR3 Visualizations</figcaption>
        
    </figure>



<p>We have a pretty imbalanced data-set.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">autoplot</span>(tskLogiFull,type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pairs&#34;</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from
##   +.gg   ggplot2</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol3/unnamed-chunk-9-1.png"   />

        
            <figcaption class="center" >Figure 2: Paired mlr3 data</figcaption>
        
    </figure>



<p>We can use any of the learners implemented, so it is a good idea to take
a quick peek at them all.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">as.data.table</span>(mlr_learners)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##                     key                                    feature_types
##  1:       classif.debug logical,integer,numeric,character,factor,ordered
##  2: classif.featureless logical,integer,numeric,character,factor,ordered
##  3:      classif.glmnet                          logical,integer,numeric
##  4:        classif.kknn           logical,integer,numeric,factor,ordered
##  5:         classif.lda           logical,integer,numeric,factor,ordered
##  6:     classif.log_reg logical,integer,numeric,character,factor,ordered
##  7: classif.naive_bayes                   logical,integer,numeric,factor
##  8:         classif.qda           logical,integer,numeric,factor,ordered
##  9:      classif.ranger logical,integer,numeric,character,factor,ordered
## 10:       classif.rpart           logical,integer,numeric,factor,ordered
## 11:         classif.svm                          logical,integer,numeric
## 12:     classif.xgboost                          logical,integer,numeric
## 13:    regr.featureless logical,integer,numeric,character,factor,ordered
## 14:         regr.glmnet                          logical,integer,numeric
## 15:           regr.kknn           logical,integer,numeric,factor,ordered
## 16:             regr.km                          logical,integer,numeric
## 17:             regr.lm                   logical,integer,numeric,factor
## 18:         regr.ranger logical,integer,numeric,character,factor,ordered
## 19:          regr.rpart           logical,integer,numeric,factor,ordered
## 20:            regr.svm                          logical,integer,numeric
## 21:        regr.xgboost                          logical,integer,numeric
##                     key                                    feature_types
##        packages
##  1:
##  2:
##  3:      glmnet
##  4:        kknn
##  5:        MASS
##  6:       stats
##  7:       e1071
##  8:        MASS
##  9:      ranger
## 10:       rpart
## 11:       e1071
## 12:     xgboost
## 13:       stats
## 14:      glmnet
## 15:        kknn
## 16: DiceKriging
## 17:       stats
## 18:      ranger
## 19:       rpart
## 20:       e1071
## 21:     xgboost
##        packages
##                                                            properties
##  1:                                      missings,multiclass,twoclass
##  2:         importance,missings,multiclass,selected_features,twoclass
##  3:                                       multiclass,twoclass,weights
##  4:                                               multiclass,twoclass
##  5:                                       multiclass,twoclass,weights
##  6:                                                  twoclass,weights
##  7:                                               multiclass,twoclass
##  8:                                       multiclass,twoclass,weights
##  9:                  importance,multiclass,oob_error,twoclass,weights
## 10: importance,missings,multiclass,selected_features,twoclass,weights
## 11:                                               multiclass,twoclass
## 12:                   importance,missings,multiclass,twoclass,weights
## 13:                             importance,missings,selected_features
## 14:                                                           weights
## 15:
## 16:
## 17:                                                           weights
## 18:                                      importance,oob_error,weights
## 19:                     importance,missings,selected_features,weights
## 20:
## 21:                                       importance,missings,weights
##                                                            properties
##     predict_types
##  1: response,prob
##  2: response,prob
##  3: response,prob
##  4: response,prob
##  5: response,prob
##  6: response,prob
##  7: response,prob
##  8: response,prob
##  9: response,prob
## 10: response,prob
## 11: response,prob
## 12: response,prob
## 13:   response,se
## 14:      response
## 15:      response
## 16:   response,se
## 17:   response,se
## 18:   response,se
## 19:      response
## 20:      response
## 21:      response
##     predict_types</code></pre></div>
<p>We can now pick the logistic one.
<a href="https://github.com/mlr-org/mlr3learners/" target="_blank">Note that</a> this essentially
proxies our requests down to the <code>stats</code> package.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">learner <span style="color:#f92672">=</span> mlr_learners<span style="color:#f92672">$</span><span style="color:#a6e22e">get</span>(<span style="color:#e6db74">&#34;classif.log_reg&#34;</span>)</code></pre></div>
<p>Now we can final solve the question, which is to simply use the model on
all our data and return the accuracy metrics.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">trainFullCred<span style="color:#f92672">=</span>learner<span style="color:#f92672">$</span><span style="color:#a6e22e">train</span>(tskLogiFull)
<span style="color:#a6e22e">print</span>(learner<span style="color:#f92672">$</span><span style="color:#a6e22e">predict</span>(tskLogiFull)<span style="color:#f92672">$</span>confusion)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##         truth
## response   No  Yes
##      No  9629  225
##      Yes   38  108</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">measure <span style="color:#f92672">=</span> <span style="color:#a6e22e">msr</span>(<span style="color:#e6db74">&#34;classif.acc&#34;</span>)
<span style="color:#a6e22e">print</span>(learner<span style="color:#f92672">$</span><span style="color:#a6e22e">predict</span>(tskLogiFull)<span style="color:#f92672">$</span><span style="color:#a6e22e">score</span>(measure))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## classif.acc
##      0.9737</code></pre></div>
<p>Note that this style of working with objects does not really utilize the
familiar <code>%&gt;%</code> interface.</p>

<p>The <code>caret</code> package still has neater default metrics so we will use that
as well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">confusionMatrix</span>(learner<span style="color:#f92672">$</span><span style="color:#a6e22e">predict</span>(tskLogiFull)<span style="color:#f92672">$</span>response,defDat<span style="color:#f92672">$</span>default)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Confusion Matrix and Statistics
##
##           Reference
## Prediction   No  Yes
##        No  9629  225
##        Yes   38  108
##
##                Accuracy : 0.9737
##                  95% CI : (0.9704, 0.9767)
##     No Information Rate : 0.9667
##     P-Value [Acc &gt; NIR] : 3.067e-05
##
##                   Kappa : 0.4396
##
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16
##
##             Sensitivity : 0.9961
##             Specificity : 0.3243
##          Pos Pred Value : 0.9772
##          Neg Pred Value : 0.7397
##              Prevalence : 0.9667
##          Detection Rate : 0.9629
##    Detection Prevalence : 0.9854
##       Balanced Accuracy : 0.6602
##
##        &#39;Positive&#39; Class : No
##</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">autoplot</span>(learner<span style="color:#f92672">$</span><span style="color:#a6e22e">predict</span>(tskLogiFull))</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol3/unnamed-chunk-14-1.png"   />

        
            <figcaption class="center" >Figure 3: Autoplot results</figcaption>
        
    </figure>



<p>We can get some other plots as well, but we need our probabilities to be
returned.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># For ROC curves</span>
lrnprob <span style="color:#f92672">=</span> <span style="color:#a6e22e">lrn</span>(<span style="color:#e6db74">&#34;classif.log_reg&#34;</span>,predict_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;prob&#34;</span>)
lrnprob<span style="color:#f92672">$</span><span style="color:#a6e22e">train</span>(tskLogiFull)
<span style="color:#a6e22e">autoplot</span>(lrnprob<span style="color:#f92672">$</span><span style="color:#a6e22e">predict</span>(tskLogiFull),type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;roc&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol3/unnamed-chunk-15-1.png"   />

        
            <figcaption class="center" >Figure 4: ROC curve</figcaption>
        
    </figure>



<h4 id="b-validation-sets-with-mlr3">b) Validation Sets with mlr3</h4>

<p>Though the question seems to require a manual validation set generation
and thresholding, we can simply use the defaults.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">train_set <span style="color:#f92672">=</span> <span style="color:#a6e22e">sample</span>(tskLogiFull<span style="color:#f92672">$</span>nrow, <span style="color:#ae81ff">0.8</span> <span style="color:#f92672">*</span> tskLogiFull<span style="color:#f92672">$</span>nrow)
test_set <span style="color:#f92672">=</span> <span style="color:#a6e22e">setdiff</span>(<span style="color:#a6e22e">seq_len</span>(tskLogiFull<span style="color:#f92672">$</span>nrow), train_set)
learner<span style="color:#f92672">$</span><span style="color:#a6e22e">train</span>(tskLogiFull,row_ids<span style="color:#f92672">=</span>train_set)
<span style="color:#a6e22e">confusionMatrix</span>(learner<span style="color:#f92672">$</span><span style="color:#a6e22e">predict</span>(tskLogiFull, row_ids<span style="color:#f92672">=</span>test_set)<span style="color:#f92672">$</span>response,defDat[<span style="color:#f92672">-</span>train_set,]<span style="color:#f92672">$</span>default)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Confusion Matrix and Statistics
##
##           Reference
## Prediction   No  Yes
##        No  1921   47
##        Yes    9   23
##
##                Accuracy : 0.972
##                  95% CI : (0.9638, 0.9788)
##     No Information Rate : 0.965
##     P-Value [Acc &gt; NIR] : 0.04663
##
##                   Kappa : 0.4387
##
##  Mcnemar&#39;s Test P-Value : 7.641e-07
##
##             Sensitivity : 0.9953
##             Specificity : 0.3286
##          Pos Pred Value : 0.9761
##          Neg Pred Value : 0.7188
##              Prevalence : 0.9650
##          Detection Rate : 0.9605
##    Detection Prevalence : 0.9840
##       Balanced Accuracy : 0.6620
##
##        &#39;Positive&#39; Class : No
##</code></pre></div>
<p>For a reasonable comparison, we will demonstrate a standard approach as
well. In this instance we will not use <code>caret</code> to ensure that our class
distribution in the train and test sets are not sampled to remain the
same.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">trainNoCaret<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">sample</span>(<span style="color:#a6e22e">nrow</span>(defDat), size <span style="color:#f92672">=</span> <span style="color:#a6e22e">floor</span>(<span style="color:#ae81ff">.8</span><span style="color:#f92672">*</span><span style="color:#a6e22e">nrow</span>(defDat)), replace <span style="color:#f92672">=</span> F)
glm.fit<span style="color:#f92672">=</span><span style="color:#a6e22e">glm</span>(default<span style="color:#f92672">~</span>income<span style="color:#f92672">+</span>balance,data<span style="color:#f92672">=</span>defDat,family<span style="color:#f92672">=</span>binomial,subset<span style="color:#f92672">=</span>trainNoCaret)
glm.probs<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">predict</span>(glm.fit,defDat[<span style="color:#f92672">-</span>trainNoCaret,],type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;response&#34;</span>)
glm.preds<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">ifelse</span>(glm.probs <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.5</span>, <span style="color:#e6db74">&#34;No&#34;</span>, <span style="color:#e6db74">&#34;Yes&#34;</span>)
<span style="color:#a6e22e">confusionMatrix</span>(glm.preds <span style="color:#f92672">%&gt;%</span> factor,defDat[<span style="color:#f92672">-</span>trainNoCaret,]<span style="color:#f92672">$</span>default)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Confusion Matrix and Statistics
##
##           Reference
## Prediction   No  Yes
##        No  1930   46
##        Yes    6   18
##
##                Accuracy : 0.974
##                  95% CI : (0.966, 0.9805)
##     No Information Rate : 0.968
##     P-Value [Acc &gt; NIR] : 0.06859
##
##                   Kappa : 0.3986
##
##  Mcnemar&#39;s Test P-Value : 6.362e-08
##
##             Sensitivity : 0.9969
##             Specificity : 0.2812
##          Pos Pred Value : 0.9767
##          Neg Pred Value : 0.7500
##              Prevalence : 0.9680
##          Detection Rate : 0.9650
##    Detection Prevalence : 0.9880
##       Balanced Accuracy : 0.6391
##
##        &#39;Positive&#39; Class : No
##</code></pre></div>
<p>Since the two approaches use different samples there is a little
variation, but we can see that the accuracy is essentially the same.</p>

<h4 id="c-3-fold-cross-validation">c) 3-fold cross validation</h4>

<p>As per the question, we can repeat the block above three times, or
extract it into a function which takes a seed value and run that three
times. Either way, here we will present the <code>mlr3</code> approach to cross
validation and resampling.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">rr <span style="color:#f92672">=</span> <span style="color:#a6e22e">resample</span>(tskLogiFull, lrnprob, <span style="color:#a6e22e">rsmp</span>(<span style="color:#e6db74">&#34;cv&#34;</span>, folds <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## INFO  [22:12:30.025] Applying learner &#39;classif.log_reg&#39; on task &#39;credit&#39; (iter 1/3)
## INFO  [22:12:30.212] Applying learner &#39;classif.log_reg&#39; on task &#39;credit&#39; (iter 2/3)
## INFO  [22:12:30.360] Applying learner &#39;classif.log_reg&#39; on task &#39;credit&#39; (iter 3/3)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">autoplot</span>(rr,type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;roc&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol3/unnamed-chunk-18-1.png"   />

        
            <figcaption class="center" >Figure 5: Resampled ROC curve</figcaption>
        
    </figure>



<p>We might want the average as well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">rr<span style="color:#f92672">$</span><span style="color:#a6e22e">aggregate</span>(<span style="color:#a6e22e">msr</span>(<span style="color:#e6db74">&#34;classif.ce&#34;</span>)) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## classif.ce
## 0.02630035</code></pre></div>
<h4 id="adding-student-as-a-dummy-variable">Adding Student as a dummy variable</h4>

<p>We will stick to the <code>mlr3</code> approach because it is faster.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">redDat2<span style="color:#f92672">&lt;-</span>defDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">mutate</span>(student<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(defDat<span style="color:#f92672">$</span>student))
tskLogi2<span style="color:#f92672">=</span>TaskClassif<span style="color:#f92672">$</span><span style="color:#a6e22e">new</span>(id<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;credit&#34;</span>,backend<span style="color:#f92672">=</span>redDat2,target<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;default&#34;</span>)
<span style="color:#a6e22e">print</span>(tskLogi2)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &lt;TaskClassif:credit&gt; (10000 x 4)
## * Target: default
## * Properties: twoclass
## * Features (3):
##   - dbl (3): balance, income, student</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">autoplot</span>(tskLogi2,type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pairs&#34;</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol3/unnamed-chunk-21-1.png"   />

        
            <figcaption class="center" >Figure 6: Logistic regression pairs data</figcaption>
        
    </figure>



<p>This gives us a visual indicator and premonition that we might not be
getting incredible results with our new variable in the mix, but we
should still work it through.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">confusionMatrix</span>(lrnprob<span style="color:#f92672">$</span><span style="color:#a6e22e">predict</span>(tskLogi2)<span style="color:#f92672">$</span>response,defDat<span style="color:#f92672">$</span>default)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Confusion Matrix and Statistics
##
##           Reference
## Prediction   No  Yes
##        No  9629  225
##        Yes   38  108
##
##                Accuracy : 0.9737
##                  95% CI : (0.9704, 0.9767)
##     No Information Rate : 0.9667
##     P-Value [Acc &gt; NIR] : 3.067e-05
##
##                   Kappa : 0.4396
##
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16
##
##             Sensitivity : 0.9961
##             Specificity : 0.3243
##          Pos Pred Value : 0.9772
##          Neg Pred Value : 0.7397
##              Prevalence : 0.9667
##          Detection Rate : 0.9629
##    Detection Prevalence : 0.9854
##       Balanced Accuracy : 0.6602
##
##        &#39;Positive&#39; Class : No
##</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">autoplot</span>(lrnprob<span style="color:#f92672">$</span><span style="color:#a6e22e">predict</span>(tskLogi2))</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol3/unnamed-chunk-23-1.png"   />

        
            <figcaption class="center" >Figure 7: Autoplot figure</figcaption>
        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lrnprob<span style="color:#f92672">$</span><span style="color:#a6e22e">train</span>(tskLogi2)
<span style="color:#a6e22e">autoplot</span>(lrnprob<span style="color:#f92672">$</span><span style="color:#a6e22e">predict</span>(tskLogi2),type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;roc&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol3/unnamed-chunk-24-1.png"   />

        
            <figcaption class="center" >Figure 8: ROC plot</figcaption>
        
    </figure>



<p>Although we have slightly better accuracy with the new variable, it
needs to be compared to determine if it is worth further investigation.</p>

<p>With a three-fold validation approach,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(<span style="color:#e6db74">&#34;gridExtra&#34;</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;gridExtra&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:dplyr&#39;:
##
##     combine</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">rr2 <span style="color:#f92672">=</span> <span style="color:#a6e22e">resample</span>(tskLogi2, lrnprob, <span style="color:#a6e22e">rsmp</span>(<span style="color:#e6db74">&#34;cv&#34;</span>, folds <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## INFO  [22:12:39.670] Applying learner &#39;classif.log_reg&#39; on task &#39;credit&#39; (iter 1/3)
## INFO  [22:12:39.731] Applying learner &#39;classif.log_reg&#39; on task &#39;credit&#39; (iter 2/3)
## INFO  [22:12:39.780] Applying learner &#39;classif.log_reg&#39; on task &#39;credit&#39; (iter 3/3)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">wS<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">autoplot</span>(rr2)
nS<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">autoplot</span>(rr)
<span style="color:#a6e22e">grid.arrange</span>(wS,nS,ncol<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,bottom<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;With student (left) and without (right)&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol3/unnamed-chunk-25-1.png"   />

        
            <figcaption class="center" >Figure 9: Plot of accuracy</figcaption>
        
    </figure>



<p>Given the results, it is fair to say that adding the student data is
useful in general.</p>

<h2 id="question-5.6---page-199">Question 5.6 - Page 199</h2>

<p>We continue to consider the use of a logistic regression model to
predict the probability of <code>default</code> using <code>income</code> and <code>balance</code> on the
<code>Default</code> data set. In particular, we will now compute estimates for the
standard errors of the <code>income</code> and <code>balance</code> logistic regression
coefficients in two different ways: (1) using the bootstrap, and (2)
using the standard formula for computing the standard errors in the
<code>glm()</code> function. Do not forget to set a random seed before beginning
your analysis.</p>

<p><strong>(a)</strong> Using the <code>summary()</code> and <code>glm()</code> functions, determine the
estimated standard errors for the coefficients associated with <code>income</code>
and <code>balance</code> in a multiple logistic regression model that uses both
predictors.</p>

<p><strong>(b)</strong> Write a function, <code>boot.fn()</code> , that takes as input the <code>Default</code>
data set as well as an index of the observations, and that outputs the
coefficient estimates for <code>income</code> and <code>balance</code> in the multiple
logistic regression model.</p>

<p><strong>&copy;</strong> Use the <code>boot()</code> function together with your <code>boot.fn()</code> function
to estimate the standard errors of the logistic regression coefficients
for <code>income</code> and <code>balance</code>.</p>

<p><strong>(d)</strong> Comment on the estimated standard errors obtained using the
<code>glm()</code> function and using your bootstrap function.</p>

<h3 id="answer-1">Answer</h3>

<p>This question is slightly more specific to the packages in the book so
we will use them.</p>

<h4 id="a-fit-summary">a) Fit summary</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">glm.fit <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## glm(formula = default ~ income + balance, family = binomial,
##     data = defDat, subset = trainNoCaret)
##
## Deviance Residuals:
##     Min       1Q   Median       3Q      Max
## -2.1943  -0.1488  -0.0588  -0.0217   3.7058
##
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -1.150e+01  4.814e-01 -23.885  &lt; 2e-16 ***
## income       2.288e-05  5.553e-06   4.121 3.78e-05 ***
## balance      5.593e-03  2.509e-04  22.295  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
##     Null deviance: 2354.0  on 7999  degrees of freedom
## Residual deviance: 1283.6  on 7997  degrees of freedom
## AIC: 1289.6
##
## Number of Fisher Scoring iterations: 8</code></pre></div>
<h4 id="b-function">b) Function</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boot.fn<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(data,subs){<span style="color:#a6e22e">return</span>(<span style="color:#a6e22e">coef</span>(<span style="color:#a6e22e">glm</span>(default<span style="color:#f92672">~</span>income<span style="color:#f92672">+</span>balance,data<span style="color:#f92672">=</span>data, family<span style="color:#f92672">=</span>binomial,subset<span style="color:#f92672">=</span>subs)))}</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">boot.fn</span>(defDat,train_set) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##   (Intercept)        income       balance
## -1.136824e+01  1.846153e-05  5.576468e-03</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">glm</span>(default<span style="color:#f92672">~</span>income<span style="color:#f92672">+</span>balance,data<span style="color:#f92672">=</span>defDat,family<span style="color:#f92672">=</span>binomial,subset<span style="color:#f92672">=</span>train_set) <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## glm(formula = default ~ income + balance, family = binomial,
##     data = defDat, subset = train_set)
##
## Deviance Residuals:
##     Min       1Q   Median       3Q      Max
## -2.4280  -0.1465  -0.0582  -0.0218   3.7115
##
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -1.137e+01  4.813e-01 -23.618  &lt; 2e-16 ***
## income       1.846e-05  5.553e-06   3.324 0.000886 ***
## balance      5.576e-03  2.529e-04  22.046  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
##     Null deviance: 2313.6  on 7999  degrees of freedom
## Residual deviance: 1266.4  on 7997  degrees of freedom
## AIC: 1272.4
##
## Number of Fisher Scoring iterations: 8</code></pre></div>
<p>We see that the statistics obtained from both are the same.</p>

<h4 id="c-bootstrap">c) Bootstrap</h4>

<p>The old fashioned way. <code>R</code> is the resample rate, <code>boot.fn</code> is the
statistic used.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(boot)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;boot&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following object is masked from &#39;package:lattice&#39;:
##
##     melanoma</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">boot</span>(defDat,boot.fn,R<span style="color:#f92672">=</span><span style="color:#ae81ff">184</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## ORDINARY NONPARAMETRIC BOOTSTRAP
##
##
## Call:
## boot(data = defDat, statistic = boot.fn, R = 184)
##
##
## Bootstrap Statistics :
##          original        bias     std. error
## t1* -1.154047e+01 -1.407368e-02 4.073453e-01
## t2*  2.080898e-05 -6.386634e-08 4.720109e-06
## t3*  5.647103e-03  1.350950e-05 2.111547e-04</code></pre></div>
<h4 id="d-comparison">d) Comparison</h4>

<ul>
<li>Clearly, there is not much difference in the standard error estimates</li>
</ul>

<p class="verse">
Var | Bootstrap | Summary |<br />
| :---------: | --------- |<br />
Intercept | 4.428026e-01 | 4.883e-01 |<br />
income | 2.797011e-06 | 5.548e-06 |<br />
balance | 2.423002e-04 | 2.591e-04 |<br />
</p>

<h2 id="question-5.8---page-200">Question 5.8 - Page 200</h2>

<p>We will now perform cross-validation on a simulated data set. <strong>(a)</strong>
Generate a simulated data set as follows:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#f92672">&gt;</span> set <span style="color:#a6e22e">. seed </span>(<span style="color:#ae81ff">1</span>)
<span style="color:#f92672">&gt;</span> y <span style="color:#f92672">=</span> <span style="color:#a6e22e">rnorm </span>(<span style="color:#ae81ff">100</span>)
<span style="color:#f92672">&gt;</span> x <span style="color:#f92672">=</span> <span style="color:#a6e22e">rnorm </span>(<span style="color:#ae81ff">100</span>)
<span style="color:#f92672">&gt;</span> y <span style="color:#f92672">=</span>x <span style="color:#ae81ff">-2</span>\<span style="color:#f92672">*</span> x ^2<span style="color:#f92672">+</span> <span style="color:#a6e22e">rnorm </span>(<span style="color:#ae81ff">100</span>)</code></pre></div>
<p>In this data set, what is n and what is p? Write out the model used to
generate the data in equation form.</p>

<p><strong>(b)</strong> Create a scatterplot of \(X\) against \(Y\). Comment on what you find.</p>

<p><strong>&copy;</strong> Set a random seed, and then compute the LOOCV errors that result
from fitting the following four models using least squares:</p>

<ol>
<li><p>\(Y=\beta_0+\beta_1X+\eta\)</p></li>

<li><p>\(Y=\beta_0+\beta_1X+\beta_2X^2+\eta\)</p></li>

<li><p>\(Y=\beta_0+\beta_1X+\beta_2X^2+\beta_{3}X^{3}+\eta\)</p></li>

<li><p>\(Y=\beta_0+\beta_1X+\beta_2X^2+\beta_{3}X^{3}+\beta_{4}X^{4}+\eta\)</p></li>
</ol>

<p>Note you may find it helpful to use the <code>data.frame()</code> function to
create a single data set containing both \(X\) and \(Y\).</p>

<p><strong>(d)</strong> Repeat &copy; using another random seed, and report your results. Are
your results the same as what you got in &copy;? Why?</p>

<p><strong>(e)</strong> Which of the models in &copy; had the smallest LOOCV error? Is this
what you expected? Explain your answer.</p>

<p><strong>(f)</strong> Comment on the statistical significance of the coefficient esti-
mates that results from fitting each of the models in &copy; using least
squares. Do these results agree with the conclusions drawn based on the
cross-validation results?</p>

<h3 id="answer-2">Answer</h3>

<h4 id="a-modeling-data">a) Modeling data</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">1</span>)
y <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rnorm</span>(<span style="color:#ae81ff">100</span>)
x <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rnorm</span>(<span style="color:#ae81ff">100</span>)
y <span style="color:#f92672">&lt;-</span> x <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>x^2 <span style="color:#f92672">+</span> <span style="color:#a6e22e">rnorm</span>(<span style="color:#ae81ff">100</span>)</code></pre></div>
<p>Clearly:</p>

<ul>
<li>Our equation is \(y=x-2x^{2}+\epsilon\) where \(epsilon\) is normally
distributed from 100 samples</li>
<li>We have \(n=100\) observations</li>
<li>\(p=2\) where \(p\) is the number of features</li>
</ul>

<h4 id="b-visual-inspection">b) Visual inspection</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">qplot</span>(x,y)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol3/unnamed-chunk-31-1.png"   />

        
            <figcaption class="center" >Figure 10: Model data plot</figcaption>
        
    </figure>



<p>We observe that the data is quadratic, as we also know from the
generating function, which was a quadratic equation plus normally
distributed noise.</p>

<h4 id="c-least-squares-fits">c) Least squares fits</h4>

<p>Not very important, but here we use the <code>caret</code> form.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">pow<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(x,y){<span style="color:#a6e22e">return</span>(x^y)}
dfDat <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">data.frame</span>(y,x,x2<span style="color:#f92672">=</span><span style="color:#a6e22e">pow</span>(x,<span style="color:#ae81ff">2</span>),x3<span style="color:#f92672">=</span><span style="color:#a6e22e">pow</span>(x,<span style="color:#ae81ff">3</span>),x4<span style="color:#f92672">=</span><span style="color:#a6e22e">pow</span>(x,<span style="color:#ae81ff">4</span>))</code></pre></div>
<p>We might have also just used <code>poly(x,n)</code> to skip making the data frame.</p>

<p>We will set our resampling method as follows:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">fitControl<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">trainControl</span>(method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOOCV&#34;</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>x,data<span style="color:#f92672">=</span>dfDat,trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   1 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared    MAE
##   2.427134  0.05389864  1.878566
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>x<span style="color:#f92672">+</span>x2,data<span style="color:#f92672">=</span>dfDat,trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   2 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared   MAE
##   1.042399  0.8032414  0.8029942
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>x<span style="color:#f92672">+</span>x2<span style="color:#f92672">+</span>x3,data<span style="color:#f92672">=</span>dfDat,trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   3 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared   MAE
##   1.050041  0.8003517  0.8073024
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>x<span style="color:#f92672">+</span>x2<span style="color:#f92672">+</span>x3<span style="color:#f92672">+</span>x4,data<span style="color:#f92672">=</span>dfDat,trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   4 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared   MAE
##   1.055828  0.7982111  0.8150296
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div>
<h4 id="d-seeding-effects">d) Seeding effects</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">1995</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>x,data<span style="color:#f92672">=</span>dfDat,trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   1 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared    MAE
##   2.427134  0.05389864  1.878566
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>x<span style="color:#f92672">+</span>x2,data<span style="color:#f92672">=</span>dfDat,trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   2 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared   MAE
##   1.042399  0.8032414  0.8029942
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>x<span style="color:#f92672">+</span>x2<span style="color:#f92672">+</span>x3,data<span style="color:#f92672">=</span>dfDat,trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   3 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared   MAE
##   1.050041  0.8003517  0.8073024
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>x<span style="color:#f92672">+</span>x2<span style="color:#f92672">+</span>x3<span style="color:#f92672">+</span>x4,data<span style="color:#f92672">=</span>dfDat,trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   4 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared   MAE
##   1.055828  0.7982111  0.8150296
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div>
<p>We note that there is no change on varying the seed because LOOCV is
exhaustive and uses n folds for each observation.</p>

<h4 id="e-analysis">e) Analysis</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>x,data<span style="color:#f92672">=</span>dfDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(y,x)),trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   1 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared    MAE
##   2.427134  0.05389864  1.878566
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(x,<span style="color:#ae81ff">2</span>),data<span style="color:#f92672">=</span>dfDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(y,x)),trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   1 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared   MAE
##   1.042399  0.8032414  0.8029942
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(x,<span style="color:#ae81ff">3</span>),data<span style="color:#f92672">=</span>dfDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(y,x)),trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   1 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared   MAE
##   1.050041  0.8003517  0.8073024
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(x,<span style="color:#ae81ff">4</span>),data<span style="color:#f92672">=</span>dfDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(y,x)),trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Linear Regression
##
## 100 samples
##   1 predictor
##
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation
## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ...
## Resampling results:
##
##   RMSE      Rsquared   MAE
##   1.055828  0.7982111  0.8150296
##
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre></div>
<p>Clearly the quadratic polynomial has the lowest error, which makes sense
given how the data was generated.</p>

<h4 id="f-statistical-significance">f) Statistical significance</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span>x,data<span style="color:#f92672">=</span>dfDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(y,x)),trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = .outcome ~ ., data = dat)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -7.3469 -0.9275  0.8028  1.5608  4.3974
##
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  -1.8185     0.2364  -7.692 1.14e-11 ***
## x             0.2430     0.2479   0.981    0.329
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 2.362 on 98 degrees of freedom
## Multiple R-squared:  0.009717,   Adjusted R-squared:  -0.0003881
## F-statistic: 0.9616 on 1 and 98 DF,  p-value: 0.3292</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(x,<span style="color:#ae81ff">2</span>),data<span style="color:#f92672">=</span>dfDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(y,x)),trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = .outcome ~ ., data = dat)
##
## Residuals:
##      Min       1Q   Median       3Q      Max
## -2.89884 -0.53765  0.04135  0.61490  2.73607
##
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    -1.8277     0.1032 -17.704   &lt;2e-16 ***
## `poly(x, 2)1`   2.3164     1.0324   2.244   0.0271 *
## `poly(x, 2)2` -21.0586     1.0324 -20.399   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 1.032 on 97 degrees of freedom
## Multiple R-squared:  0.8128, Adjusted R-squared:  0.8089
## F-statistic: 210.6 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(x,<span style="color:#ae81ff">3</span>),data<span style="color:#f92672">=</span>dfDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(y,x)),trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = .outcome ~ ., data = dat)
##
## Residuals:
##      Min       1Q   Median       3Q      Max
## -2.87250 -0.53881  0.02862  0.59383  2.74350
##
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    -1.8277     0.1037 -17.621   &lt;2e-16 ***
## `poly(x, 3)1`   2.3164     1.0372   2.233   0.0279 *
## `poly(x, 3)2` -21.0586     1.0372 -20.302   &lt;2e-16 ***
## `poly(x, 3)3`  -0.3048     1.0372  -0.294   0.7695
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 1.037 on 96 degrees of freedom
## Multiple R-squared:  0.813,  Adjusted R-squared:  0.8071
## F-statistic: 139.1 on 3 and 96 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">train</span>(y<span style="color:#f92672">~</span><span style="color:#a6e22e">poly</span>(x,<span style="color:#ae81ff">4</span>),data<span style="color:#f92672">=</span>dfDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(y,x)),trControl<span style="color:#f92672">=</span>fitControl,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lm&#34;</span>) <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = .outcome ~ ., data = dat)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -2.8914 -0.5244  0.0749  0.5932  2.7796
##
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    -1.8277     0.1041 -17.549   &lt;2e-16 ***
## `poly(x, 4)1`   2.3164     1.0415   2.224   0.0285 *
## `poly(x, 4)2` -21.0586     1.0415 -20.220   &lt;2e-16 ***
## `poly(x, 4)3`  -0.3048     1.0415  -0.293   0.7704
## `poly(x, 4)4`  -0.4926     1.0415  -0.473   0.6373
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 1.041 on 95 degrees of freedom
## Multiple R-squared:  0.8134, Adjusted R-squared:  0.8055
## F-statistic: 103.5 on 4 and 95 DF,  p-value: &lt; 2.2e-16</code></pre></div>
<ul>
<li>Clearly, the second order terms are the most significant, as expected</li>
</ul>

<h2 id="question-5.9---page-201">Question 5.9 - Page 201</h2>

<p>We will now consider the Boston housing data set, from the MASS library.</p>

<p><strong>(a)</strong> Based on this data set, provide an estimate for the population
mean of <code>medv</code>. Call this estimate \(\hat{\mu}\).</p>

<p><strong>(b)</strong> Provide an estimate of the standard error of \(\hat{\mu}\).
Interpret this result. <em>Hint: We can compute the standard error of the
sample mean by dividing the sample standard deviation by the square root
of the number of observations.</em></p>

<p><strong>&copy;</strong> Now estimate the standard error of \(\hat{\mu}\) using the
bootstrap. How does this compare to your answer from (b)?</p>

<p><strong>(d)</strong> Based on your bootstrap estimate from &copy;, provide a 95 %
confidence interval for the mean of <code>medv</code>. Compare it to the results
obtained using <code>t.test(Boston\$medv)</code>. <em>Hint: You can approximate a 95 %
confidence interval using the formula
\([\hat{\mu} − 2SE(\hat{\mu}), \hat{\mu} + 2SE(\hat{\mu})]\).</em></p>

<p><strong>(e)</strong> Based on this data set, provide an estimate, \(\hat{\mu_{med}}\),
for the median value of <code>medv</code> in the population.</p>

<p><strong>(f)</strong> We now would like to estimate the standard error of \(\hat{\mu}\)
med. Unfortunately, there is no simple formula for computing the
standard error of the median. Instead, estimate the standard error of
the median using the bootstrap. Comment on your findings.</p>

<p><strong>(g)</strong> Based on this data set, provide an estimate for the tenth
percentile of <code>medv</code> in Boston suburbs. Call this quantity
\(\hat{\mu_{0.1}}\). (You can use the <code>quantile()</code> function.)</p>

<p><strong>(h)</strong> Use the bootstrap to estimate the standard error of
\(\hat{\mu_{0.1}}\). Comment on your findings.</p>

<h3 id="answer-3">Answer</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">&lt;-</span>MASS<span style="color:#f92672">::</span>Boston</code></pre></div>
<ul>
<li>Reminder</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> summary <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       crim                zn             indus            chas
##  Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000
##  1st Qu.: 0.08204   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000
##  Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000
##  Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917
##  3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000
##  Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000
##       nox               rm             age              dis
##  Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130
##  1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100
##  Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207
##  Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795
##  3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188
##  Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127
##       rad              tax           ptratio          black
##  Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32
##  1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38
##  Median : 5.000   Median :330.0   Median :19.05   Median :391.44
##  Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67
##  3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23
##  Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90
##      lstat            medv
##  Min.   : 1.73   Min.   : 5.00
##  1st Qu.: 6.95   1st Qu.:17.02
##  Median :11.36   Median :21.20
##  Mean   :12.65   Mean   :22.53
##  3rd Qu.:16.95   3rd Qu.:25.00
##  Max.   :37.97   Max.   :50.00</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> str <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    506 obs. of  14 variables:
##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
##  $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...
##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
##  $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...
##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
##  $ black  : num  397 397 393 395 397 ...
##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...
## NULL</code></pre></div>
<h4 id="a-mean">a) Mean</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">muhat<span style="color:#f92672">=</span>boston<span style="color:#f92672">$</span>medv <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">mean</span>()
<span style="color:#a6e22e">print</span>(muhat)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 22.53281</code></pre></div>
<h4 id="b-standard-error">b) Standard error</h4>

<p>Recall that \(SE=\frac{SD}{\sqrt{N_{obs}}}\)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">$</span>medv <span style="color:#f92672">%&gt;%</span> sd<span style="color:#f92672">/</span>(<span style="color:#a6e22e">nrow</span>(boston)^0.5) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 22.49444</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 0.4088611</code></pre></div>
<h4 id="c-bootstrap-estimate">c) Bootstrap estimate</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(boot)
myMean<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">function</span>(frame,ind){<span style="color:#a6e22e">return</span>(<span style="color:#a6e22e">mean</span>(frame[ind]))}</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">boot</span>(boston<span style="color:#f92672">$</span>medv,myMean,R<span style="color:#f92672">=</span><span style="color:#ae81ff">184</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## ORDINARY NONPARAMETRIC BOOTSTRAP
##
##
## Call:
## boot(data = boston$medv, statistic = myMean, R = 184)
##
##
## Bootstrap Statistics :
##     original     bias    std. error
## t1* 22.53281 0.03451839    0.409621</code></pre></div>
<p>We see that the bootstrapped error over 184 samples is <code>0.4341499</code> while
without it we had <code>0.4088611</code> which is similar enough.</p>

<h4 id="d-confidence-intervals-with-bootstrap-and-t.test">d) Confidence intervals with bootstrap and t.test</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">$</span>medv <span style="color:#f92672">%&gt;%</span> t.test <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
##  One Sample t-test
##
## data:  .
## t = 55.111, df = 505, p-value &lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  21.72953 23.33608
## sample estimates:
## mean of x
##  22.53281</code></pre></div>
<p>We can approximate this with what we already have</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">bRes<span style="color:#f92672">=</span><span style="color:#a6e22e">boot</span>(boston<span style="color:#f92672">$</span>medv,myMean,R<span style="color:#f92672">=</span><span style="color:#ae81ff">184</span>)
seBoot<span style="color:#f92672">&lt;-</span>bRes<span style="color:#f92672">$</span>t <span style="color:#f92672">%&gt;%</span> var <span style="color:#f92672">%&gt;%</span> sqrt
xlow<span style="color:#f92672">=</span>muhat<span style="color:#ae81ff">-2</span><span style="color:#f92672">*</span>(seBoot)
xhigh<span style="color:#f92672">=</span>muhat<span style="color:#ae81ff">+2</span><span style="color:#f92672">*</span>(seBoot)
<span style="color:#a6e22e">c</span>(xlow,xhigh) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 21.72675 23.33887</code></pre></div>
<p>Our intervals are also pretty close to each other.</p>

<h4 id="e-median">e) Median</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">$</span>medv <span style="color:#f92672">%&gt;%</span> sort <span style="color:#f92672">%&gt;%</span> median <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 21.2</code></pre></div>
<h4 id="f-median-standard-error">f) Median standard error</h4>

<p>We can reuse the logic of the <code>myMean</code> function defined previously.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">myMedian<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(data,ind){<span style="color:#a6e22e">return</span>(<span style="color:#a6e22e">median</span>(data[ind]))}</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">$</span>medv <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">boot</span>(myMedian,R<span style="color:#f92672">=</span><span style="color:#ae81ff">1500</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## ORDINARY NONPARAMETRIC BOOTSTRAP
##
##
## Call:
## boot(data = ., statistic = myMedian, R = 1500)
##
##
## Bootstrap Statistics :
##     original      bias    std. error
## t1*     21.2 -0.03773333    0.387315</code></pre></div>
<p>We see that the standard error is <code>0.3767072</code>.</p>

<h4 id="g-tenth-percentile">g) Tenth percentile</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">mu0one<span style="color:#f92672">&lt;-</span>boston<span style="color:#f92672">$</span>medv <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">quantile</span>(<span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">0.1</span>))
<span style="color:#a6e22e">print</span>(mu0one)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##   10%
## 12.75</code></pre></div>
<h4 id="h-bootstrap">h) Bootstrap</h4>

<p>Once again.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">myQuant<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(data,ind){<span style="color:#a6e22e">return</span>(<span style="color:#a6e22e">quantile</span>(data[ind],<span style="color:#ae81ff">0.1</span>))}</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">$</span>medv <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">boot</span>(myQuant,R<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>) <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## ORDINARY NONPARAMETRIC BOOTSTRAP
##
##
## Call:
## boot(data = ., statistic = myQuant, R = 500)
##
##
## Bootstrap Statistics :
##     original  bias    std. error
## t1*    12.75 -0.0095   0.4951415</code></pre></div>
<p>The standard error is <code>0.5024526</code></p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1">James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science &amp; Business Media.
 <a class="footnote-return" href="#fnref:fn-1"><sup>[return]</sup></a></li>
<li id="fn:fn-2">Lang et al., (2019). mlr3: A modern object-oriented machine learning framework in R. Journal of Open Source Software, 4(44), 1903, <a href="https://doi.org/10.21105/joss.01903" target="_blank">https://doi.org/10.21105/joss.01903</a>
 <a class="footnote-return" href="#fnref:fn-2"><sup>[return]</sup></a></li>
</ol>
</div>
]]></content>
        </item>
        
        <item>
            <title>ISLR :: Classification</title>
            <link>https://rgoswami.me/posts/islr-ch4/</link>
            <pubDate>Mon, 17 Feb 2020 15:28:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/islr-ch4/</guid>
            <description>Chapter IV - Classification All the questions are as per the ISL seventh printing&amp;nbsp;1.
Common Stuff Here I&amp;rsquo;ll load things I will be using throughout, mostly libraries.
libsUsed&amp;lt;-c(&amp;#34;dplyr&amp;#34;,&amp;#34;ggplot2&amp;#34;,&amp;#34;tidyverse&amp;#34;,&amp;#34;ISLR&amp;#34;,&amp;#34;caret&amp;#34;) invisible(lapply(libsUsed, library, character.only = TRUE))# # Attaching package: &amp;#39;dplyr&amp;#39;# The following objects are masked from &amp;#39;package:stats&amp;#39;: # # filter, lag# The following objects are masked from &amp;#39;package:base&amp;#39;: # # intersect, setdiff, setequal, union# ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──# ✔ tibble 2.</description>
            <content type="html"><![CDATA[

<h2 id="chapter-iv---classification">Chapter IV - Classification</h2>

<p>All the questions are as per the
<a href="https://faculty.marshall.usc.edu/gareth-james/ISL/" target="_blank">ISL seventh
printing</a>&nbsp;<sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup>.</p>

<h3 id="common-stuff">Common Stuff</h3>

<p>Here I&rsquo;ll load things I will be using throughout, mostly libraries.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">libsUsed<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">c</span>(<span style="color:#e6db74">&#34;dplyr&#34;</span>,<span style="color:#e6db74">&#34;ggplot2&#34;</span>,<span style="color:#e6db74">&#34;tidyverse&#34;</span>,<span style="color:#e6db74">&#34;ISLR&#34;</span>,<span style="color:#e6db74">&#34;caret&#34;</span>)
<span style="color:#a6e22e">invisible</span>(<span style="color:#a6e22e">lapply</span>(libsUsed, library, character.only <span style="color:#f92672">=</span> <span style="color:#66d9ef">TRUE</span>))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#
# Attaching package: &#39;dplyr&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># The following objects are masked from &#39;package:stats&#39;:
#
#     filter, lag</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># The following objects are masked from &#39;package:base&#39;:
#
#     intersect, setdiff, setequal, union</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># ✔ tibble  2.1.3     ✔ purrr   0.3.3
# ✔ tidyr   1.0.0     ✔ stringr 1.4.0
# ✔ readr   1.3.1     ✔ forcats 0.4.0</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
# ✖ dplyr::filter() masks stats::filter()
# ✖ dplyr::lag()    masks stats::lag()</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Loading required package: lattice</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#
# Attaching package: &#39;caret&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># The following object is masked from &#39;package:purrr&#39;:
#
#     lift</code></pre></div>
<h2 id="question-4.10---page-171">Question 4.10 - Page 171</h2>

<p>This question should be answered using the <code>Weekly</code> data set, which is
part of the ISLR package. This data is similar in nature to the Smarket
data from this chapter&rsquo;s lab, except that it contains 1, 089 weekly
returns for 21 years, from the beginning of 1990 to the end of 2010.</p>

<p><strong>(a)</strong> Produce some numerical and graphical summaries of the Weekly data.
Do there appear to be any patterns?</p>

<p><strong>(b)</strong> Use the full data set to perform a logistic regression with
<code>Direction</code> as the response and the five lag variables plus <code>Volume</code> as
predictors. Use the summary function to print the results. Do any of the
predictors appear to be statistically significant? If so, which ones?</p>

<p><strong>&copy;</strong> Compute the confusion matrix and overall fraction of correct
predictions. Explain what the confusion matrix is telling you about the
types of mistakes made by logistic regression.</p>

<p><strong>(d)</strong> Now fit the logistic regression model using a training data period
from 1990 to 2008, with <code>Lag2</code> as the only predictor. Compute the
confusion matrix and the overall fraction of correct predictions for the
held out data (that is, the data from 2009 and 2010).</p>

<p><strong>(e)</strong> Repeat <strong>(d)</strong> using LDA.</p>

<p><strong>(f)</strong> Repeat <strong>(d)</strong> using QDA.</p>

<p><strong>(g)</strong> Repeat <strong>(d)</strong> using KNN with \(K = 1\).</p>

<p><strong>(h)</strong> Which of these methods appears to provide the best results on this
data?</p>

<p><strong>(i)</strong> Experiment with different combinations of predictors, including
possible transformations and interactions, for each of the methods.
Report the variables, method, and associated confusion matrix that
appears to provide the best results on the held out data. Note that you
should also experiment with values for K in the KNN classifier.</p>

<h3 id="answer">Answer</h3>

<p>We will need the data in a variable for ease of use.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat<span style="color:#f92672">&lt;-</span>ISLR<span style="color:#f92672">::</span>Weekly</code></pre></div>
<h3 id="a-summary-statistics">a) Summary Statistics</h3>

<h4 id="text">Text</h4>

<p>Most of this segment relies heavily on usage of <code>dplyr</code> and especially
the <code>%&gt;%</code> or pipe operator for readability. The use of the <code>skimr</code>
package<sup class="footnote-ref" id="fnref:fn-2"><a href="#fn:fn-2">2</a></sup> might added more descriptive statistics, but is not
covered here.</p>

<h4 id="basic-summaries">Basic Summaries</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> str</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># &#39;data.frame&#39;: 1089 obs. of  9 variables:
#  $ Year     : num  1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ...
#  $ Lag1     : num  0.816 -0.27 -2.576 3.514 0.712 ...
#  $ Lag2     : num  1.572 0.816 -0.27 -2.576 3.514 ...
#  $ Lag3     : num  -3.936 1.572 0.816 -0.27 -2.576 ...
#  $ Lag4     : num  -0.229 -3.936 1.572 0.816 -0.27 ...
#  $ Lag5     : num  -3.484 -0.229 -3.936 1.572 0.816 ...
#  $ Volume   : num  0.155 0.149 0.16 0.162 0.154 ...
#  $ Today    : num  -0.27 -2.576 3.514 0.712 1.178 ...
#  $ Direction: Factor w/ 2 levels &#34;Down&#34;,&#34;Up&#34;: 1 1 2 2 2 1 2 2 2 1 ...</code></pre></div>
<p>We see that there is only one <code>Factor</code>, which makes sense.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#       Year           Lag1               Lag2               Lag3
#  Min.   :1990   Min.   :-18.1950   Min.   :-18.1950   Min.   :-18.1950
#  1st Qu.:1995   1st Qu.: -1.1540   1st Qu.: -1.1540   1st Qu.: -1.1580
#  Median :2000   Median :  0.2410   Median :  0.2410   Median :  0.2410
#  Mean   :2000   Mean   :  0.1506   Mean   :  0.1511   Mean   :  0.1472
#  3rd Qu.:2005   3rd Qu.:  1.4050   3rd Qu.:  1.4090   3rd Qu.:  1.4090
#  Max.   :2010   Max.   : 12.0260   Max.   : 12.0260   Max.   : 12.0260
#       Lag4               Lag5              Volume            Today
#  Min.   :-18.1950   Min.   :-18.1950   Min.   :0.08747   Min.   :-18.1950
#  1st Qu.: -1.1580   1st Qu.: -1.1660   1st Qu.:0.33202   1st Qu.: -1.1540
#  Median :  0.2380   Median :  0.2340   Median :1.00268   Median :  0.2410
#  Mean   :  0.1458   Mean   :  0.1399   Mean   :1.57462   Mean   :  0.1499
#  3rd Qu.:  1.4090   3rd Qu.:  1.4050   3rd Qu.:2.05373   3rd Qu.:  1.4050
#  Max.   : 12.0260   Max.   : 12.0260   Max.   :9.32821   Max.   : 12.0260
#  Direction
#  Down:484
#  Up  :605
#
#
#
#</code></pre></div>
<h4 id="unique-values">Unique Values</h4>

<p>We might also want to know how many unique values are there in each
column.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(unique) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(length)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#      Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume     Today
#        21      1004      1005      1005      1005      1005      1089      1003
# Direction
#         2</code></pre></div>
<p>We note that year has disproportionately lower values, something to keep
in mind while constructing models later.</p>

<h4 id="range">Range</h4>

<p>The range of each variable might be useful as well, but we have to
ignore the factor.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(Direction)) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(range)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#      Year    Lag1    Lag2    Lag3    Lag4    Lag5   Volume   Today
# [1,] 1990 -18.195 -18.195 -18.195 -18.195 -18.195 0.087465 -18.195
# [2,] 2010  12.026  12.026  12.026  12.026  12.026 9.328214  12.026</code></pre></div>
<p>The most interesting thing about this is probably that the <code>Lag</code>
variables all have the same range, also something to be kept in mind
while applying transformations to the variable (if at all).</p>

<h4 id="mean-and-std.-dev">Mean and Std. Dev</h4>

<p>By now we might have a pretty good idea of how this will look, but it is
still worth seeing.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(Direction)) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(mean)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#         Year         Lag1         Lag2         Lag3         Lag4         Lag5
# 2000.0486685    0.1505849    0.1510790    0.1472048    0.1458182    0.1398926
#       Volume        Today
#    1.5746176    0.1498990</code></pre></div>
<p>As expected, the <code>Lag</code> values have almost the same mean, what is a bit
interesting though, is that the <code>Today</code> variable has roughly the same
mean as the <code>Lag</code> variables.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(Direction)) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(sd)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#     Year     Lag1     Lag2     Lag3     Lag4     Lag5   Volume    Today
# 6.033182 2.357013 2.357254 2.360502 2.360279 2.361285 1.686636 2.356927</code></pre></div>
<p>This is largely redundant in terms of new information.</p>

<h4 id="correlations">Correlations</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(Direction)) <span style="color:#f92672">%&gt;%</span> cor</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#               Year         Lag1        Lag2        Lag3         Lag4
# Year    1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923
# Lag1   -0.03228927  1.000000000 -0.07485305  0.05863568 -0.071273876
# Lag2   -0.03339001 -0.074853051  1.00000000 -0.07572091  0.058381535
# Lag3   -0.03000649  0.058635682 -0.07572091  1.00000000 -0.075395865
# Lag4   -0.03112792 -0.071273876  0.05838153 -0.07539587  1.000000000
# Lag5   -0.03051910 -0.008183096 -0.07249948  0.06065717 -0.075675027
# Volume  0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617
# Today  -0.03245989 -0.075031842  0.05916672 -0.07124364 -0.007825873
#                Lag5      Volume        Today
# Year   -0.030519101  0.84194162 -0.032459894
# Lag1   -0.008183096 -0.06495131 -0.075031842
# Lag2   -0.072499482 -0.08551314  0.059166717
# Lag3    0.060657175 -0.06928771 -0.071243639
# Lag4   -0.075675027 -0.06107462 -0.007825873
# Lag5    1.000000000 -0.05851741  0.011012698
# Volume -0.058517414  1.00000000 -0.033077783
# Today   0.011012698 -0.03307778  1.000000000</code></pre></div>
<p>Useful though this is, it is kind of difficult to work with, in this
form, so we might as well programmatic-ally remove strongly correlated
data instead.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># Uses caret</span>
corrCols<span style="color:#f92672">=</span>weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(Direction)) <span style="color:#f92672">%&gt;%</span> cor <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">findCorrelation</span>(cutoff<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>)
reducedDat<span style="color:#f92672">&lt;-</span>weeklyDat[<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(corrCols)]
reducedDat <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#       Year           Lag1               Lag2               Lag3
#  Min.   :1990   Min.   :-18.1950   Min.   :-18.1950   Min.   :-18.1950
#  1st Qu.:1995   1st Qu.: -1.1540   1st Qu.: -1.1540   1st Qu.: -1.1580
#  Median :2000   Median :  0.2410   Median :  0.2410   Median :  0.2410
#  Mean   :2000   Mean   :  0.1506   Mean   :  0.1511   Mean   :  0.1472
#  3rd Qu.:2005   3rd Qu.:  1.4050   3rd Qu.:  1.4090   3rd Qu.:  1.4090
#  Max.   :2010   Max.   : 12.0260   Max.   : 12.0260   Max.   : 12.0260
#       Lag4               Lag5              Today          Direction
#  Min.   :-18.1950   Min.   :-18.1950   Min.   :-18.1950   Down:484
#  1st Qu.: -1.1580   1st Qu.: -1.1660   1st Qu.: -1.1540   Up  :605
#  Median :  0.2380   Median :  0.2340   Median :  0.2410
#  Mean   :  0.1458   Mean   :  0.1399   Mean   :  0.1499
#  3rd Qu.:  1.4090   3rd Qu.:  1.4050   3rd Qu.:  1.4050
#  Max.   : 12.0260   Max.   : 12.0260   Max.   : 12.0260</code></pre></div>
<p>We can see that the <code>Volume</code> variable has been dropped, since it
evidently is strongly correlated with <code>Year</code>. This may or may not be a
useful insight, but it is good to keep in mind.</p>

<h4 id="visualization">Visualization</h4>

<p>We will be using the <code>ggplot2</code> library throughout for this segment.</p>

<p>Lets start with some scatter plots in a one v/s all scheme, similar to
the methodology
<a href="https://www.r-bloggers.com/plot-some-variables-against-many-others-with-tidyr-and-ggplot2/" target="_blank">described
here</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(Direction)) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">gather</span>(<span style="color:#f92672">-</span>Year,key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Variable&#34;</span>, value<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Value&#34;</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>Value,y<span style="color:#f92672">=</span>Year)) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_point</span>() <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">facet_wrap</span>(<span style="color:#f92672">~</span>Variable) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">coord_flip</span>()</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-11-1.png"   />

        
            <figcaption class="center" >Figure 1: One v/s all for Direction</figcaption>
        
    </figure>



<p>That didn&rsquo;t really tell us much which we didn&rsquo;t already get from the
<code>cor()</code> function, but we can go the whole hog and do this for every
variable since we don&rsquo;t have that many in the first place..</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(Direction)) <span style="color:#f92672">%&gt;%</span> pairs</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-12-1.png"   />

        
            <figcaption class="center" >Figure 2: Pairs</figcaption>
        
    </figure>



<p>This is not especially useful, and it is doubtful if more scatter-plots
will help at all, so lets move on to box plots.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">pivot_longer</span>(<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(Direction,Volume,Today,Year),names_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Lag&#34;</span>,values_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Value&#34;</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>Direction,y<span style="color:#f92672">=</span>Value,fill<span style="color:#f92672">=</span>Lag)) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_boxplot</span>()</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-13-1.png"   />

        
            <figcaption class="center" >Figure 3: Box plots for Direction</figcaption>
        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">pivot_longer</span>(<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(Direction,Volume,Today,Year),names_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Lag&#34;</span>,values_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Value&#34;</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>Today,y<span style="color:#f92672">=</span>Value,fill<span style="color:#f92672">=</span>Lag)) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_boxplot</span>()</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-14-1.png"   />

        
            <figcaption class="center" >Figure 4: More box plots</figcaption>
        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">pivot_longer</span>(<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(Direction,Volume,Today,Year),names_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Lag&#34;</span>,values_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Value&#34;</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>Lag,y<span style="color:#f92672">=</span>Value,fill<span style="color:#f92672">=</span>Direction)) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_boxplot</span>()</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-15-1.png"   />

        
            <figcaption class="center" >Figure 5: Lag v/s all</figcaption>
        
    </figure>



<p>This does summarize our text analysis quite well. Importantly, it tells
us that the <code>Today</code> value is largely unrelated to the \(4\) <code>Lag</code>
variables.</p>

<p>A really good-looking box-plot is easy to get with the <code>caret</code> library:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(Direction)) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">featurePlot</span>(
            y <span style="color:#f92672">=</span> weeklyDat<span style="color:#f92672">$</span>Direction,
            plot <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;box&#34;</span>,
            <span style="color:#75715e"># Pass in options to bwplot()</span>
            scales <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(y <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(relation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;free&#34;</span>),
                          x <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(rot <span style="color:#f92672">=</span> <span style="color:#ae81ff">90</span>)),
            auto.key <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(columns <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>))</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-16-1.png"   />

        
            <figcaption class="center" >Figure 6: Plots with `caret`</figcaption>
        
    </figure>



<p>We might want to
<a href="http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization" target="_blank">visualize</a>
our correlation matrix as well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(reshape2)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#
# Attaching package: &#39;reshape2&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># The following object is masked from &#39;package:tidyr&#39;:
#
#     smiths</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(Direction)) <span style="color:#f92672">%&gt;%</span> cor <span style="color:#f92672">%&gt;%</span> melt <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>Var1,y<span style="color:#f92672">=</span>Var2,fill<span style="color:#f92672">=</span>value)) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_tile</span>()</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-17-1.png"   />

        
            <figcaption class="center" >Figure 7: Heatmap of the correlation matrix</figcaption>
        
    </figure>



<h3 id="b-logistic-regression---predictor-significance">b) Logistic Regression - Predictor Significance</h3>

<p>Lets start with the native <code>glm</code> function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">glm.fit<span style="color:#f92672">=</span><span style="color:#a6e22e">glm</span>(Direction<span style="color:#f92672">~</span>Lag1<span style="color:#f92672">+</span>Lag2<span style="color:#f92672">+</span>Lag3<span style="color:#f92672">+</span>Lag4<span style="color:#f92672">+</span>Lag5<span style="color:#f92672">+</span>Volume, data<span style="color:#f92672">=</span>weeklyDat, family<span style="color:#f92672">=</span>binomial)
<span style="color:#a6e22e">summary</span>(glm.fit)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#
# Call:
# glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 +
#     Volume, family = binomial, data = weeklyDat)
#
# Deviance Residuals:
#     Min       1Q   Median       3Q      Max
# -1.6949  -1.2565   0.9913   1.0849   1.4579
#
# Coefficients:
#             Estimate Std. Error z value Pr(&gt;|z|)
# (Intercept)  0.26686    0.08593   3.106   0.0019 **
# Lag1        -0.04127    0.02641  -1.563   0.1181
# Lag2         0.05844    0.02686   2.175   0.0296 *
# Lag3        -0.01606    0.02666  -0.602   0.5469
# Lag4        -0.02779    0.02646  -1.050   0.2937
# Lag5        -0.01447    0.02638  -0.549   0.5833
# Volume      -0.02274    0.03690  -0.616   0.5377
# ---
# Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#
# (Dispersion parameter for binomial family taken to be 1)
#
#     Null deviance: 1496.2  on 1088  degrees of freedom
# Residual deviance: 1486.4  on 1082  degrees of freedom
# AIC: 1500.4
#
# Number of Fisher Scoring iterations: 4</code></pre></div>
<p>Evidently, only the <code>Lag2</code> value is of statistical significance.</p>

<p>It is always of importance to figure out what numeric values R will
assign to our factors, and it is best not to guess.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">contrasts</span>(weeklyDat<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#      Up
# Down  0
# Up    1</code></pre></div>
<h3 id="c-confusion-matrix-and-metrics">c) Confusion Matrix and Metrics</h3>

<p>Essentially:</p>

<ul>
<li>Predict the response</li>
<li>Create an output length vector</li>
<li>Apply thresholding to obtain labels</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">glm.probs <span style="color:#f92672">=</span> <span style="color:#a6e22e">predict</span>(glm.fit, type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;response&#34;</span>)
glm.pred <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#e6db74">&#34;Up&#34;</span>,<span style="color:#a6e22e">length</span>(glm.probs))
glm.pred[glm.probs<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0.5</span>]<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Down&#34;</span>
glm.pred<span style="color:#f92672">=</span><span style="color:#a6e22e">factor</span>(glm.pred)
<span style="color:#a6e22e">confusionMatrix</span>(glm.pred,weeklyDat<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction Down  Up
#       Down   54  48
#       Up    430 557
#
#                Accuracy : 0.5611
#                  95% CI : (0.531, 0.5908)
#     No Information Rate : 0.5556
#     P-Value [Acc &gt; NIR] : 0.369
#
#                   Kappa : 0.035
#
#  Mcnemar&#39;s Test P-Value : &lt;2e-16
#
#             Sensitivity : 0.11157
#             Specificity : 0.92066
#          Pos Pred Value : 0.52941
#          Neg Pred Value : 0.56434
#              Prevalence : 0.44444
#          Detection Rate : 0.04959
#    Detection Prevalence : 0.09366
#       Balanced Accuracy : 0.51612
#
#        &#39;Positive&#39; Class : Down
#</code></pre></div>
<ul>
<li>We have used the <code>confusionMatrix</code> function from <code>caret</code>
(<a href="https://rdrr.io/cran/caret/man/confusionMatrix.html" target="_blank">documented
here</a>) instead of displaying the results with <code>table</code> and then
calculating precision, recall and the rest by hand.</li>
</ul>

<h3 id="d-train-test-splits">d) Train Test Splits</h3>

<p>Although we could have used the indices and passed it to <code>glm</code> as the
<code>subset</code> attribute, it is cleaner to just make subsets instead.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">weeklyVal<span style="color:#f92672">&lt;-</span>weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">filter</span>(Year<span style="color:#f92672">&gt;=</span><span style="color:#ae81ff">2009</span>)
weeklyTrain<span style="color:#f92672">&lt;-</span>weeklyDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">filter</span>(Year<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">2009</span>)</code></pre></div>
<p>Now we can train a model on our training data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">glm.fit<span style="color:#f92672">=</span><span style="color:#a6e22e">glm</span>(Direction<span style="color:#f92672">~</span>Lag2,data<span style="color:#f92672">=</span>weeklyTrain,family<span style="color:#f92672">=</span>binomial)
<span style="color:#a6e22e">summary</span>(glm.fit)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#
# Call:
# glm(formula = Direction ~ Lag2, family = binomial, data = weeklyTrain)
#
# Deviance Residuals:
#    Min      1Q  Median      3Q     Max
# -1.536  -1.264   1.021   1.091   1.368
#
# Coefficients:
#             Estimate Std. Error z value Pr(&gt;|z|)
# (Intercept)  0.20326    0.06428   3.162  0.00157 **
# Lag2         0.05810    0.02870   2.024  0.04298 *
# ---
# Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#
# (Dispersion parameter for binomial family taken to be 1)
#
#     Null deviance: 1354.7  on 984  degrees of freedom
# Residual deviance: 1350.5  on 983  degrees of freedom
# AIC: 1354.5
#
# Number of Fisher Scoring iterations: 4</code></pre></div>
<p>Having fit our model, we will test the predictions on our held out data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">glm.probs <span style="color:#f92672">=</span> <span style="color:#a6e22e">predict</span>(glm.fit,weeklyVal, type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;response&#34;</span>)
glm.pred <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#e6db74">&#34;Up&#34;</span>,<span style="color:#a6e22e">length</span>(glm.probs))
glm.pred[glm.probs<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0.5</span>]<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Down&#34;</span>
glm.pred<span style="color:#f92672">=</span><span style="color:#a6e22e">factor</span>(glm.pred)
<span style="color:#a6e22e">confusionMatrix</span>(glm.pred,weeklyVal<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction Down Up
#       Down    9  5
#       Up     34 56
#
#                Accuracy : 0.625
#                  95% CI : (0.5247, 0.718)
#     No Information Rate : 0.5865
#     P-Value [Acc &gt; NIR] : 0.2439
#
#                   Kappa : 0.1414
#
#  Mcnemar&#39;s Test P-Value : 7.34e-06
#
#             Sensitivity : 0.20930
#             Specificity : 0.91803
#          Pos Pred Value : 0.64286
#          Neg Pred Value : 0.62222
#              Prevalence : 0.41346
#          Detection Rate : 0.08654
#    Detection Prevalence : 0.13462
#       Balanced Accuracy : 0.56367
#
#        &#39;Positive&#39; Class : Down
#</code></pre></div>
<p>We really aren&rsquo;t doing very well with this single variable model as is
evident.</p>

<h3 id="e-lda-models">e) LDA models</h3>

<p>At this stage we could use <code>MASS</code> to get the <code>lda</code> function, but it
would be better to just switch to using <code>caret</code>. Note that the <code>caret</code>
prediction is a label by default, so thresholding needs to be specified
differently if required.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lda.fit<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(Direction<span style="color:#f92672">~</span>Lag2,data<span style="color:#f92672">=</span>weeklyTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lda&#34;</span>)
<span style="color:#a6e22e">summary</span>(lda.fit)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#             Length Class      Mode
# prior       2      -none-     numeric
# counts      2      -none-     numeric
# means       2      -none-     numeric
# scaling     1      -none-     numeric
# lev         2      -none-     character
# svd         1      -none-     numeric
# N           1      -none-     numeric
# call        3      -none-     call
# xNames      1      -none-     character
# problemType 1      -none-     character
# tuneValue   1      data.frame list
# obsLevels   2      -none-     character
# param       0      -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">predict</span>(lda.fit,weeklyVal) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">confusionMatrix</span>(weeklyVal<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction Down Up
#       Down    9  5
#       Up     34 56
#
#                Accuracy : 0.625
#                  95% CI : (0.5247, 0.718)
#     No Information Rate : 0.5865
#     P-Value [Acc &gt; NIR] : 0.2439
#
#                   Kappa : 0.1414
#
#  Mcnemar&#39;s Test P-Value : 7.34e-06
#
#             Sensitivity : 0.20930
#             Specificity : 0.91803
#          Pos Pred Value : 0.64286
#          Neg Pred Value : 0.62222
#              Prevalence : 0.41346
#          Detection Rate : 0.08654
#    Detection Prevalence : 0.13462
#       Balanced Accuracy : 0.56367
#
#        &#39;Positive&#39; Class : Down
#</code></pre></div>
<h3 id="f-qda-models">f) QDA models</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">qda.fit<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(Direction<span style="color:#f92672">~</span>Lag2,data<span style="color:#f92672">=</span>weeklyTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;qda&#34;</span>)
<span style="color:#a6e22e">summary</span>(qda.fit)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#             Length Class      Mode
# prior       2      -none-     numeric
# counts      2      -none-     numeric
# means       2      -none-     numeric
# scaling     2      -none-     numeric
# ldet        2      -none-     numeric
# lev         2      -none-     character
# N           1      -none-     numeric
# call        3      -none-     call
# xNames      1      -none-     character
# problemType 1      -none-     character
# tuneValue   1      data.frame list
# obsLevels   2      -none-     character
# param       0      -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">predict</span>(qda.fit,weeklyVal) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">confusionMatrix</span>(weeklyVal<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction Down Up
#       Down    0  0
#       Up     43 61
#
#                Accuracy : 0.5865
#                  95% CI : (0.4858, 0.6823)
#     No Information Rate : 0.5865
#     P-Value [Acc &gt; NIR] : 0.5419
#
#                   Kappa : 0
#
#  Mcnemar&#39;s Test P-Value : 1.504e-10
#
#             Sensitivity : 0.0000
#             Specificity : 1.0000
#          Pos Pred Value :    NaN
#          Neg Pred Value : 0.5865
#              Prevalence : 0.4135
#          Detection Rate : 0.0000
#    Detection Prevalence : 0.0000
#       Balanced Accuracy : 0.5000
#
#        &#39;Positive&#39; Class : Down
#</code></pre></div>
<p>This is quite possibly the worst of the lot. As is evident, the model
just predicts <code>Up</code> no matter what.</p>

<h3 id="g-knn">g) KNN</h3>

<p><code>caret</code> tends to over-zealously retrain models and find the best
possible parameters. In this case that is annoying and redundant so we
will use the <code>class</code> library. We <strong>should really scale our data</strong> before
using KNN though.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(class)
<span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">1</span>)
knn.pred<span style="color:#f92672">=</span><span style="color:#a6e22e">knn</span>(<span style="color:#a6e22e">as.matrix</span>(weeklyTrain<span style="color:#f92672">$</span>Lag2),<span style="color:#a6e22e">as.matrix</span>(weeklyVal<span style="color:#f92672">$</span>Lag2),weeklyTrain<span style="color:#f92672">$</span>Direction,k<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
<span style="color:#a6e22e">confusionMatrix</span>(knn.pred,weeklyVal<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction Down Up
#       Down   21 30
#       Up     22 31
#
#                Accuracy : 0.5
#                  95% CI : (0.4003, 0.5997)
#     No Information Rate : 0.5865
#     P-Value [Acc &gt; NIR] : 0.9700
#
#                   Kappa : -0.0033
#
#  Mcnemar&#39;s Test P-Value : 0.3317
#
#             Sensitivity : 0.4884
#             Specificity : 0.5082
#          Pos Pred Value : 0.4118
#          Neg Pred Value : 0.5849
#              Prevalence : 0.4135
#          Detection Rate : 0.2019
#    Detection Prevalence : 0.4904
#       Balanced Accuracy : 0.4983
#
#        &#39;Positive&#39; Class : Down
#</code></pre></div>
<p>Clearly this model is not doing very well.</p>

<h3 id="h-model-selection">h) Model Selection</h3>

<p>We will first get the ROC curves.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(pROC)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Type &#39;citation(&#34;pROC&#34;)&#39; for a citation.</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#
# Attaching package: &#39;pROC&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># The following objects are masked from &#39;package:stats&#39;:
#
#     cov, smooth, var</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">knnROC<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">roc</span>(predictor<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(knn.pred),response<span style="color:#f92672">=</span>weeklyVal<span style="color:#f92672">$</span>Direction,levels<span style="color:#f92672">=</span><span style="color:#a6e22e">rev</span>(<span style="color:#a6e22e">levels</span>(weeklyVal<span style="color:#f92672">$</span>Direction)))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting direction: controls &lt; cases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">logiROC<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">roc</span>(predictor<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(<span style="color:#a6e22e">predict</span>(glm.fit,weeklyVal)),response<span style="color:#f92672">=</span>weeklyVal<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting levels: control = Down, case = Up</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting direction: controls &gt; cases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ldaROC<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">roc</span>(predictor<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(<span style="color:#a6e22e">predict</span>(lda.fit,weeklyVal)),response<span style="color:#f92672">=</span>weeklyVal<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting levels: control = Down, case = Up</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting direction: controls &lt; cases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">qdaROC<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">roc</span>(predictor<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(<span style="color:#a6e22e">predict</span>(qda.fit,weeklyVal)),response<span style="color:#f92672">=</span>weeklyVal<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting levels: control = Down, case = Up
# Setting direction: controls &lt; cases</code></pre></div>
<p>Now to plot them.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggroc</span>(<span style="color:#a6e22e">list</span>(KNN<span style="color:#f92672">=</span>knnROC,Logistic<span style="color:#f92672">=</span>logiROC,LDA<span style="color:#f92672">=</span>ldaROC,QDA<span style="color:#f92672">=</span>qdaROC))</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-28-1.png"   />

        
            <figcaption class="center" >Figure 8: ROC curves for Weekly data</figcaption>
        
    </figure>



<p>To compare models with <code>caret</code> it is easy to refit the logistic and knn
models in the caret formulation.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">knnCaret<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(Direction<span style="color:#f92672">~</span>Lag2,data<span style="color:#f92672">=</span>weeklyTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;knn&#34;</span>)</code></pre></div>
<p>However, the KNN model is the best parameter model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">resmod <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">resamples</span>(<span style="color:#a6e22e">list</span>(lda<span style="color:#f92672">=</span>lda.fit, qda<span style="color:#f92672">=</span>qda.fit, KNN<span style="color:#f92672">=</span>knnCaret))
<span style="color:#a6e22e">summary</span>(resmod)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#
# Call:
# summary.resamples(object = resmod)
#
# Models: lda, qda, KNN
# Number of resamples: 25
#
# Accuracy
#          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
# lda 0.5043228 0.5344353 0.5529101 0.5500861 0.5683060 0.5846995    0
# qda 0.5044248 0.5204360 0.5307263 0.5326785 0.5462428 0.5777778    0
# KNN 0.4472222 0.5082873 0.5240642 0.5168327 0.5302198 0.5485714    0
#
# Kappa
#            Min.      1st Qu.      Median         Mean    3rd Qu.       Max.
# lda -0.02618939 -0.003638168 0.005796908  0.007801904 0.01635328 0.05431238
# qda -0.06383592 -0.005606123 0.000000000 -0.003229697 0.00000000 0.03606344
# KNN -0.11297539  0.004168597 0.024774647  0.016171229 0.04456142 0.07724439
#     NA&#39;s
# lda    0
# qda    0
# KNN    0</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">bwplot</span>(resmod)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-30-1.png"   />

        
            <figcaption class="center" >Figure 9: Caret plots for comparison</figcaption>
        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">dotplot</span>(resmod)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-30-2.png"   />

        
    </figure>



<p>Kappa or Cohen&rsquo;s Kappa is essentially classification accuracy,
normalized at the baseline of random chance. It is a more useful measure
to use on problems that have imbalanced classes. There&rsquo;s more on model
selection
<a href="https://machinelearningmastery.com/machine-learning-evaluation-metrics-in-r/" target="_blank">here</a>.</p>

<h3 id="i-further-tuning">i) Further Tuning</h3>

<p>Do note the <code>caret</code>
<a href="https://topepo.github.io/caret/model-training-and-tuning.html" target="_blank">defaults</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">fitControl <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">trainControl</span>(<span style="color:#75715e"># 10-fold CV</span>
                           method <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;repeatedcv&#34;</span>,
                           number <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>,
                           <span style="color:#75715e"># repeated ten times</span>
                           repeats <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>)</code></pre></div>
<h3 id="logistic">Logistic</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">glm2.fit<span style="color:#f92672">=</span><span style="color:#a6e22e">glm</span>(Direction<span style="color:#f92672">~</span>Lag1<span style="color:#f92672">+</span>Lag2<span style="color:#f92672">+</span>Lag3<span style="color:#f92672">+</span>Lag4<span style="color:#f92672">+</span>Lag5<span style="color:#f92672">+</span>Volume, data<span style="color:#f92672">=</span>weeklyDat, family<span style="color:#f92672">=</span>binomial)

glm2.probs <span style="color:#f92672">=</span> <span style="color:#a6e22e">predict</span>(glm2.fit,weeklyVal, type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;response&#34;</span>)
glm2.pred <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#e6db74">&#34;Up&#34;</span>,<span style="color:#a6e22e">length</span>(glm2.probs))
glm2.pred[glm2.probs<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0.5</span>]<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Down&#34;</span>
glm2.pred<span style="color:#f92672">=</span><span style="color:#a6e22e">factor</span>(glm2.pred)
<span style="color:#a6e22e">confusionMatrix</span>(glm2.pred,weeklyVal<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction Down Up
#       Down   17 13
#       Up     26 48
#
#                Accuracy : 0.625
#                  95% CI : (0.5247, 0.718)
#     No Information Rate : 0.5865
#     P-Value [Acc &gt; NIR] : 0.24395
#
#                   Kappa : 0.1907
#
#  Mcnemar&#39;s Test P-Value : 0.05466
#
#             Sensitivity : 0.3953
#             Specificity : 0.7869
#          Pos Pred Value : 0.5667
#          Neg Pred Value : 0.6486
#              Prevalence : 0.4135
#          Detection Rate : 0.1635
#    Detection Prevalence : 0.2885
#       Balanced Accuracy : 0.5911
#
#        &#39;Positive&#39; Class : Down
#</code></pre></div>
<h4 id="qda">QDA</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">qdaCaret<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(Direction<span style="color:#f92672">~</span>Lag2<span style="color:#f92672">+</span>Lag4,data<span style="color:#f92672">=</span>weeklyTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;qda&#34;</span>,trainControl<span style="color:#f92672">=</span>fitControl)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(qdaCaret)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#             Length Class      Mode
# prior       2      -none-     numeric
# counts      2      -none-     numeric
# means       4      -none-     numeric
# scaling     8      -none-     numeric
# ldet        2      -none-     numeric
# lev         2      -none-     character
# N           1      -none-     numeric
# call        4      -none-     call
# xNames      2      -none-     character
# problemType 1      -none-     character
# tuneValue   1      data.frame list
# obsLevels   2      -none-     character
# param       1      -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">predict</span>(qdaCaret,weeklyVal) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">confusionMatrix</span>(weeklyVal<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction Down Up
#       Down    9 14
#       Up     34 47
#
#                Accuracy : 0.5385
#                  95% CI : (0.438, 0.6367)
#     No Information Rate : 0.5865
#     P-Value [Acc &gt; NIR] : 0.863079
#
#                   Kappa : -0.0217
#
#  Mcnemar&#39;s Test P-Value : 0.006099
#
#             Sensitivity : 0.20930
#             Specificity : 0.77049
#          Pos Pred Value : 0.39130
#          Neg Pred Value : 0.58025
#              Prevalence : 0.41346
#          Detection Rate : 0.08654
#    Detection Prevalence : 0.22115
#       Balanced Accuracy : 0.48990
#
#        &#39;Positive&#39; Class : Down
#</code></pre></div>
<h4 id="lda">LDA</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ldaCaret<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(Direction<span style="color:#f92672">~</span>Lag2<span style="color:#f92672">+</span>Lag1<span style="color:#f92672">+</span>Year,data<span style="color:#f92672">=</span>weeklyTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lda&#34;</span>,trainControl<span style="color:#f92672">=</span>fitControl)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(ldaCaret)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#             Length Class      Mode
# prior       2      -none-     numeric
# counts      2      -none-     numeric
# means       6      -none-     numeric
# scaling     3      -none-     numeric
# lev         2      -none-     character
# svd         1      -none-     numeric
# N           1      -none-     numeric
# call        4      -none-     call
# xNames      3      -none-     character
# problemType 1      -none-     character
# tuneValue   1      data.frame list
# obsLevels   2      -none-     character
# param       1      -none-     list</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">predict</span>(ldaCaret,weeklyVal) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">confusionMatrix</span>(weeklyVal<span style="color:#f92672">$</span>Direction)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction Down Up
#       Down   20 19
#       Up     23 42
#
#                Accuracy : 0.5962
#                  95% CI : (0.4954, 0.6913)
#     No Information Rate : 0.5865
#     P-Value [Acc &gt; NIR] : 0.4626
#
#                   Kappa : 0.1558
#
#  Mcnemar&#39;s Test P-Value : 0.6434
#
#             Sensitivity : 0.4651
#             Specificity : 0.6885
#          Pos Pred Value : 0.5128
#          Neg Pred Value : 0.6462
#              Prevalence : 0.4135
#          Detection Rate : 0.1923
#    Detection Prevalence : 0.3750
#       Balanced Accuracy : 0.5768
#
#        &#39;Positive&#39; Class : Down
#</code></pre></div>
<h4 id="knn">KNN</h4>

<p>Honestly, again, this should be scaled. Plot <code>KNN</code> with the best
parameters.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(knnCaret)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-37-1.png"   />

        
            <figcaption class="center" >Figure 10: KNN statistics</figcaption>
        
    </figure>



<p>Evidently, the accuracy increases with an increase in the number of
neighbors considered.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(knnCaret, print.thres <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;S&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-38-1.png"   />

        
            <figcaption class="center" >Figure 11: Visualizing thresholds for KNN</figcaption>
        
    </figure>



<p>However this shows that we don&rsquo;t actually get much of an increase in
accuracy anyway.</p>

<h2 id="question-4.11---pages-171-172">Question 4.11 - Pages 171-172</h2>

<p>In this problem, you will develop a model to predict whether a given car
gets high or low gas mileage based on the Auto data set.</p>

<p><strong>(a)</strong> Create a binary variable, <code>mpg01</code> , that contains a 1 if <code>mpg</code>
contains a value above its median, and a 0 if mpg contains a value below
its median. You can compute the median using the <code>median()</code> function.
Note you may find it helpful to use the <code>data.frame()</code> function to
create a single data set containing both <code>mpg01</code> and the other <code>Auto</code>
variables.</p>

<p><strong>(b)</strong> Explore the data graphically in order to investigate the
association between <code>mpg01</code> and the other features. Which of the other
features seem most likely to be useful in predicting <code>mpg01</code> ?
Scatter-plots and boxplots may be useful tools to answer this question.
Describe your findings.</p>

<p><strong>&copy;</strong> Split the data into a training set and a test set.</p>

<p><strong>(d)</strong> Perform LDA on the training data in order to predict <code>mpg01</code> using
the variables that seemed most associated with <code>mpg01</code> in <strong>(b)</strong>. What is
the test error of the model obtained?</p>

<p><strong>(e)</strong> Perform QDA on the training data in order to predict <code>mpg01</code> using
the variables that seemed most associated with <code>mpg01</code> in <strong>(b)</strong>. What is
the test error of the model obtained?</p>

<p><strong>(f)</strong> Perform logistic regression on the training data in order to
predict <code>mpg01</code> using the variables that seemed most associated with
<code>mpg01</code> in <strong>(b)</strong>. What is the test error of the model obtained?</p>

<p><strong>(g)</strong> Perform KNN on the training data, with several values of \(K\), in
order to predict <code>mpg01</code> . Use only the variables that seemed most
associated with <code>mpg01</code> in <strong>(b)</strong>. What test errors do you obtain? Which
value of \(K\) seems to perform the best on this data set?</p>

<h3 id="answer-1">Answer</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">autoDat<span style="color:#f92672">&lt;-</span>ISLR<span style="color:#f92672">::</span>Auto</code></pre></div>
<h3 id="a-binary-variable">a) Binary Variable</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">autoDat<span style="color:#f92672">$</span>mpg <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sort</span>() <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">median</span>()</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># [1] 22.75</code></pre></div>
<p>Now we can get a new variable from that.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">newDat<span style="color:#f92672">=</span>autoDat
newDat<span style="color:#f92672">$</span>mpg01 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">ifelse</span>(autoDat<span style="color:#f92672">$</span>mpg<span style="color:#f92672">&lt;</span>autoDat<span style="color:#f92672">$</span>mpg <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sort</span>() <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">median</span>(),<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">factor</span>()</code></pre></div>
<p>Note that the <code>ifelse</code> command takes a truthy function, value when
false, value when true, but does not return a factor automatically so we
piped it to factor to ensure it is factorial.</p>

<h3 id="b-visual-exploration">b) Visual Exploration</h3>

<p>Some box-plots:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">newDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">pivot_longer</span>(<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(mpg01,name),names_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Params&#34;</span>,values_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Value&#34;</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>mpg01,y<span style="color:#f92672">=</span>Value)) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_boxplot</span>() <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">facet_wrap</span>(<span style="color:#f92672">~</span> Params, scales <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;free_y&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-42-1.png"   />

        
            <figcaption class="center" >Figure 12: Box plots</figcaption>
        
    </figure>



<p>With some scatter plots as well:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">newDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">pivot_longer</span>(<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(mpg01,name,weight),names_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Params&#34;</span>,values_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Value&#34;</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>weight,y<span style="color:#f92672">=</span>Value,color<span style="color:#f92672">=</span>mpg01)) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_point</span>() <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">facet_wrap</span>(<span style="color:#f92672">~</span> Params, scales <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;free_y&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-43-1.png"   />

        
            <figcaption class="center" >Figure 13: Scatter plots</figcaption>
        
    </figure>



<p>Clearly, <code>origin</code>, <code>year</code> and <code>cylinder</code> are essentially not very
relevant numerically for the regression lines and confidence intervals.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">newDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">select</span>(<span style="color:#f92672">-</span>year,<span style="color:#f92672">-</span>origin,<span style="color:#f92672">-</span>cylinders) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">pivot_longer</span>(<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(mpg01,name,mpg),names_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Params&#34;</span>,values_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Value&#34;</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>mpg,y<span style="color:#f92672">=</span>Value,color<span style="color:#f92672">=</span>mpg01)) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_point</span>() <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_smooth</span>(method<span style="color:#f92672">=</span>lm) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">facet_wrap</span>(<span style="color:#f92672">~</span> Params, scales <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;free_y&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-44-1.png"   />

        
    </figure>



<h3 id="c-train-test-split">c) Train-Test Split</h3>

<p>We can split our data
<a href="https://topepo.github.io/caret/data-splitting.html#simple-splitting-based-on-the-outcome" target="_blank">very
easily</a> with <code>caret</code>. It is important to remember that for factors,
random sampling occurs within each class to preserve the overall class
distribution of the data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">1984</span>)
trainInd <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">createDataPartition</span>(newDat<span style="color:#f92672">$</span>mpg01, <span style="color:#75715e"># Factor, so class sampling</span>
                                p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>, <span style="color:#75715e"># 70-30 train-test</span>
                                list<span style="color:#f92672">=</span><span style="color:#66d9ef">FALSE</span>, <span style="color:#75715e"># No lists</span>
                                times<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># No bootstrap</span>
autoTrain<span style="color:#f92672">&lt;-</span>newDat[trainInd,]
autoTest<span style="color:#f92672">&lt;-</span>newDat[<span style="color:#f92672">-</span>trainInd,]</code></pre></div>
<h3 id="d-lda-with-significant-variables">d) LDA with Significant Variables</h3>

<p>Whenever I see significant I think correlation, so let&rsquo;s take a look at
that.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">newDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">select</span>(<span style="color:#f92672">-</span>mpg01,<span style="color:#f92672">-</span>name) <span style="color:#f92672">%&gt;%</span> cor</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#                     mpg  cylinders displacement horsepower     weight
# mpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442
# cylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273
# displacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944
# horsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377
# weight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000
# acceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392
# year          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199
# origin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054
#              acceleration       year     origin
# mpg             0.4233285  0.5805410  0.5652088
# cylinders      -0.5046834 -0.3456474 -0.5689316
# displacement   -0.5438005 -0.3698552 -0.6145351
# horsepower     -0.6891955 -0.4163615 -0.4551715
# weight         -0.4168392 -0.3091199 -0.5850054
# acceleration    1.0000000  0.2903161  0.2127458
# year            0.2903161  1.0000000  0.1815277
# origin          0.2127458  0.1815277  1.0000000</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">newDat <span style="color:#f92672">%&gt;%</span> length</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># [1] 10</code></pre></div>
<p>Now lets quickly see what it looks like with correlated values removed.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">corrCols2<span style="color:#f92672">=</span>newDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">select</span>(<span style="color:#f92672">-</span>mpg01,<span style="color:#f92672">-</span>name) <span style="color:#f92672">%&gt;%</span> cor <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">findCorrelation</span>(cutoff<span style="color:#f92672">=</span><span style="color:#ae81ff">0.85</span>)
newRed<span style="color:#f92672">&lt;-</span>newDat[<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(corrCols2)]
newRed <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#       mpg            weight      acceleration        year           origin
#  Min.   : 9.00   Min.   :1613   Min.   : 8.00   Min.   :70.00   Min.   :1.000
#  1st Qu.:17.00   1st Qu.:2225   1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000
#  Median :22.75   Median :2804   Median :15.50   Median :76.00   Median :1.000
#  Mean   :23.45   Mean   :2978   Mean   :15.54   Mean   :75.98   Mean   :1.577
#  3rd Qu.:29.00   3rd Qu.:3615   3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000
#  Max.   :46.60   Max.   :5140   Max.   :24.80   Max.   :82.00   Max.   :3.000
#
#                  name     mpg01
#  amc matador       :  5   0:196
#  ford pinto        :  5   1:196
#  toyota corolla    :  5
#  amc gremlin       :  4
#  amc hornet        :  4
#  chevrolet chevette:  4
#  (Other)           :365</code></pre></div>
<p>Inherent in this discussion is the fact that I consider what is
correlated to <code>mpg</code> to be a good indicator of what will help <code>mpg01</code> for
obvious reasons.</p>

<p>Now we can just use the columns we found with <code>findCorrelation</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">corrCols2 <span style="color:#f92672">%&gt;%</span> print</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># [1] 3 4 2</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">names</span>(newDat)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#  [1] &#34;mpg&#34;          &#34;cylinders&#34;    &#34;displacement&#34; &#34;horsepower&#34;   &#34;weight&#34;
#  [6] &#34;acceleration&#34; &#34;year&#34;         &#34;origin&#34;       &#34;name&#34;         &#34;mpg01&#34;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">autoLDA<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(mpg01<span style="color:#f92672">~</span>cylinders<span style="color:#f92672">+</span>displacement<span style="color:#f92672">+</span>horsepower,data<span style="color:#f92672">=</span>autoTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lda&#34;</span>)
valScoreLDA<span style="color:#f92672">=</span><span style="color:#a6e22e">predict</span>(autoLDA,autoTest)</code></pre></div>
<p>Now we can check the statistics.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">confusionMatrix</span>(valScoreLDA,autoTest<span style="color:#f92672">$</span>mpg01)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction  0  1
#          0 56  2
#          1  2 56
#
#                Accuracy : 0.9655
#                  95% CI : (0.9141, 0.9905)
#     No Information Rate : 0.5
#     P-Value [Acc &gt; NIR] : &lt;2e-16
#
#                   Kappa : 0.931
#
#  Mcnemar&#39;s Test P-Value : 1
#
#             Sensitivity : 0.9655
#             Specificity : 0.9655
#          Pos Pred Value : 0.9655
#          Neg Pred Value : 0.9655
#              Prevalence : 0.5000
#          Detection Rate : 0.4828
#    Detection Prevalence : 0.5000
#       Balanced Accuracy : 0.9655
#
#        &#39;Positive&#39; Class : 0
#</code></pre></div>
<p>That is an amazingly accurate model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">auto_ldaROC<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">roc</span>(predictor<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(valScoreLDA),response<span style="color:#f92672">=</span>autoTest<span style="color:#f92672">$</span>mpg01,levels<span style="color:#f92672">=</span><span style="color:#a6e22e">levels</span>(autoTest<span style="color:#f92672">$</span>mpg01))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting direction: controls &lt; cases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggroc</span>(auto_ldaROC)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-51-1.png"   />

        
    </figure>



<h3 id="e-qda-with-significant-variables">e) QDA with Significant Variables</h3>

<p>Same deal as before.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">autoQDA<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(mpg01<span style="color:#f92672">~</span>cylinders<span style="color:#f92672">+</span>displacement<span style="color:#f92672">+</span>horsepower,data<span style="color:#f92672">=</span>autoTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;qda&#34;</span>)
valScoreQDA<span style="color:#f92672">=</span><span style="color:#a6e22e">predict</span>(autoQDA,autoTest)</code></pre></div>
<p>Now we can check the statistics.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">confusionMatrix</span>(valScoreQDA,autoTest<span style="color:#f92672">$</span>mpg01)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction  0  1
#          0 56  2
#          1  2 56
#
#                Accuracy : 0.9655
#                  95% CI : (0.9141, 0.9905)
#     No Information Rate : 0.5
#     P-Value [Acc &gt; NIR] : &lt;2e-16
#
#                   Kappa : 0.931
#
#  Mcnemar&#39;s Test P-Value : 1
#
#             Sensitivity : 0.9655
#             Specificity : 0.9655
#          Pos Pred Value : 0.9655
#          Neg Pred Value : 0.9655
#              Prevalence : 0.5000
#          Detection Rate : 0.4828
#    Detection Prevalence : 0.5000
#       Balanced Accuracy : 0.9655
#
#        &#39;Positive&#39; Class : 0
#</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">auto_qdaROC<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">roc</span>(predictor<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(valScoreQDA),response<span style="color:#f92672">=</span>autoTest<span style="color:#f92672">$</span>mpg01,levels<span style="color:#f92672">=</span><span style="color:#a6e22e">levels</span>(autoTest<span style="color:#f92672">$</span>mpg01))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting direction: controls &lt; cases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggroc</span>(auto_qdaROC)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-54-1.png"   />

        
    </figure>



<p>OK, this is weird enough to check if it isn&rsquo;t some sort of artifact.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">autoQDA2<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(mpg01<span style="color:#f92672">~</span>horsepower, data<span style="color:#f92672">=</span>autoTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;qda&#39;</span>)
valScoreQDA2<span style="color:#f92672">=</span><span style="color:#a6e22e">predict</span>(autoQDA2, autoTest)
<span style="color:#a6e22e">confusionMatrix</span>(valScoreQDA2,autoTest<span style="color:#f92672">$</span>mpg01)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction  0  1
#          0 42  3
#          1 16 55
#
#                Accuracy : 0.8362
#                  95% CI : (0.7561, 0.8984)
#     No Information Rate : 0.5
#     P-Value [Acc &gt; NIR] : 4.315e-14
#
#                   Kappa : 0.6724
#
#  Mcnemar&#39;s Test P-Value : 0.005905
#
#             Sensitivity : 0.7241
#             Specificity : 0.9483
#          Pos Pred Value : 0.9333
#          Neg Pred Value : 0.7746
#              Prevalence : 0.5000
#          Detection Rate : 0.3621
#    Detection Prevalence : 0.3879
#       Balanced Accuracy : 0.8362
#
#        &#39;Positive&#39; Class : 0
#</code></pre></div>
<p>OK, so the model isn&rsquo;t completely creepily correct all the time. In this
case we should probably think about what is going on. I would think it
is because of the nature of the <code>train-test</code> split we performed. We have
ensured during the sampling of our data that the train and test sets
contain the SAME distribution (assumed). So that&rsquo;s why our training
result and test results are both incredibly good. They&rsquo;re essentially
the same thing.</p>

<p>In fact, this is the perfect time to consider a validation set, just to
see what the models are really doing. Won&rsquo;t get into it right now
though.</p>

<h3 id="f-logistic-with-significant-variables">f) Logistic with Significant Variables</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">glmAuto.fit<span style="color:#f92672">=</span><span style="color:#a6e22e">glm</span>(mpg01<span style="color:#f92672">~</span>cylinders<span style="color:#f92672">+</span>displacement<span style="color:#f92672">+</span>horsepower, data<span style="color:#f92672">=</span>autoTrain, family<span style="color:#f92672">=</span>binomial)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">glmAuto.probs <span style="color:#f92672">=</span> <span style="color:#a6e22e">predict</span>(glmAuto.fit,autoTest, type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;response&#34;</span>)
glmAuto.pred <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#ae81ff">1</span>,<span style="color:#a6e22e">length</span>(glmAuto.probs))
glmAuto.pred[glmAuto.probs<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0.5</span>]<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
glmAuto.pred<span style="color:#f92672">=</span><span style="color:#a6e22e">factor</span>(glmAuto.pred)
<span style="color:#a6e22e">confusionMatrix</span>(glmAuto.pred,autoTest<span style="color:#f92672">$</span>mpg01)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction  0  1
#          0 56  4
#          1  2 54
#
#                Accuracy : 0.9483
#                  95% CI : (0.8908, 0.9808)
#     No Information Rate : 0.5
#     P-Value [Acc &gt; NIR] : &lt;2e-16
#
#                   Kappa : 0.8966
#
#  Mcnemar&#39;s Test P-Value : 0.6831
#
#             Sensitivity : 0.9655
#             Specificity : 0.9310
#          Pos Pred Value : 0.9333
#          Neg Pred Value : 0.9643
#              Prevalence : 0.5000
#          Detection Rate : 0.4828
#    Detection Prevalence : 0.5172
#       Balanced Accuracy : 0.9483
#
#        &#39;Positive&#39; Class : 0
#</code></pre></div>
<h3 id="g-knn-modeling">g) KNN Modeling</h3>

<p>Scale the parameters later.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">knnAuto<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(mpg01<span style="color:#f92672">~</span>cylinders<span style="color:#f92672">+</span>displacement<span style="color:#f92672">+</span>horsepower,data<span style="color:#f92672">=</span>autoTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;knn&#34;</span>)</code></pre></div>
<p>Plot <code>KNN</code> with the best parameters.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(knnCaret)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-59-1.png"   />

        
    </figure>



<p>Evidently, the accuracy increases with an increase in the number of
neighbors considered.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(knnAuto, print.thres <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;S&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-60-1.png"   />

        
    </figure>



<p>So we can see that \(5\) neighbors is a good compromise.</p>

<h2 id="question-4.12---pages-172-173">Question 4.12 - Pages 172-173</h2>

<p>This problem involves writing functions.</p>

<p><strong>(a)</strong> Write a function, <code>Power()</code> , that prints out the result of
raising 2 to the 3rd power. In other words, your function should compute
2^3 and print out the results.</p>

<p><em>Hint: Recall that <code>x^a</code> raises x to the power a. Use the <code>print()</code>
function to output the result.</em></p>

<p><strong>(b)</strong> Create a new function, <code>Power2()</code> , that allows you to pass any
two numbers, <code>x</code> and <code>a</code> , and prints out the value of <code>x^a</code> . You can
do this by beginning your function with the line</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">Power2<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(x,a){}</code></pre></div>
<p>You should be able to call your function by entering, for instance,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">Power2</span>(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">8</span>)</code></pre></div>
<p>on the command line. This should output the value of \(3^8\), namely,
\(6,651\).</p>

<p><strong>&copy;</strong> Using the <code>Power2()</code> function that you just wrote, compute \(10^3\),
\(8^{17}\), and \(131^3\).</p>

<p><strong>(d)</strong> Now create a new function, <code>Power3()</code>, that actually <em>returns</em> the
result <code>x^a</code> as an <code>R</code> object, rather than simply printing it to the
screen. That is, if you store the value <code>x^a</code> in an object called
<code>result</code> within your function, then you can simply <code>return()</code> this
result, using the following line:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">return</span>(result)</code></pre></div>
<p>The line above should be the last line in your function, before the <code>}</code>
symbol.</p>

<p><strong>(e)</strong> Now using the <code>Power3()</code> function, create a plot of \(f(x)=x^2\).
The <em>x</em>-axis should display a range of integers from \(1\) to \(10\), and
the <em>y</em>-axis should display \(x^2\) . Label the axes appropriately, and
use an appropriate title for the figure. Consider displaying either the
<em>x</em>-axis, the <em>y</em>-axis, or both on the log-scale. You can do this by
using <code>log=‘‘x’’</code>, <code>log=‘‘y’’</code>, or <code>log=‘‘xy’’</code> as arguments to the
<code>plot()</code> function.</p>

<p><strong>(f)</strong> Create a function, <code>PlotPower()</code> , that allows you to create a
plot of <code>x</code> against <code>x^a</code> for a fixed <code>a</code> and for a range of values of
<code>x</code>. For instance, if you call</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">PlotPower </span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span> ,<span style="color:#ae81ff">3</span>)</code></pre></div>
<p>then a plot should be created with an <em>x</em>-axis taking on values
\(1,2,&hellip;,10\) and a <em>y</em>-axis taking on values \(1^3,2^3,&hellip;,10^3\)</p>

<h3 id="answer-2">Answer</h3>

<h3 id="a-create-a-squaring-function">a) Create a Squaring Function</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">Power<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(x){<span style="color:#a6e22e">print</span>(<span style="color:#ae81ff">2</span>^x)}
<span style="color:#a6e22e">Power</span>(<span style="color:#ae81ff">3</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># [1] 8</code></pre></div>
<h3 id="b-generalizing-power-to-arbitrary-numbers">b) Generalizing Power to arbitrary numbers</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">Power2<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(x,a){<span style="color:#a6e22e">print</span>(x^a)}</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">Power2</span>(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">8</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># [1] 6561</code></pre></div>
<h3 id="c-random-testing-of-power2">c) Random Testing of Power2</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">Power2</span>(<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">3</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># [1] 1000</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">Power2</span>(<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">17</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># [1] 2.2518e+15</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">Power2</span>(<span style="color:#ae81ff">131</span>,<span style="color:#ae81ff">2</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># [1] 17161</code></pre></div>
<h3 id="d-return-a-value">d) Return a value</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">Power3<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(x,a){<span style="color:#a6e22e">return</span>(x^a)}</code></pre></div>
<h3 id="e-plot-something-with-power3">e) Plot something with Power3</h3>

<p>Actually now would be a good place to introduce <code>LaTeX</code> labeling.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e">#install.packages(&#34;latex2exp&#34;)</span>
<span style="color:#a6e22e">library</span>(latex2exp)</code></pre></div>
<p>No log scale.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">qplot</span>(x<span style="color:#f92672">=</span><span style="color:#a6e22e">seq</span>(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">10</span>),y<span style="color:#f92672">=</span><span style="color:#a6e22e">Power3</span>(<span style="color:#a6e22e">seq</span>(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">10</span>),<span style="color:#ae81ff">2</span>)) <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;Function without a log scale&#34;</span>) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_point</span>() <span style="color:#f92672">+</span> <span style="color:#a6e22e">xlab</span>(<span style="color:#e6db74">&#34;X&#34;</span>) <span style="color:#f92672">+</span> <span style="color:#a6e22e">ylab</span>(<span style="color:#a6e22e">TeX</span>(<span style="color:#e6db74">&#34;$X^2$&#34;</span>))</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-67-1.png"   />

        
    </figure>



<p>With a log scale.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">qplot</span>(x<span style="color:#f92672">=</span><span style="color:#a6e22e">seq</span>(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">10</span>),y<span style="color:#f92672">=</span><span style="color:#a6e22e">Power3</span>(<span style="color:#a6e22e">seq</span>(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">10</span>),<span style="color:#ae81ff">2</span>)) <span style="color:#f92672">+</span> <span style="color:#a6e22e">ggtitle</span>(<span style="color:#e6db74">&#34;Function with a log scale&#34;</span>) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_point</span>() <span style="color:#f92672">+</span> <span style="color:#a6e22e">xlab</span>(<span style="color:#e6db74">&#34;X&#34;</span>) <span style="color:#f92672">+</span> <span style="color:#a6e22e">ylab</span>(<span style="color:#a6e22e">TeX</span>(<span style="color:#e6db74">&#34;$X^2$&#34;</span>)) <span style="color:#f92672">+</span> <span style="color:#a6e22e">scale_y_log10</span>()</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-68-1.png"   />

        
    </figure>



<h3 id="f-plotpower-function">f) PlotPower Function</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">PlotPower<span style="color:#f92672">=</span><span style="color:#a6e22e">function</span>(xrange,pow){<span style="color:#a6e22e">return</span>(<span style="color:#a6e22e">qplot</span>(x<span style="color:#f92672">=</span>xrange,y<span style="color:#f92672">=</span><span style="color:#a6e22e">Power3</span>(xrange,pow)))}</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">plotter<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">PlotPower</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">3</span>)
plotter</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-70-1.png"   />

        
    </figure>



<p>The <a href="http://www.cookbook-r.com/Graphs/Titles%5F(ggplot2)/" target="_blank">R Cookbook</a>
is quite neat for some simple tasks like this.</p>

<h2 id="question-4.13---pages-173">Question 4.13 - Pages 173</h2>

<p>Using the <code>Boston</code> data set, fit classification models in order to
predict whether a given suburb has a crime rate above or below the
median. Explore logistic regression, LDA, and KNN models using various
subsets of the predictors. Describe your findings.</p>

<h3 id="answer-3">Answer</h3>

<p>OK, to speed this up, I will simply run through all the work done on the
<code>Auto</code> set. Recall that details about this data-set are
<a href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html" target="_blank">also
here</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">&lt;-</span>MASS<span style="color:#f92672">::</span>Boston</code></pre></div>
<ul>
<li>Check unique values</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(unique) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(length)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#    crim      zn   indus    chas     nox      rm     age     dis     rad     tax
#     504      26      76       2      81     446     356     412       9      66
# ptratio   black   lstat    medv
#      46     357     455     229</code></pre></div>
<p><code>CHAS</code> is of course something which should be a factor, and with <code>RAD</code>
having only \(9\) levels, I&rsquo;m inclined to make it a factor as well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">&lt;-</span>boston <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">mutate</span>(rad<span style="color:#f92672">=</span><span style="color:#a6e22e">factor</span>(rad),chas<span style="color:#f92672">=</span><span style="color:#a6e22e">factor</span>(chas))</code></pre></div>
<ul>
<li>Make a median variable</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston<span style="color:#f92672">$</span>highCrime<span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">ifelse</span>(boston<span style="color:#f92672">$</span>crim<span style="color:#f92672">&lt;</span>boston<span style="color:#f92672">$</span>crim <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">median</span>(),<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">factor</span>()</code></pre></div>
<ul>
<li>Take a look at the data</li>
</ul>

<p>Some box-plots:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">pivot_longer</span>(<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(rad,chas,highCrime),names_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Param&#34;</span>,values_to<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Value&#34;</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x<span style="color:#f92672">=</span>highCrime,y<span style="color:#f92672">=</span>Value,fill<span style="color:#f92672">=</span>chas)) <span style="color:#f92672">+</span>
  <span style="color:#a6e22e">geom_boxplot</span>()<span style="color:#f92672">+</span>
  <span style="color:#a6e22e">facet_wrap</span>(<span style="color:#f92672">~</span>Param,scales<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;free_y&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-75-1.png"   />

        
    </figure>



<p>It is surprising, but evidently the <code>CHAS</code> variable is strangely
relevant. 1 implies the tract bounds the river, otherwise 0.</p>

<ul>
<li>Correlations</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">boston <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">select</span>(<span style="color:#f92672">-</span><span style="color:#a6e22e">c</span>(rad,chas,highCrime)) <span style="color:#f92672">%&gt;%</span> cor</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#               crim         zn      indus        nox         rm        age
# crim     1.0000000 -0.2004692  0.4065834  0.4209717 -0.2192467  0.3527343
# zn      -0.2004692  1.0000000 -0.5338282 -0.5166037  0.3119906 -0.5695373
# indus    0.4065834 -0.5338282  1.0000000  0.7636514 -0.3916759  0.6447785
# nox      0.4209717 -0.5166037  0.7636514  1.0000000 -0.3021882  0.7314701
# rm      -0.2192467  0.3119906 -0.3916759 -0.3021882  1.0000000 -0.2402649
# age      0.3527343 -0.5695373  0.6447785  0.7314701 -0.2402649  1.0000000
# dis     -0.3796701  0.6644082 -0.7080270 -0.7692301  0.2052462 -0.7478805
# tax      0.5827643 -0.3145633  0.7207602  0.6680232 -0.2920478  0.5064556
# ptratio  0.2899456 -0.3916785  0.3832476  0.1889327 -0.3555015  0.2615150
# black   -0.3850639  0.1755203 -0.3569765 -0.3800506  0.1280686 -0.2735340
# lstat    0.4556215 -0.4129946  0.6037997  0.5908789 -0.6138083  0.6023385
# medv    -0.3883046  0.3604453 -0.4837252 -0.4273208  0.6953599 -0.3769546
#                dis        tax    ptratio      black      lstat       medv
# crim    -0.3796701  0.5827643  0.2899456 -0.3850639  0.4556215 -0.3883046
# zn       0.6644082 -0.3145633 -0.3916785  0.1755203 -0.4129946  0.3604453
# indus   -0.7080270  0.7207602  0.3832476 -0.3569765  0.6037997 -0.4837252
# nox     -0.7692301  0.6680232  0.1889327 -0.3800506  0.5908789 -0.4273208
# rm       0.2052462 -0.2920478 -0.3555015  0.1280686 -0.6138083  0.6953599
# age     -0.7478805  0.5064556  0.2615150 -0.2735340  0.6023385 -0.3769546
# dis      1.0000000 -0.5344316 -0.2324705  0.2915117 -0.4969958  0.2499287
# tax     -0.5344316  1.0000000  0.4608530 -0.4418080  0.5439934 -0.4685359
# ptratio -0.2324705  0.4608530  1.0000000 -0.1773833  0.3740443 -0.5077867
# black    0.2915117 -0.4418080 -0.1773833  1.0000000 -0.3660869  0.3334608
# lstat   -0.4969958  0.5439934  0.3740443 -0.3660869  1.0000000 -0.7376627
# medv     0.2499287 -0.4685359 -0.5077867  0.3334608 -0.7376627  1.0000000</code></pre></div>
<p>Now, unsurprisingly, there&rsquo;s nothing which is really strongly correlated
here for some reason.</p>

<ul>
<li>Train test splits</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">set.seed</span>(<span style="color:#ae81ff">1984</span>)
trainIndCri <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">createDataPartition</span>(boston<span style="color:#f92672">$</span>highCrime, <span style="color:#75715e"># Factor, so class sampling</span>
                                p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.7</span>, <span style="color:#75715e"># 70-30 train-test</span>
                                list<span style="color:#f92672">=</span><span style="color:#66d9ef">FALSE</span>, <span style="color:#75715e"># No lists</span>
                                times<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># No bootstrap</span>
bostonTrain<span style="color:#f92672">&lt;-</span>boston[trainIndCri,]
bostonTest<span style="color:#f92672">&lt;-</span>boston[<span style="color:#f92672">-</span>trainIndCri,]</code></pre></div>
<ul>
<li>Make a bunch of models</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">glmBos.fit<span style="color:#f92672">=</span><span style="color:#a6e22e">glm</span>(highCrime<span style="color:#f92672">~</span>., data<span style="color:#f92672">=</span>bostonTrain, family<span style="color:#f92672">=</span>binomial)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Warning: glm.fit: algorithm did not converge</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">glmBos.probs <span style="color:#f92672">=</span> <span style="color:#a6e22e">predict</span>(glmBos.fit,bostonTest, type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;response&#34;</span>)
glmBos.pred <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#ae81ff">1</span>,<span style="color:#a6e22e">length</span>(glmBos.probs))
glmBos.pred[glmBos.probs<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0.5</span>]<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
glmBos.pred<span style="color:#f92672">=</span><span style="color:#a6e22e">factor</span>(glmBos.pred)
<span style="color:#a6e22e">confusionMatrix</span>(glmBos.pred,bostonTest<span style="color:#f92672">$</span>highCrime)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction  0  1
#          0 68  6
#          1  7 69
#
#                Accuracy : 0.9133
#                  95% CI : (0.8564, 0.953)
#     No Information Rate : 0.5
#     P-Value [Acc &gt; NIR] : &lt;2e-16
#
#                   Kappa : 0.8267
#
#  Mcnemar&#39;s Test P-Value : 1
#
#             Sensitivity : 0.9067
#             Specificity : 0.9200
#          Pos Pred Value : 0.9189
#          Neg Pred Value : 0.9079
#              Prevalence : 0.5000
#          Detection Rate : 0.4533
#    Detection Prevalence : 0.4933
#       Balanced Accuracy : 0.9133
#
#        &#39;Positive&#39; Class : 0
#</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">bostonLDA<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(highCrime<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>bostonTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lda&#39;</span>)
bostonQDA<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(highCrime<span style="color:#f92672">~</span>tax<span style="color:#f92672">+</span>crim,data<span style="color:#f92672">=</span>bostonTrain,method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;qda&#39;</span>)
bostonKNN<span style="color:#f92672">=</span><span style="color:#a6e22e">train</span>(highCrime<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>bostonTrain,preProcess <span style="color:#f92672">=</span> <span style="color:#a6e22e">c</span>(<span style="color:#e6db74">&#34;center&#34;</span>,<span style="color:#e6db74">&#34;scale&#34;</span>),method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;knn&#39;</span>)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">bLDAp<span style="color:#f92672">=</span><span style="color:#a6e22e">predict</span>(bostonLDA,bostonTest)
bQDAp<span style="color:#f92672">=</span><span style="color:#a6e22e">predict</span>(bostonQDA,bostonTest)
bKNNp<span style="color:#f92672">=</span><span style="color:#a6e22e">predict</span>(bostonKNN,bostonTest)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">confusionMatrix</span>(bLDAp,bostonTest<span style="color:#f92672">$</span>highCrime)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction  0  1
#          0 72  6
#          1  3 69
#
#                Accuracy : 0.94
#                  95% CI : (0.8892, 0.9722)
#     No Information Rate : 0.5
#     P-Value [Acc &gt; NIR] : &lt;2e-16
#
#                   Kappa : 0.88
#
#  Mcnemar&#39;s Test P-Value : 0.505
#
#             Sensitivity : 0.9600
#             Specificity : 0.9200
#          Pos Pred Value : 0.9231
#          Neg Pred Value : 0.9583
#              Prevalence : 0.5000
#          Detection Rate : 0.4800
#    Detection Prevalence : 0.5200
#       Balanced Accuracy : 0.9400
#
#        &#39;Positive&#39; Class : 0
#</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">confusionMatrix</span>(bQDAp,bostonTest<span style="color:#f92672">$</span>highCrime)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction  0  1
#          0 73  5
#          1  2 70
#
#                Accuracy : 0.9533
#                  95% CI : (0.9062, 0.981)
#     No Information Rate : 0.5
#     P-Value [Acc &gt; NIR] : &lt;2e-16
#
#                   Kappa : 0.9067
#
#  Mcnemar&#39;s Test P-Value : 0.4497
#
#             Sensitivity : 0.9733
#             Specificity : 0.9333
#          Pos Pred Value : 0.9359
#          Neg Pred Value : 0.9722
#              Prevalence : 0.5000
#          Detection Rate : 0.4867
#    Detection Prevalence : 0.5200
#       Balanced Accuracy : 0.9533
#
#        &#39;Positive&#39; Class : 0
#</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">confusionMatrix</span>(bKNNp,bostonTest<span style="color:#f92672">$</span>highCrime)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Confusion Matrix and Statistics
#
#           Reference
# Prediction  0  1
#          0 74  6
#          1  1 69
#
#                Accuracy : 0.9533
#                  95% CI : (0.9062, 0.981)
#     No Information Rate : 0.5
#     P-Value [Acc &gt; NIR] : &lt;2e-16
#
#                   Kappa : 0.9067
#
#  Mcnemar&#39;s Test P-Value : 0.1306
#
#             Sensitivity : 0.9867
#             Specificity : 0.9200
#          Pos Pred Value : 0.9250
#          Neg Pred Value : 0.9857
#              Prevalence : 0.5000
#          Detection Rate : 0.4933
#    Detection Prevalence : 0.5333
#       Balanced Accuracy : 0.9533
#
#        &#39;Positive&#39; Class : 0
#</code></pre></div>
<p>Clearly in this particular case, an LDA model seems to be working out
the best for this data when trained on all the parameters, though
Logistic Regression is doing quite well too.</p>

<ul>
<li>Notes on KNN</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(bostonKNN)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-84-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(bostonKNN, print.thres <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;S&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-85-1.png"   />

        
    </figure>



<ul>
<li>Comparison</li>
</ul>

<p>Finally, we will quickly plot some indicative measures.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">knnBosROC<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">roc</span>(predictor<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(bKNNp),response<span style="color:#f92672">=</span>bostonTest<span style="color:#f92672">$</span>highCrime)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting levels: control = 0, case = 1</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting direction: controls &lt; cases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">logiBosROC<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">roc</span>(predictor<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(glmBos.probs),response<span style="color:#f92672">=</span>bostonTest<span style="color:#f92672">$</span>highCrime)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting levels: control = 0, case = 1
# Setting direction: controls &lt; cases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">ldaBosROC<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">roc</span>(predictor<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(bLDAp),response<span style="color:#f92672">=</span>bostonTest<span style="color:#f92672">$</span>highCrime)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting levels: control = 0, case = 1
# Setting direction: controls &lt; cases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">qdaBosROC<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">roc</span>(predictor<span style="color:#f92672">=</span><span style="color:#a6e22e">as.numeric</span>(bQDAp),response<span style="color:#f92672">=</span>bostonTest<span style="color:#f92672">$</span>highCrime)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text"># Setting levels: control = 0, case = 1
# Setting direction: controls &lt; cases</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">ggroc</span>(<span style="color:#a6e22e">list</span>(KNN<span style="color:#f92672">=</span>knnBosROC,Logistic<span style="color:#f92672">=</span>logiBosROC,LDA<span style="color:#f92672">=</span>ldaBosROC,QDA<span style="color:#f92672">=</span>qdaBosROC))</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol2/unnamed-chunk-87-1.png"   />

        
            <figcaption class="center" >Figure 14: plot of chunk unnamed-chunk-87</figcaption>
        
    </figure>



<p>OK, one of the reasons why these models do so well is because they are
all assuming an equal distribution of train and test classes, and they
use <code>crim</code> itself as a predictor. This is no doubt a strong reason why
these models uniformly perform so well. I&rsquo;d say 5 is the best option.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1">James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science &amp; Business Media.
 <a class="footnote-return" href="#fnref:fn-1"><sup>[return]</sup></a></li>
<li id="fn:fn-2">A good introduction to the caret and skimr packages <a href="https://www.machinelearningplus.com/machine-learning/caret-package/" target="_blank">is here</a>
 <a class="footnote-return" href="#fnref:fn-2"><sup>[return]</sup></a></li>
</ol>
</div>
]]></content>
        </item>
        
        <item>
            <title>Pandora and Proxychains</title>
            <link>https://rgoswami.me/posts/pandora-proxychains/</link>
            <pubDate>Sat, 15 Feb 2020 05:28:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/pandora-proxychains/</guid>
            <description>Background  Pandora doesn&amp;rsquo;t work outside the states I keep forgetting how to set-up proxychains  Proxychains Technically this article expects proxychains-ng, which seems to be the more up-to-date fork of the original proxychains.
 Install proxychains-ng
# I am on archlinux.. sudo pacman -S proxychains-ng Copy the configuration to the $HOME directory
cp /etc/proxychains.conf . Edit said configuration to add some US-based proxy
  In my particular case, I don&amp;rsquo;t keep the tor section enabled.</description>
            <content type="html"><![CDATA[

<h2 id="background">Background</h2>

<ul>
<li>Pandora doesn&rsquo;t work outside the states</li>
<li>I keep forgetting how to set-up <code>proxychains</code></li>
</ul>

<h2 id="proxychains">Proxychains</h2>

<p>Technically this article <a href="https://github.com/rofl0r/proxychains-ng" target="_blank">expects proxychains-ng</a>, which seems to be the more
up-to-date fork of the original <code>proxychains</code>.</p>

<ol>
<li><p>Install <code>proxychains-ng</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># I am on archlinux..</span>
sudo pacman -S proxychains-ng</code></pre></div></li>

<li><p>Copy the configuration to the <code>$HOME</code> directory</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cp /etc/proxychains.conf .</code></pre></div></li>

<li><p>Edit said configuration to add some US-based proxy</p></li>
</ol>

<p>In my particular case, I don&rsquo;t keep the tor section enabled.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tail $HOME/proxychains.conf</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#
#       proxy types: http, socks4, socks5
#        ( auth types supported: &#34;basic&#34;-http  &#34;user/pass&#34;-socks )
#
[ProxyList]
# add proxy here ...
# meanwile
# defaults set to &#34;tor&#34;
# socks4 	127.0.0.1 9050</code></pre></div>
<p>I actually use <a href="https://windscribe.com" target="_blank">Windscribe</a> for my VPN needs, and they have a neat <a href="https://windscribe.com/getconfig/socks" target="_blank">SOCKS5 proxy
setup</a>. This works out to a line like <code>socks5 $IP $PORT $USERNAME $PASS</code> being
added. The default generator gives you a pretty server name, but to get the IP
I use <code>ping $SERVER</code> and put that in the <code>conf</code> file.</p>

<h2 id="pandora">Pandora</h2>

<p>I use the excellent <code>pianobar</code> frontend.</p>

<ol>
<li><p>Get <a href="https://github.com/PromyLOPh/pianobar" target="_blank">pianobar</a></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo pacman -S pianobar</code></pre></div></li>

<li><p>Use it with <code>proxychains</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">proxychains pianobar</code></pre></div></li>

<li><p>Profit</p></li>
</ol>

<p>I also like setting up some defaults to make life easier:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p ~/.config/pianobar
vim ~/.config/pianobar/config</code></pre></div>
<p>I normally set the following (inspired by the <a href="https://wiki.archlinux.org/index.php/Pianobar" target="_blank">ArchWiki</a>):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-conf" data-lang="conf">audio_quality = {high, medium, low}
autostart_station = $ID
password = &#34;$PASS&#34;
user = &#34;$emailID&#34;</code></pre></div>
<p>The <code>autostart_station ID</code> can be obtained by inspecting the terminal output
during an initial run. I usually set it to the QuickMix station.</p>
]]></content>
        </item>
        
        <item>
            <title>Replacing Jupyter with Orgmode</title>
            <link>https://rgoswami.me/posts/jupyter-orgmode/</link>
            <pubDate>Thu, 13 Feb 2020 22:36:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/jupyter-orgmode/</guid>
            <description>Background  I dislike Jupyter notebooks (and JupyterHub) a lot EIN is really not much of a solution either  In the past I have written some posts on TeX with JupyterHub and discussed ways to use virtual Python with JupyterHub in a more reasonable manner.
However, I personally found that EIN was a huge pain to work with, and I mostly ended up working with the web-interface anyway.</description>
            <content type="html"><![CDATA[

<h2 id="background">Background</h2>

<ul>
<li>I dislike Jupyter notebooks (and <a href="https://jupyter.org/" target="_blank">JupyterHub</a>) a lot</li>
<li><a href="https://tkf.github.io/emacs-ipython-notebook/" target="_blank">EIN</a> is really not much of a solution either</li>
</ul>

<p>In the past I have written some posts on <a href="https://grimoire.science/latex-and-jupyterhub/" target="_blank">TeX with JupyterHub</a> and discussed ways
to use virtual <a href="https://grimoire.science/python-and-jupyterhub/" target="_blank">Python with JupyterHub</a> in a more reasonable manner.</p>

<p>However, I personally found that EIN was a huge pain to work with, and I mostly
ended up working with the web-interface anyway.</p>

<p>It is a bit redundant to do so, given that at-least for my purposes, the end
result was a LaTeX document. Breaking down the rest of my requirements went a
bit like this:</p>

<dl>
<dt>What exports well to TeX?</dt>
<dd><strong>Org</strong>, Markdown, anything which goes into pandoc</dd>
<dt>What displays code really well?</dt>
<dd>LaTeX, Markdown, <strong>Org</strong></dd>
<dt>What allows easy visualization of code snippets?</dt>
<dd>Rmarkdown, RStudio,
JupyterHub, <strong>Org</strong> with babel</dd>
</dl>

<p>Clearly, <a href="https://orgmode.org/manual/" target="_blank">orgmode</a> is the common denominator, and ergo, a perfect JupyterHub alternative.</p>

<h2 id="setup">Setup</h2>

<p>Throughout this post I will assume the following structure:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tree tmp
mkdir -p tmp/images
touch tmp/myFakeJupyter.org</code></pre></div>
<table>
<thead>
<tr>
<th>tmp</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>├──</td>
<td>images</td>
<td></td>
<td></td>
</tr>

<tr>
<td>└──</td>
<td>myFakeJupyter.org</td>
<td></td>
<td></td>
</tr>

<tr>
<td>1</td>
<td>directory,</td>
<td>1</td>
<td>file</td>
</tr>
</tbody>
</table>

<p>As is evident, we have a folder <code>tmp</code> which will have all the things we need for
dealing with our setup.</p>

<h3 id="virtual-python">Virtual Python</h3>

<p>Without waxing too eloquent on the whole reason behind doing this, since I will
rant about virtual python management systems elsewhere, here I will simply
describe my preferred method, which is <a href="https://python-poetry.org/" target="_blank">using poetry</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># In a folder above tmp</span>
poetry init
poetry add numpy matplotlib scipy pandas</code></pre></div>
<p>The next part is optional, but a good idea if you figure out <a href="https://direnv.net/" target="_blank">using direnv</a> and
have configured <code>layout_poetry</code> as <a href="https://rgoswami.me/posts/poetry-direnv" target="_blank">described here</a>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Same place as the poetry files</span>
echo <span style="color:#e6db74">&#34;layout_poetry()&#34;</span> &gt;&gt; .envrc</code></pre></div>
<p><strong>Note:</strong></p>

<ul>
<li>We can nest an arbitrary number of the <code>tmp</code> structures under a single place
we define the poetry setup</li>
<li>I prefer using <code>direnv</code> to ensure that I never forget to hook into the right environment</li>
</ul>

<h2 id="orgmode">Orgmode</h2>

<p>This is not an introduction to org, however in particular, there are some basic
settings to keep in mind to make sure the set-up works as expected.</p>

<h3 id="indentation">Indentation</h3>

<p>Python is notoriously weird about whitespace, so we will ensure that our export
process does not mangle whitespace and offend the python interpreter. We will
have the following line at the top of our <code>orgmode</code> file:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-orgmode" data-lang="orgmode"><span style="color:#75715e"># -*- org-src-preserve-indentation: t; org-edit-src-content: 0; -*-</span></code></pre></div>
<p><strong>Note:</strong></p>

<ul>
<li>this post is actually generating the file being discussed here by</li>
</ul>

<p><a href="https://orgmode.org/manual/Extracting-Source-Code.html" target="_blank">tangling the file</a></p>

<ul>
<li>You can get the <a href="https://github.com/HaoZeke/haozeke.github.io/blob/src/content-org/tmp/myFakeJupyter.org" target="_blank">whole file here</a></li>
</ul>

<h3 id="tex-settings">TeX Settings</h3>

<p>These are also basically optional, but at the very least you will need the
following:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-orgmode" data-lang="orgmode"><span style="color:#75715e">#+author</span><span style="color:#75715e">: Rohit Goswami</span>
<span style="color:#75715e">#+title</span><span style="color:#75715e">: Whatever</span>
<span style="color:#75715e">#+subtitle</span><span style="color:#75715e">: Wittier line about whatever</span>
<span style="color:#75715e">#+date</span><span style="color:#75715e">: \today</span>
<span style="color:#75715e">#+OPTIONS</span><span style="color:#75715e">: toc:nil</span></code></pre></div>
<p>I actually use a lot of math using the <code>TeX</code> input mode in Emacs, so I like the
following settings for math:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-orgmode" data-lang="orgmode"><span style="color:#75715e"># For math display</span>
<span style="color:#75715e">#+LATEX_HEADER</span><span style="color:#75715e">: \usepackage{amsfonts}</span>
<span style="color:#75715e">#+LATEX_HEADER</span><span style="color:#75715e">: \usepackage{unicode-math}</span></code></pre></div>
<p>There are a bunch of other settings which may be used, but these are the bare
minimum, more on that would be in a snippet anyway.</p>

<p><strong>Note:</strong></p>

<ul>
<li>rendering math in the <code>orgmode</code> file in this manner requires that we
use <code>XeTeX</code> to compile the final file</li>
</ul>

<h3 id="org-python">Org-Python</h3>

<p>We essentially need to ensure that:</p>

<ul>
<li>Babel uses our virtual python</li>
<li>The same session is used for each block</li>
</ul>

<p>We will get our poetry python pretty easily:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">which python</code></pre></div>
<p>Now we will use this as a common <code>header-arg</code> passed into the property drawer to
make sure we don&rsquo;t need to set them in every code block.</p>

<p>We can use the following structure in our file:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-orgmode" data-lang="orgmode">\* Python Stuff
<span style="color:#75715e">  :PROPERTIES:
</span><span style="color:#75715e"></span><span style="color:#75715e">  :header-args:    :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both
</span><span style="color:#75715e"></span><span style="color:#75715e">  :END:</span>
Now we can simply work with code as we normally would
\#+BEGIN_SRC python
print(&#34;Hello World&#34;)
\#+END_SRC</code></pre></div>
<p><strong>Note:</strong></p>

<ul>
<li>For some reason, this property needs to be set on <strong>every</strong> heading (as of Feb 13 2020)</li>
<li>In the actual file you will want to remove extraneous  \ symbols:

<ul>
<li>\* → *</li>
<li>\#+BEGIN_SRC → #+BEGIN_SRC</li>
<li>\#+END_SRC → #+END_SRC</li>
</ul></li>
</ul>

<h3 id="python-images-and-orgmode">Python Images and Orgmode</h3>

<p>To view images in <code>orgmode</code> as we would in a JupyterLab notebook, we will use a
slight trick.</p>

<ul>
<li>We will ensure that the code block returns a file object with the arguments</li>

<li><p>The code block should end with a print statement to actually generate the file
name</p>

<p>So we want a code block like this:</p></li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#+BEGIN_SRC python :results output file :exports both
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=&#39;autumn&#39;)
plt.xlabel(&#39;x1&#39;)
plt.ylabel(&#39;x2&#39;)
plt.savefig(&#39;images/plotCircles.png&#39;, dpi = 300)
print(&#39;images/plotCircles.png&#39;) # return filename to org-mode
#+end_src</code></pre></div>
<p>Which would give the following when executed:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">#+RESULTS:
[[file:images/plotCircles.png]]</code></pre></div>
<p>Since that looks pretty ugly, this will actually look like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> sklearn.datasets.samples_generator <span style="color:#f92672">import</span> make_circles
X, y <span style="color:#f92672">=</span> make_circles(<span style="color:#ae81ff">100</span>, factor<span style="color:#f92672">=.</span><span style="color:#ae81ff">1</span>, noise<span style="color:#f92672">=.</span><span style="color:#ae81ff">1</span>)
plt<span style="color:#f92672">.</span>scatter(X[:, <span style="color:#ae81ff">0</span>], X[:, <span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span>y, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;autumn&#39;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;x1&#39;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;x2&#39;</span>)
plt<span style="color:#f92672">.</span>savefig(<span style="color:#e6db74">&#39;images/plotCircles.png&#39;</span>, dpi <span style="color:#f92672">=</span> <span style="color:#ae81ff">300</span>)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;images/plotCircles.png&#39;</span>) <span style="color:#75715e"># return filename to org-mode</span></code></pre></div>

    <figure class="left" >
        <img src="/ox-hugo/plotCircles.png"   />

        
    </figure>



<h3 id="bonus">Bonus</h3>

<p>A better way to simulate standard <code>jupyter</code> workflows is to just specify the
properties once at the beginning.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-orgmode" data-lang="orgmode"><span style="color:#75715e">#+PROPERTY</span><span style="color:#75715e">: header-args:python :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both</span></code></pre></div>
<p>This setup circumvents having to set the properties per sub-tree, though for
very large projects, it is useful to use different processes.</p>

<h2 id="conclusions">Conclusions</h2>

<ul>
<li>The last step is of course to export the file as to a <code>TeX</code> file and then
compile that with something like <code>latexmk -pdfxe -shell-escape file.tex</code></li>
</ul>

<p>There are a million and one variations of this of course, but this is enough to
get started.</p>

<p>The whole file is also <a href="https://github.com/HaoZeke/haozeke.github.io/blob/src/content-org/tmp/myFakeJupyter.org" target="_blank">reproduced here</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>Poetry and Direnv</title>
            <link>https://rgoswami.me/posts/poetry-direnv/</link>
            <pubDate>Thu, 13 Feb 2020 21:36:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/poetry-direnv/</guid>
            <description>Background  I end up writing about using poetry a lot I almost always use direnv in real life too I don&amp;rsquo;t keep writing mini scripts in my .envrc  Honestly there&amp;rsquo;s nothing here anyone using the direnv wiki will find surprising, but then it is still neat to link back to.
Setting Up Poetry This essentially works by simply modifying the global .direnvrc which essentially gets sourced by every local .</description>
            <content type="html"><![CDATA[

<h2 id="background">Background</h2>

<ul>
<li>I end up writing about using <a href="https://python-poetry.org/" target="_blank">poetry</a> a lot</li>
<li>I almost always <a href="https://direnv.net/" target="_blank">use direnv</a> in real life too</li>
<li>I don&rsquo;t keep writing mini scripts in my <code>.envrc</code></li>
</ul>

<p>Honestly there&rsquo;s nothing here anyone using the <a href="https://github.com/direnv/direnv/wiki/Python" target="_blank">direnv wiki</a> will find surprising,
but then it is still neat to link back to.</p>

<h2 id="setting-up-poetry">Setting Up Poetry</h2>

<p>This essentially works by simply modifying the global <code>.direnvrc</code> which
essentially gets sourced by every local <code>.envrc</code> anyway.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">vim $HOME/.direnvrc</code></pre></div>
<p>So what we put in there is the following snippet derived from other snippets <a href="https://github.com/direnv/direnv/wiki/Python" target="_blank">on
the wiki</a>, and is actually now there too.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># PUT this here</span>
layout_poetry<span style="color:#f92672">()</span> <span style="color:#f92672">{</span>
  <span style="color:#66d9ef">if</span> <span style="color:#f92672">[[</span> ! -f pyproject.toml <span style="color:#f92672">]]</span>; <span style="color:#66d9ef">then</span>
    log_error <span style="color:#e6db74">&#39;No pyproject.toml found.  Use `poetry new` or `poetry init` to create one first.&#39;</span>
    exit <span style="color:#ae81ff">2</span>
  <span style="color:#66d9ef">fi</span>

  local VENV<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>dirname <span style="color:#66d9ef">$(</span>poetry run which python<span style="color:#66d9ef">))</span>
  export VIRTUAL_ENV<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>echo <span style="color:#e6db74">&#34;</span>$VENV<span style="color:#e6db74">&#34;</span> | rev | cut -d<span style="color:#e6db74">&#39;/&#39;</span> -f2- | rev<span style="color:#66d9ef">)</span>
  export POETRY_ACTIVE<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
  PATH_add <span style="color:#e6db74">&#34;</span>$VENV<span style="color:#e6db74">&#34;</span>
<span style="color:#f92672">}</span></code></pre></div>
<p>Now we can just make <code>.envrc</code> files with <code>layout_poetry</code> and everything will
<em>just work™</em>.</p>
]]></content>
        </item>
        
        <item>
            <title>Taming Github Notifications</title>
            <link>https://rgoswami.me/posts/ghnotif/</link>
            <pubDate>Wed, 12 Feb 2020 11:36:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/ghnotif/</guid>
            <description>Background As a member of several large organizations, I get a lot of github notifications. Not all of these are of relevance to me. This is especially true of psuedo-monorepo style repositories like the JOSS review system and especially the exercism community.
 I recently (re-)joined the exercism community as a maintainer for the C++ lessons after having been a (sporadic) teacher This was largely in response to a community call to action as the group needed new blood to usher in v3 of the exercism project  Anyway, I have since found that at the small cost of possibly much of my public repo data, I can manage my notifications better with Octobox</description>
            <content type="html"><![CDATA[

<h2 id="background">Background</h2>

<p>As a member of several large organizations, I get a lot of github notifications.
Not all of these are of relevance to me. This is especially true of
<code>psuedo-monorepo</code> style repositories like the <a href="https://github.com/openjournals/joss-reviews" target="_blank">JOSS review system</a> and
<strong>especially</strong> the <a href="https://github.com/exercism/v3/" target="_blank">exercism community</a>.</p>

<ul>
<li>I recently (re-)joined the <a href="https://exercism.io/" target="_blank">exercism community</a> as a maintainer for the C++
lessons after having been a (sporadic) teacher</li>
<li>This was largely in response to a community call to action as the group needed
new blood to usher in <strong>v3</strong> of the exercism project</li>
</ul>

<p>Anyway, I have since found that at the small cost of possibly much of my public
repo data, I can manage my notifications better with <a href="https://octobox.io/" target="_blank">Octobox</a></p>

<h2 id="octobox">Octobox</h2>

<ul>
<li>It appears to be free for now</li>
<li>It syncs on demand (useful)</li>
<li>I can search things quite easily</li>
<li>They have a neat logo</li>
<li>There appear to be many features I probably won&rsquo;t use</li>
</ul>

<p>It looks like this:</p>


    <figure class="left" >
        <img src="/ox-hugo/octoboxSample.png"   />

        
            <figcaption class="center" >Figure 1: Octobox Stock Photo</figcaption>
        
    </figure>


]]></content>
        </item>
        
        <item>
            <title>Site Rationale</title>
            <link>https://rgoswami.me/posts/rationale/</link>
            <pubDate>Tue, 11 Feb 2020 23:28:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/rationale/</guid>
            <description> Why this site exists I have a lot of online presences. I have been around (or at-least, lurking) for over ten years. Almost as long as I have been programming. Anyway, I have a penchant lately for using emacs and honestly there isn&amp;rsquo;t very good support for org-mode files. There are options recently with gatsby as well, but this seemed kinda neat.
What &amp;lsquo;this&amp;rsquo; is  This site is built by Hugo The posts are generated with ox-hugo The theme is based of this excellent one and my modifications are here  What is here  Mostly random thoughts I don&amp;rsquo;t mind people knowing Some tech stuff which isn&amp;rsquo;t coherent enough to be put in any form with references Emacs specific workflows which I might want to write about more than short notes on the config  What isn&amp;rsquo;t here  More coherent thoughts will not be here, that should and will go to my grimoire My doom-emacs configuration Academic stuff is better tracked on Publons or Google Scholar or my pages hosted by my favorite IITK group or UI group  </description>
            <content type="html"><![CDATA[

<h2 id="why-this-site-exists">Why this site exists</h2>

<p>I have a lot of online presences. I have been around (or at-least, lurking) for
over ten years. Almost as long as I have been programming. Anyway, I have a
penchant lately for using <code>emacs</code> and honestly there isn&rsquo;t very good support for
<code>org-mode</code> files. There are options recently with <code>gatsby</code> as well, but this
seemed kinda neat.</p>

<h2 id="what-this-is">What &lsquo;this&rsquo; is</h2>

<ul>
<li>This site is <a href="http://gohugo.io/" target="_blank">built by Hugo</a></li>
<li>The posts are <a href="https://ox-hugo.scripter.co/" target="_blank">generated with ox-hugo</a></li>
<li>The theme is based of this <a href="https://github.com/rhazdon/hugo-theme-hello-friend-ng" target="_blank">excellent one</a> and my modifications <a href="https://github.com/HaoZeke/hugo-theme-hello-friend-ng-hz" target="_blank">are here</a></li>
</ul>

<h2 id="what-is-here">What is here</h2>

<ul>
<li>Mostly random thoughts I don&rsquo;t mind people knowing</li>
<li>Some tech stuff which isn&rsquo;t coherent enough to be put in any form with
references</li>
<li>Emacs specific workflows which I might want to write about more than <a href="https://dotdoom.grimoire.science/" target="_blank">short
notes on the config</a></li>
</ul>

<h2 id="what-isn-t-here">What isn&rsquo;t here</h2>

<ul>
<li>More coherent thoughts will <strong>not</strong> be here, that should and will go to my <a href="https://grimoire.science" target="_blank">grimoire</a></li>
<li>My <a href="https://dotdoom.grimoire.science/" target="_blank">doom-emacs configuration</a></li>
<li>Academic stuff is better tracked on <a href="https://publons.com/researcher/2911170/rohit-goswami/" target="_blank">Publons</a> or <a href="https://scholar.google.co.in/citations?user=36gIdJMAAAAJ&amp;hl=en" target="_blank">Google Scholar</a> or my pages
hosted by my favorite <a href="https://femtolab.science/people/rohit" target="_blank">IITK group</a> or <a href="https://www.hi.is/starfsfolk/rog32" target="_blank">UI group</a></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>ISLR :: Multiple Linear Regression</title>
            <link>https://rgoswami.me/posts/islr-ch2-ch3/</link>
            <pubDate>Wed, 15 Jan 2020 05:28:00 +0000</pubDate>
            
            <guid>https://rgoswami.me/posts/islr-ch2-ch3/</guid>
            <description>Chapter II - Statistical Learning All the questions are as per the ISL seventh printing&amp;nbsp;1.
Question 2.8 - Pages 54-55 This exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for \(777\) different universities and colleges in the US. The variables are
 Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10 % of high school class Top25perc : New students from top 25 % of high school class F.</description>
            <content type="html"><![CDATA[

<h2 id="chapter-ii---statistical-learning">Chapter II - Statistical Learning</h2>

<p>All the questions are as per the
<a href="https://faculty.marshall.usc.edu/gareth-james/ISL/" target="_blank">ISL seventh
printing</a>&nbsp;<sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup>.</p>

<h3 id="question-2.8---pages-54-55">Question 2.8 - Pages 54-55</h3>

<p>This exercise relates to the <code>College</code> data set, which can be found in
the file <code>College.csv</code>. It contains a number of variables for \(777\)
different universities and colleges in the US. The variables are</p>

<ul>
<li><code>Private</code> : Public/private indicator</li>
<li><code>Apps</code> : Number of applications received</li>
<li><code>Accept</code> : Number of applicants accepted</li>
<li><code>Enroll</code> : Number of new students enrolled</li>
<li><code>Top10perc</code> : New students from top 10 % of high school class</li>
<li><code>Top25perc</code> : New students from top 25 % of high school class</li>
<li><code>F.Undergrad</code> : Number of full-time undergraduates</li>
<li><code>P.Undergrad</code> : Number of part-time undergraduates</li>
<li><code>Outstate</code> : Out-of-state tuition</li>
<li><code>Room.Board</code> : Room and board costs</li>
<li><code>Books</code> : Estimated book costs</li>
<li><code>Personal</code> : Estimated personal spending</li>
<li><code>PhD</code> : Percent of faculty with Ph.D.&rsquo;s</li>
<li><code>Terminal</code> : Percent of faculty with terminal degree</li>
<li><code>S.F.Ratio</code> : Student/faculty ratio</li>
<li><code>perc.alumni</code> : Percent of alumni who donate</li>
<li><code>Expend</code> : Instructional expenditure per student</li>
<li><code>Grad.Rate</code> : Graduation rate</li>
</ul>

<p>Before reading the data into R, it can be viewed in Excel or a text
editor.</p>

<p><strong>(a)</strong> Use the <code>read.csv()</code> function to read the data into R . Call the
loaded data <code>college</code>. Make sure that you have the directory set to the
correct location for the data.</p>

<p><strong>(b)</strong> Look at the data using the <code>fix()</code> function. You should notice
that the ﬁrst column is just the name of each university. We don&rsquo;t
really want R to treat this as data. However, it may be handy to have
these names for later. Try the following commands:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">rownames</span>(college)<span style="color:#f92672">=</span>college[,<span style="color:#ae81ff">1</span>]
<span style="color:#a6e22e">fix</span>(college)</code></pre></div>
<p>You should see that there is now a row.names column with the name of
each university recorded. This means that R has given each row a name
corresponding to the appropriate university. R will not try to perform
calculations on the row names. However, we still need to eliminate the
ﬁrst column in the data where the names are stored. Try:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">college<span style="color:#f92672">=</span>college[,<span style="color:#ae81ff">-1</span>]
<span style="color:#a6e22e">fix</span>(college)</code></pre></div>
<p><strong>&copy;</strong></p>

<ol>
<li>Use the <code>summary()</code> function to produce a numerical summary of the
variables in the data set.</li>
<li>Use the <code>pairs()</code> function to produce a scatterplot matrix of the
ﬁrst ten columns or variables of the data. Recall that you can
reference the first ten columns of a matrix <code>A</code> using <code>A[,1:10]</code> .</li>
<li>Use the <code>plot()</code> function to produce side-by-side boxplots of
Outstate versus Private .</li>
<li>Create a new qualitative variable, called Elite , by binning the
<code>Top10perc</code> variable. We are going to divide universities into two
groups based on whether or not the proportion of students coming from
the top \(10%\) of their high school classes exceeds \(50%\).</li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">Elite <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(<span style="color:#e6db74">&#34;No&#34;</span>, <span style="color:#a6e22e">nrow</span>(college))
Elite [college<span style="color:#f92672">$</span>Top10perc <span style="color:#f92672">&gt;</span><span style="color:#ae81ff">50</span>]<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Yes&#34;</span>
Elite <span style="color:#f92672">=</span> <span style="color:#a6e22e">as.factor </span>(Elite)
college <span style="color:#f92672">=</span> <span style="color:#a6e22e">data.frame </span>(college, Elite)</code></pre></div>
<p>Use the <code>summary()</code> function to see how many elite univer- sities there
are. Now use the <code>plot()</code> function to produce side-by-side boxplots of
<code>Outstate</code> versus <code>Elite</code> .</p>

<ol>
<li>Use the hist() function to produce some histograms with differing
numbers of bins for a few of the quantitative vari- ables. You may
fnd the command <code>par(mfrow=c(2,2))</code> useful: it will divide the print
window into four regions so that four plots can be made
simultaneously. Modifying the arguments to this function will divide
the screen in other ways.</li>
<li>Continue exploring the data, and provide a brief summary of what you
discover.</li>
</ol>

<h4 id="answer">Answer</h4>

<p>Instead of reading in data, for ISLR in particular we can load the
<a href="https://rdrr.io/cran/ISLR/" target="_blank">ISLR library</a> which is on CRAN and
contains the data-sets required for the book.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">install.packages</span>(<span style="color:#e6db74">&#34;ISLR&#34;</span>)</code></pre></div>
<p>Thus, we can now read it in as <code>library(&quot;ISLR&quot;)</code></p>

<p>The remaining sections are meant to be executed, and are marked as such,
with <code>r</code> in <code>{}</code>.</p>

<p><strong>&copy;</strong></p>

<p>We will load the dataset once for the whole document.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(<span style="color:#e6db74">&#34;ISLR&#34;</span>)</code></pre></div>
<ol>
<li>Usage of the <code>summary()</code> function</li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(ISLR<span style="color:#f92672">::</span>College)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##  Private        Apps           Accept          Enroll       Top10perc
##  No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00
##  Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00
##            Median : 1558   Median : 1110   Median : 434   Median :23.00
##            Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56
##            3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00
##            Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00
##    Top25perc      F.Undergrad     P.Undergrad         Outstate
##  Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340
##  1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320
##  Median : 54.0   Median : 1707   Median :  353.0   Median : 9990
##  Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441
##  3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925
##  Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700
##    Room.Board       Books           Personal         PhD
##  Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00
##  1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00
##  Median :4200   Median : 500.0   Median :1200   Median : 75.00
##  Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66
##  3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00
##  Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00
##     Terminal       S.F.Ratio      perc.alumni        Expend
##  Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186
##  1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751
##  Median : 82.0   Median :13.60   Median :21.00   Median : 8377
##  Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660
##  3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830
##  Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233
##    Grad.Rate
##  Min.   : 10.00
##  1st Qu.: 53.00
##  Median : 65.00
##  Mean   : 65.46
##  3rd Qu.: 78.00
##  Max.   :118.00</code></pre></div>
<ol>
<li>Usage of <code>pairs()</code></li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">tenColl <span style="color:#f92672">&lt;-</span> ISLR<span style="color:#f92672">::</span>College[,<span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">10</span>] <span style="color:#75715e"># For getting the first ten columns</span>
<span style="color:#a6e22e">pairs</span>(tenColl) <span style="color:#75715e"># Scatterplot</span></code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-3-1.png"   />

        
            <figcaption class="center" >Figure 1: Pairs</figcaption>
        
    </figure>



<ol>
<li>Boxplot creation with <code>plot()</code></li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(ISLR<span style="color:#f92672">::</span>College<span style="color:#f92672">$</span>Private,ISLR<span style="color:#f92672">::</span>College<span style="color:#f92672">$</span>Outstate,xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Private&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Outstate&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-4-1.png"   />

        
            <figcaption class="center" >Figure 2: Boxplots</figcaption>
        
    </figure>



<ol>
<li>Binning and plotting</li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">college<span style="color:#f92672">=</span>ISLR<span style="color:#f92672">::</span>College
Elite<span style="color:#f92672">=</span><span style="color:#a6e22e">rep</span>(<span style="color:#e6db74">&#34;No&#34;</span>,<span style="color:#a6e22e">nrow</span>(college))
Elite[college<span style="color:#f92672">$</span>Top10perc<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">50</span>]<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Yes&#34;</span>
Elite<span style="color:#f92672">=</span><span style="color:#a6e22e">as.factor</span>(Elite)
college<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">data.frame</span>(college,Elite)
<span style="color:#a6e22e">summary</span>(college<span style="color:#f92672">$</span>Elite)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##  No Yes
## 699  78</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(college<span style="color:#f92672">$</span>Outstate,college<span style="color:#f92672">$</span>Elite,xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Outstate&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Elite&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-6-1.png"   />

        
            <figcaption class="center" >Figure 3: Plotting Outstate and Elite</figcaption>
        
    </figure>



<ol>
<li>Histograms with <code>hist()</code></li>
</ol>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">hist</span>(college<span style="color:#f92672">$</span>Enroll)
<span style="color:#a6e22e">hist</span>(college<span style="color:#f92672">$</span>perc.alumni, col<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
<span style="color:#a6e22e">hist</span>(college<span style="color:#f92672">$</span>Personal, col<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, breaks<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
<span style="color:#a6e22e">hist</span>(college<span style="color:#f92672">$</span>PhD, breaks<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-7-1.png"   />

        
            <figcaption class="center" >Figure 4: Histogram</figcaption>
        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">hist</span>(college<span style="color:#f92672">$</span>Top10perc, col<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;blue&#34;</span>)
<span style="color:#a6e22e">hist</span>(college<span style="color:#f92672">$</span>Outstate, col<span style="color:#f92672">=</span><span style="color:#ae81ff">23</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-7-2.png"   />

        
            <figcaption class="center" >Figure 5: Colored Histogram</figcaption>
        
    </figure>



<ol>
<li>Explorations (graphical)</li>
</ol>

<p>\(0\) implies the faculty have PhDs. It is clear that people donate more
when faculty do not have terminal degrees.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(college<span style="color:#f92672">$</span>Terminal<span style="color:#f92672">-</span>college<span style="color:#f92672">$</span>PhD, college<span style="color:#f92672">$</span>perc.alumni)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-8-1.png"   />

        
            <figcaption class="center" >Figure 6: Terminal degrees and alumni</figcaption>
        
    </figure>



<p>High tuition correlates to high graduation rate.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(college<span style="color:#f92672">$</span>Expend, college<span style="color:#f92672">$</span>Grad.Rate)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-9-1.png"   />

        
            <figcaption class="center" >Figure 7: Tuiton and graduation</figcaption>
        
    </figure>



<p>Low acceptance implies a low student to faculty ratio.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">plot</span>(college<span style="color:#f92672">$</span>Accept <span style="color:#f92672">/</span> college<span style="color:#f92672">$</span>Apps, college<span style="color:#f92672">$</span>S.F.Ratio)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-10-1.png"   />

        
            <figcaption class="center" >Figure 8: Acceptance and Student/Faculty ratio</figcaption>
        
    </figure>



<h3 id="question-2.9---page-56">Question 2.9 - Page 56</h3>

<p>This exercise involves the <code>Auto</code> data set studied in the lab. Make sure
that the missing values have been removed from the data.</p>

<p><strong>(a)</strong> Which of the predictors are quantitative, and which are
qualitative?</p>

<p><strong>(b)</strong> What is the <em>range</em> of each quantitative predictor? You can answer
this using the <code>range()</code> function.</p>

<p><strong>&copy;</strong> What is the mean and standard deviation of each quantitative
predictor?</p>

<p><strong>(d)</strong> Now remove the 10th through 85th observations. What is the range,
mean, and standard deviation of each predictor in the subset of the data
that remains?</p>

<p><strong>(e)</strong> Using the full data set, investigate the predictors graphically,
using scatterplots or other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment on your
findings.</p>

<p><strong>(f)</strong> Suppose that we wish to predict gas mileage (<code>mpg</code>) on the basis
of the other variables. Do your plots suggest that any of the other
variables might be useful in predicting <code>mpg</code>? Justify your answer.</p>

<h4 id="answer-1">Answer</h4>

<p>Once again, since the dataset is loaded from the library, we will simply
start manipulating it.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># Clean data</span>
autoDat<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">na.omit</span>(ISLR<span style="color:#f92672">::</span>Auto) <span style="color:#75715e"># renamed for convenience</span></code></pre></div>
<p><strong>(a)</strong> To determine weather the variables a qualitative or quantitative
we can either inspect the variables by eye, or query the dataset.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(autoDat) <span style="color:#75715e"># Observe the output for variance</span></code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       mpg          cylinders      displacement     horsepower        weight
##  Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613
##  1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225
##  Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804
##  Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978
##  3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615
##  Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140
##
##   acceleration        year           origin                      name
##  Min.   : 8.00   Min.   :70.00   Min.   :1.000   amc matador       :  5
##  1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   ford pinto        :  5
##  Median :15.50   Median :76.00   Median :1.000   toyota corolla    :  5
##  Mean   :15.54   Mean   :75.98   Mean   :1.577   amc gremlin       :  4
##  3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000   amc hornet        :  4
##  Max.   :24.80   Max.   :82.00   Max.   :3.000   chevrolet chevette:  4
##                                                  (Other)           :365</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">str</span>(autoDat) <span style="color:#75715e"># Directly find find out</span></code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    392 obs. of  9 variables:
##  $ mpg         : num  18 15 18 16 17 15 14 14 14 15 ...
##  $ cylinders   : num  8 8 8 8 8 8 8 8 8 8 ...
##  $ displacement: num  307 350 318 304 302 429 454 440 455 390 ...
##  $ horsepower  : num  130 165 150 150 140 198 220 215 225 190 ...
##  $ weight      : num  3504 3693 3436 3433 3449 ...
##  $ acceleration: num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...
##  $ year        : num  70 70 70 70 70 70 70 70 70 70 ...
##  $ origin      : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ name        : Factor w/ 304 levels &#34;amc ambassador brougham&#34;,..: 49 36 231 14 161 141 54 223 241 2 ...</code></pre></div>
<p>From the above view, we can see that there is only one listed as a
qualitative variable or factor, and that is <code>name</code>. However, we can also
do this in a cleaner manner or at-least in a different manner with a
function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">findFactors <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">sapply</span>(autoDat,is.factor)
findFactors</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##          mpg    cylinders displacement   horsepower       weight acceleration
##        FALSE        FALSE        FALSE        FALSE        FALSE        FALSE
##         year       origin         name
##        FALSE        FALSE         TRUE</code></pre></div>
<p>Though only <strong>name</strong> is listed as a qualitative variable, we note that
<strong>origin</strong> seems to be almost qualitative as well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">length</span>(<span style="color:#a6e22e">unique</span>(autoDat<span style="color:#f92672">$</span>origin))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 3</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">unique</span>(autoDat<span style="color:#f92672">$</span>origin)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## [1] 1 3 2</code></pre></div>
<p>Infact we can check that nothing else has this property by repeated
application of <code>sapply</code>, though a pipe would be more satisfying</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">getUniq<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">sapply</span>(autoDat, unique)
getLengths<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">sapply</span>(getUniq,length)
getLengths</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##          mpg    cylinders displacement   horsepower       weight acceleration
##          127            5           81           93          346           95
##         year       origin         name
##           13            3          301</code></pre></div>
<p>This is really nicer with pipes</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">library</span>(dplyr)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Attaching package: &#39;dplyr&#39;</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:stats&#39;:
##
##     filter, lag</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## The following objects are masked from &#39;package:base&#39;:
##
##     intersect, setdiff, setequal, union</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">autoDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(unique) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(length)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##          mpg    cylinders displacement   horsepower       weight acceleration
##          127            5           81           93          346           95
##         year       origin         name
##           13            3          301</code></pre></div>
<p>At any rate, we know now that <strong>origin</strong> and <strong>name</strong> are probably
qualitative, and the rest are quantitative.</p>

<p><strong>(b)</strong> Using <code>range()</code></p>

<p>A nice feature of the dataset we have is that the suspected qualitative
variables are at the end of the dataset. So we can simply select the
first \(7\) rows and go nuts on them.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">autoDat[,<span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">7</span>] <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(range) <span style="color:#75715e"># or sapply(autoDat[,1:7],range)</span></code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       mpg cylinders displacement horsepower weight acceleration year
## [1,]  9.0         3           68         46   1613          8.0   70
## [2,] 46.6         8          455        230   5140         24.8   82</code></pre></div>
<p>Once again, more elegant with pipes and <code>subset()</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">autoDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">subset</span>(select<span style="color:#f92672">=-</span><span style="color:#a6e22e">c</span>(name,origin)) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(range)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       mpg cylinders displacement horsepower weight acceleration year
## [1,]  9.0         3           68         46   1613          8.0   70
## [2,] 46.6         8          455        230   5140         24.8   82</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># Even simpler with dplyr</span>
autoDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">select</span>(<span style="color:#f92672">-</span>name,<span style="color:#f92672">-</span>origin) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(range)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       mpg cylinders displacement horsepower weight acceleration year
## [1,]  9.0         3           68         46   1613          8.0   70
## [2,] 46.6         8          455        230   5140         24.8   82</code></pre></div>
<p><strong>&copy;</strong> Mean and standard deviation</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">&lt;-</span> autoDat <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">select</span>(<span style="color:#f92672">-</span>name,<span style="color:#f92672">-</span>origin)
noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(mean)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##          mpg    cylinders displacement   horsepower       weight acceleration
##    23.445918     5.471939   194.411990   104.469388  2977.584184    15.541327
##         year
##    75.979592</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(sd)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##          mpg    cylinders displacement   horsepower       weight acceleration
##     7.805007     1.705783   104.644004    38.491160   849.402560     2.758864
##         year
##     3.683737</code></pre></div>
<p><strong>(d)</strong> Removing observations 10-85 and testing.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors[<span style="color:#f92672">-</span>(<span style="color:#ae81ff">10</span><span style="color:#f92672">:</span><span style="color:#ae81ff">85</span>),] <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(mean)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##          mpg    cylinders displacement   horsepower       weight acceleration
##    24.404430     5.373418   187.240506   100.721519  2935.971519    15.726899
##         year
##    77.145570</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors[<span style="color:#f92672">-</span>(<span style="color:#ae81ff">10</span><span style="color:#f92672">:</span><span style="color:#ae81ff">85</span>),] <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(sd)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##          mpg    cylinders displacement   horsepower       weight acceleration
##     7.867283     1.654179    99.678367    35.708853   811.300208     2.693721
##         year
##     3.106217</code></pre></div>
<p><strong>(e)</strong> Plots for determining relationships</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(autoDat<span style="color:#f92672">$</span>weight, autoDat<span style="color:#f92672">$</span>horsepower)
<span style="color:#a6e22e">plot</span>(autoDat<span style="color:#f92672">$</span>weight, autoDat<span style="color:#f92672">$</span>acceleration)
<span style="color:#a6e22e">plot</span>(autoDat<span style="color:#f92672">$</span>displacement, autoDat<span style="color:#f92672">$</span>acceleration)
<span style="color:#a6e22e">plot</span>(autoDat<span style="color:#f92672">$</span>cylinders, autoDat<span style="color:#f92672">$</span>acceleration)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-21-1.png"   />

        
            <figcaption class="center" >Figure 9: Relationship determination</figcaption>
        
    </figure>



<ul>
<li>Evidently horsepower is directly proportional to weight but
acceleration is inversely proportional to weight</li>
<li>Acceleration is also inversely proportional to displacement</li>
<li>Cylinders are a poor measure, not surprising since there are only \(5\)
values</li>
</ul>

<p><strong>(f)</strong> Choosing predictors for gas mileage <code>mpg</code></p>

<p>Let us recall certain key elements of the quantitative aspects of the
dataset.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(noFactors) <span style="color:#75715e"># To understand the spread</span></code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       mpg          cylinders      displacement     horsepower        weight
##  Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613
##  1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225
##  Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804
##  Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978
##  3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615
##  Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140
##   acceleration        year
##  Min.   : 8.00   Min.   :70.00
##  1st Qu.:13.78   1st Qu.:73.00
##  Median :15.50   Median :76.00
##  Mean   :15.54   Mean   :75.98
##  3rd Qu.:17.02   3rd Qu.:79.00
##  Max.   :24.80   Max.   :82.00</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">getLengths <span style="color:#75715e"># To get the number of unique values</span></code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##          mpg    cylinders displacement   horsepower       weight acceleration
##          127            5           81           93          346           95
##         year       origin         name
##           13            3          301</code></pre></div>
<p>From this we can assert easily that the number of cylinders is not of
much interest for predictions of the mileage.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(noFactors<span style="color:#f92672">$</span>mpg,noFactors<span style="color:#f92672">$</span>horsepower)
<span style="color:#a6e22e">plot</span>(noFactors<span style="color:#f92672">$</span>mpg,noFactors<span style="color:#f92672">$</span>weight)
<span style="color:#a6e22e">plot</span>(noFactors<span style="color:#f92672">$</span>mpg,noFactors<span style="color:#f92672">$</span>displacement)
<span style="color:#a6e22e">plot</span>(noFactors<span style="color:#f92672">$</span>mpg,noFactors<span style="color:#f92672">$</span>acceleration)
<span style="color:#a6e22e">plot</span>(noFactors<span style="color:#f92672">$</span>mpg,noFactors<span style="color:#f92672">$</span>year)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-23-1.png"   />

        
            <figcaption class="center" >Figure 10: Predictions</figcaption>
        
    </figure>



<ul>
<li>So now we know that the mileage increases when horsepower is low,
weight is low, displacement is low and acceleration is high</li>
</ul>

<p>Where low represents an inverse response and high represents a direct
response.</p>

<ul>
<li>It is also clear that the mileage increases every year</li>
</ul>

<h2 id="chapter-iii---linear-regression">Chapter III - Linear Regression</h2>

<h3 id="question-3.9---page-122">Question 3.9 - Page 122</h3>

<p>This question involves the use of multiple linear regression on the Auto
data set.</p>

<p><strong>(a)</strong> Produce a scatterplot matrix which includes all of the variables
in the data set.</p>

<p><strong>(b)</strong> Compute the matrix of correlations between the variables using the
function <code>cor()</code> . You will need to exclude the name variable, <code>cor()</code>
which is qualitative.</p>

<p><strong>&copy;</strong> Use the <code>lm()</code> function to perform a multiple linear regression
with <code>mpg</code> as the response and all other variables except name as the
predictors. Use the <code>summary()</code> function to print the results. Comment
on the output. For instance:</p>

<ol>
<li>Is there a relationship between the predictors and the response?</li>
<li>Which predictors appear to have a statistically significant
relationship to the response?</li>
<li>What does the coefficient for the year variable suggest?</li>
</ol>

<p><strong>(d)</strong> Use the <code>plot()</code> function to produce diagnostic plots of the
linear regression fit. Comment on any problems you see with the fit. Do
the residual plots suggest any unusually large outliers? Does the
leverage plot identify any observations with unusually high leverage?</p>

<p><strong>(e)</strong> Use the <code>*</code> and <code>:</code> symbols to fit linear regression models with
interaction effects. Do any interactions appear to be statistically
significant?</p>

<p><strong>(f)</strong> Try a few different transformations of the variables, such as
\(\log{X}\), \(\sqrt{X}\), \(X^2\).Comment on your ﬁndings.</p>

<h4 id="answer-2">Answer</h4>

<p>Once again, we will use the dataset from the library.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">cleanAuto <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">na.omit</span>(autoDat)
<span style="color:#a6e22e">summary</span>(cleanAuto) <span style="color:#75715e"># Already created above, so no need to do na.omit again</span></code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       mpg          cylinders      displacement     horsepower        weight
##  Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613
##  1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225
##  Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804
##  Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978
##  3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615
##  Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140
##
##   acceleration        year           origin                      name
##  Min.   : 8.00   Min.   :70.00   Min.   :1.000   amc matador       :  5
##  1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   ford pinto        :  5
##  Median :15.50   Median :76.00   Median :1.000   toyota corolla    :  5
##  Mean   :15.54   Mean   :75.98   Mean   :1.577   amc gremlin       :  4
##  3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000   amc hornet        :  4
##  Max.   :24.80   Max.   :82.00   Max.   :3.000   chevrolet chevette:  4
##                                                  (Other)           :365</code></pre></div>
<p><strong>(a)</strong> Scatterplot</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">pairs</span>(cleanAuto)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-25-1.png"   />

        
            <figcaption class="center" >Figure 11: Scatterplot</figcaption>
        
    </figure>



<p><strong>(b)</strong> Correlation matrix. For this we exclude the qualitative variables
either by using <code>select</code> or by using the existing <code>noFactors</code> dataset</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># A full set</span>
ISLR<span style="color:#f92672">::</span>Auto <span style="color:#f92672">%&gt;%</span> na.omit <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">select</span>(<span style="color:#f92672">-</span>name,<span style="color:#f92672">-</span>origin) <span style="color:#f92672">%&gt;%</span> cor</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##                     mpg  cylinders displacement horsepower     weight
## mpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442
## cylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273
## displacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944
## horsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377
## weight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000
## acceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392
## year          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199
##              acceleration       year
## mpg             0.4233285  0.5805410
## cylinders      -0.5046834 -0.3456474
## displacement   -0.5438005 -0.3698552
## horsepower     -0.6891955 -0.4163615
## weight         -0.4168392 -0.3091199
## acceleration    1.0000000  0.2903161
## year            0.2903161  1.0000000</code></pre></div>
<p><strong>&copy;</strong> Multiple Linear Regression</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># Fit against every variable</span>
lm.fit<span style="color:#f92672">=</span><span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>noFactors)
<span style="color:#a6e22e">summary</span>(lm.fit)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = mpg ~ ., data = noFactors)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -8.6927 -2.3864 -0.0801  2.0291 14.3607
##
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  -1.454e+01  4.764e+00  -3.051  0.00244 **
## cylinders    -3.299e-01  3.321e-01  -0.993  0.32122
## displacement  7.678e-03  7.358e-03   1.044  0.29733
## horsepower   -3.914e-04  1.384e-02  -0.028  0.97745
## weight       -6.795e-03  6.700e-04 -10.141  &lt; 2e-16 ***
## acceleration  8.527e-02  1.020e-01   0.836  0.40383
## year          7.534e-01  5.262e-02  14.318  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 3.435 on 385 degrees of freedom
## Multiple R-squared:  0.8093, Adjusted R-squared:  0.8063
## F-statistic: 272.2 on 6 and 385 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># Fit against one variable</span>
noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>horsepower,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = mpg ~ horsepower, data = .)
##
## Residuals:
##      Min       1Q   Median       3Q      Max
## -13.5710  -3.2592  -0.3435   2.7630  16.9240
##
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***
## horsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 4.906 on 390 degrees of freedom
## Multiple R-squared:  0.6059, Adjusted R-squared:  0.6049
## F-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>year,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = mpg ~ year, data = .)
##
## Residuals:
##      Min       1Q   Median       3Q      Max
## -12.0212  -5.4411  -0.4412   4.9739  18.2088
##
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -70.01167    6.64516  -10.54   &lt;2e-16 ***
## year          1.23004    0.08736   14.08   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 6.363 on 390 degrees of freedom
## Multiple R-squared:  0.337,  Adjusted R-squared:  0.3353
## F-statistic: 198.3 on 1 and 390 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>acceleration,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = mpg ~ acceleration, data = .)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -17.989  -5.616  -1.199   4.801  23.239
##
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    4.8332     2.0485   2.359   0.0188 *
## acceleration   1.1976     0.1298   9.228   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 7.08 on 390 degrees of freedom
## Multiple R-squared:  0.1792, Adjusted R-squared:  0.1771
## F-statistic: 85.15 on 1 and 390 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>weight,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = mpg ~ weight, data = .)
##
## Residuals:
##      Min       1Q   Median       3Q      Max
## -11.9736  -2.7556  -0.3358   2.1379  16.5194
##
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 46.216524   0.798673   57.87   &lt;2e-16 ***
## weight      -0.007647   0.000258  -29.64   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 4.333 on 390 degrees of freedom
## Multiple R-squared:  0.6926, Adjusted R-squared:  0.6918
## F-statistic: 878.8 on 1 and 390 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>displacement,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> summary</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = mpg ~ displacement, data = .)
##
## Residuals:
##      Min       1Q   Median       3Q      Max
## -12.9170  -3.0243  -0.5021   2.3512  18.6128
##
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  35.12064    0.49443   71.03   &lt;2e-16 ***
## displacement -0.06005    0.00224  -26.81   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 4.635 on 390 degrees of freedom
## Multiple R-squared:  0.6482, Adjusted R-squared:  0.6473
## F-statistic: 718.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16</code></pre></div>
<ol>
<li><p>Clearly there is a relationship between the predictors and variables,
mostly as described previously, with the following broad trends:</p>

<ul>
<li>Inversely proportional to Horsepower, Weight, and Displacement</li>
</ul></li>

<li><p>The predictors which have a relationship to the response are (based
on R squared values):
\[ all &gt; weight &gt; displacement &gt; horsepower &gt; year &gt; acceleration \]
However, things lower than horsepower are not statistically
significant.</p></li>

<li><p>The visual analysis of the <code>year</code> variable suggests that the mileage
grows every year. However, it is clear from the summary, that there
is no statistical significance of year when used to fit a single
parameter linear model. We note that when we compare this to the
multiple linear regression analysis, we see that the year factor
accounts for \(0.7508\) of the total, that is, the cars become more
efficient every year</p></li>
</ol>

<p><strong>(d)</strong> Lets plot these</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>horsepower,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">plot</span>(main<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Mileage v/s Horsepower&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-28-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>weight,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">plot</span>(main<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Mileage v/s Weight&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-28-2.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>year,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">plot</span>(main<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Mileage v/s Year&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-28-3.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>acceleration,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">plot</span>(main<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Mileage v/s Acceleration&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-28-4.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>displacement,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">plot</span>(main<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Mileage v/s Displacement&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-28-5.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>.) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">plot</span>(main<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Mileage Multiple Regression&#34;</span>)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-28-6.png"   />

        
    </figure>



<p>Form this we can see that the fit is not very accurate as there is a
clear curve to the residuals. The 14th point has high leverage, though
it is of a small magnitude. Thus it is not expected to have affected the
plot too much.</p>

<p><strong>We know</strong> that an observation with a studentized residual greater than
\(3\) in absolute value are possible outliers. Hence we must plot this.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># Predict and get the plot</span>
fitPlot <span style="color:#f92672">&lt;-</span> noFactors <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>.)
<span style="color:#75715e"># See residuals</span>
<span style="color:#a6e22e">plot</span>(xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Prediction&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Studentized Residual&#34;</span>,x<span style="color:#f92672">=</span><span style="color:#a6e22e">predict</span>(fitPlot),y<span style="color:#f92672">=</span><span style="color:#a6e22e">rstudent</span>(fitPlot))</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-29-1.png"   />

        
    </figure>


<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># Try a linear fit of studentized residuals</span>
<span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(<span style="color:#a6e22e">lm</span>(<span style="color:#a6e22e">predict</span>(fitPlot)<span style="color:#f92672">~</span><span style="color:#a6e22e">rstudent</span>(fitPlot)))</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-29-2.png"   />

        
    </figure>



<p>Clearly the studentized residuals are nonlinear w.r.t the prediction.
Also, some points are above the absolute value of \(3\) so they might be
outliers, in keeping with the leverage plot.</p>

<p><strong>(e)</strong> Interaction effects</p>

<p><strong>We recall</strong> that <code>x*y</code> corresponds to <code>x+y+x:y</code></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># View the correlation matrix</span>
cleanAuto <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">select</span>(<span style="color:#f92672">-</span>name,<span style="color:#f92672">-</span>origin) <span style="color:#f92672">%&gt;%</span> cor</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##                     mpg  cylinders displacement horsepower     weight
## mpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442
## cylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273
## displacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944
## horsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377
## weight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000
## acceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392
## year          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199
##              acceleration       year
## mpg             0.4233285  0.5805410
## cylinders      -0.5046834 -0.3456474
## displacement   -0.5438005 -0.3698552
## horsepower     -0.6891955 -0.4163615
## weight         -0.4168392 -0.3091199
## acceleration    1.0000000  0.2903161
## year            0.2903161  1.0000000</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(<span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>weight<span style="color:#f92672">*</span>displacement<span style="color:#f92672">*</span>year,data<span style="color:#f92672">=</span>noFactors<span style="color:#a6e22e">[</span>(<span style="color:#ae81ff">10</span><span style="color:#f92672">:</span><span style="color:#ae81ff">85</span>),]))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = mpg ~ weight * displacement * year, data = noFactors[(10:85),
##     ])
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -5.3020 -0.9055  0.0966  0.8912  3.7049
##
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)               3.961e+02  2.578e+02   1.537    0.129
## weight                   -1.030e-01  1.008e-01  -1.021    0.311
## displacement             -1.587e+00  1.308e+00  -1.213    0.229
## year                     -4.889e+00  3.623e+00  -1.349    0.182
## weight:displacement       3.926e-04  3.734e-04   1.051    0.297
## weight:year               1.317e-03  1.418e-03   0.929    0.356
## displacement:year         2.150e-02  1.846e-02   1.165    0.248
## weight:displacement:year -5.287e-06  5.253e-06  -1.007    0.318
##
## Residual standard error: 1.8 on 68 degrees of freedom
## Multiple R-squared:  0.922,  Adjusted R-squared:  0.914
## F-statistic: 114.9 on 7 and 68 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(<span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>weight<span style="color:#f92672">*</span>displacement<span style="color:#f92672">*</span>year,data<span style="color:#f92672">=</span>noFactors))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = mpg ~ weight * displacement * year, data = noFactors)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -9.6093 -1.6472 -0.0531  1.2289 14.5604
##
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)              -8.437e+01  3.128e+01  -2.697   0.0073 **
## weight                    8.489e-03  1.322e-02   0.642   0.5212
## displacement              3.434e-01  1.969e-01   1.744   0.0820 .
## year                      1.828e+00  4.127e-01   4.430 1.23e-05 ***
## weight:displacement      -6.589e-05  5.055e-05  -1.303   0.1932
## weight:year              -2.433e-04  1.744e-04  -1.395   0.1638
## displacement:year        -5.566e-03  2.674e-03  -2.082   0.0380 *
## weight:displacement:year  1.144e-06  6.823e-07   1.677   0.0944 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 2.951 on 384 degrees of freedom
## Multiple R-squared:  0.8596, Adjusted R-squared:  0.8571
## F-statistic:   336 on 7 and 384 DF,  p-value: &lt; 2.2e-16</code></pre></div>
<ul>
<li>Adding the interaction effects of the \(3\) most positive R value terms
improves the existing prediction to be better than that obtained by
considering all effects.</li>
<li>We note that the best model is obtained by removing the range
identified in chapter 2.</li>
</ul>

<p><strong>(f)</strong> Nonlinear transformations</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(<span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span>weight<span style="color:#f92672">*</span>displacement<span style="color:#f92672">*</span>year<span style="color:#f92672">+</span><span style="color:#a6e22e">I</span>(year^2),data<span style="color:#f92672">=</span>noFactors<span style="color:#a6e22e">[</span>(<span style="color:#ae81ff">10</span><span style="color:#f92672">:</span><span style="color:#ae81ff">85</span>),]))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = mpg ~ weight * displacement * year + I(year^2),
##     data = noFactors[(10:85), ])
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -5.1815 -0.8235  0.0144  1.0076  3.9420
##
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)              -4.205e+03  1.810e+03  -2.324   0.0232 *
## weight                   -8.800e-02  9.709e-02  -0.906   0.3680
## displacement             -1.030e+00  1.276e+00  -0.807   0.4225
## year                      1.238e+02  5.026e+01   2.464   0.0163 *
## I(year^2)                -9.000e-01  3.506e-01  -2.567   0.0125 *
## weight:displacement       2.471e-04  3.634e-04   0.680   0.4988
## weight:year               1.113e-03  1.365e-03   0.815   0.4177
## displacement:year         1.368e-02  1.800e-02   0.760   0.4501
## weight:displacement:year -3.254e-06  5.111e-06  -0.637   0.5264
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 1.73 on 67 degrees of freedom
## Multiple R-squared:  0.929,  Adjusted R-squared:  0.9205
## F-statistic: 109.6 on 8 and 67 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(<span style="color:#a6e22e">lm</span>(mpg<span style="color:#f92672">~</span><span style="color:#a6e22e">.-I</span>(<span style="color:#a6e22e">log</span>(acceleration^2)),data<span style="color:#f92672">=</span>noFactors<span style="color:#a6e22e">[</span>(<span style="color:#ae81ff">10</span><span style="color:#f92672">:</span><span style="color:#ae81ff">85</span>),]))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = mpg ~ . - I(log(acceleration^2)), data = noFactors[(10:85),
##     ])
##
## Residuals:
##    Min     1Q Median     3Q    Max
## -6.232 -1.470 -0.211  1.075  7.088
##
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  41.3787633 24.1208720   1.715   0.0907 .
## cylinders     0.0863161  0.6112822   0.141   0.8881
## displacement -0.0148491  0.0103249  -1.438   0.1549
## horsepower   -0.0158500  0.0151259  -1.048   0.2984
## weight       -0.0039125  0.0008546  -4.578 2.02e-05 ***
## acceleration -0.1473786  0.1438220  -1.025   0.3091
## year         -0.0378187  0.3380266  -0.112   0.9112
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 2.262 on 69 degrees of freedom
## Multiple R-squared:  0.8751, Adjusted R-squared:  0.8642
## F-statistic: 80.55 on 6 and 69 DF,  p-value: &lt; 2.2e-16</code></pre></div>
<ul>
<li><p>The best model I found was still the one without the non-linear
transformation but with removed outliers and additional interaction
effects of <code>displacement</code>,=year= and <code>weight</code></p></li>

<li><p>A popular approach is to use a <code>log</code> transform for both the inputs and
the outputs</p></li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(<span style="color:#a6e22e">lm</span>(<span style="color:#a6e22e">log</span>(mpg)<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>noFactors<span style="color:#a6e22e">[</span>(<span style="color:#ae81ff">10</span><span style="color:#f92672">:</span><span style="color:#ae81ff">85</span>),]))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = log(mpg) ~ ., data = noFactors[(10:85), ])
##
## Residuals:
##       Min        1Q    Median        3Q       Max
## -0.285805 -0.052358 -0.001456  0.066521  0.209739
##
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   3.886e+00  1.028e+00   3.781 0.000328 ***
## cylinders    -1.771e-02  2.604e-02  -0.680 0.498669
## displacement -1.540e-04  4.399e-04  -0.350 0.727314
## horsepower   -2.343e-03  6.444e-04  -3.636 0.000529 ***
## weight       -1.960e-04  3.641e-05  -5.383 9.51e-07 ***
## acceleration -1.525e-02  6.128e-03  -2.489 0.015224 *
## year          4.138e-03  1.440e-02   0.287 0.774703
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 0.09636 on 69 degrees of freedom
## Multiple R-squared:  0.919,  Adjusted R-squared:  0.912
## F-statistic: 130.5 on 6 and 69 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(<span style="color:#a6e22e">lm</span>(<span style="color:#a6e22e">log</span>(mpg)<span style="color:#f92672">~</span><span style="color:#a6e22e">log</span>(weight<span style="color:#f92672">*</span>displacement<span style="color:#f92672">*</span>year),data<span style="color:#f92672">=</span>noFactors<span style="color:#a6e22e">[</span>(<span style="color:#ae81ff">10</span><span style="color:#f92672">:</span><span style="color:#ae81ff">85</span>),]))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = log(mpg) ~ log(weight * displacement * year), data = noFactors[(10:85),
##     ])
##
## Residuals:
##      Min       1Q   Median       3Q      Max
## -0.41121 -0.04107  0.01266  0.07791  0.21056
##
## Coefficients:
##                                   Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                        8.91995    0.26467   33.70   &lt;2e-16 ***
## log(weight * displacement * year) -0.34250    0.01508  -22.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 0.1158 on 74 degrees of freedom
## Multiple R-squared:  0.8745, Adjusted R-squared:  0.8728
## F-statistic: 515.6 on 1 and 74 DF,  p-value: &lt; 2.2e-16</code></pre></div>
<h3 id="question-3.10---page-123">Question 3.10 - Page 123</h3>

<p>This question should be answered using the Carseats data set.</p>

<p><strong>(a)</strong> Fit a multiple regression model to predict <code>Sales</code> using <code>Price</code>,
<code>Urban</code>, and <code>US</code>.</p>

<p><strong>(b)</strong> Provide an interpretation of each coefficient in the model. Be
careful&mdash;some of the variables in the model are qualitative!</p>

<p><strong>&copy;</strong> Write out the model in equation form, being careful to handle the
qualitative variables properly.</p>

<p><strong>(d)</strong> For which of the predictors can you reject the null hypothesis
\(H_0:\beta_j=0\)?</p>

<p><strong>(e)</strong> On the basis of your response to the previous question, ﬁt a
smaller model that only uses the predictors for which there is evidence
of association with the outcome.</p>

<p><strong>(f)</strong> How well do the models in (a) and (e) fit the data?</p>

<p><strong>(g)</strong> Using the model from (e), obtain \(95%\) confidence intervals for
the coefficient(s).</p>

<p><strong>(h)</strong> Is there evidence of outliers or high leverage observations in the
model from (e)?</p>

<h4 id="answer-3">Answer</h4>

<p>Load the dataset (and clean it)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">cleanCarSeats <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">na.omit</span>(ISLR<span style="color:#f92672">::</span>Carseats)</code></pre></div>
<p>Obtain summary statistics</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">cleanCarSeats <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(unique) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">sapply</span>(length)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##       Sales   CompPrice      Income Advertising  Population       Price
##         336          73          98          28         275         101
##   ShelveLoc         Age   Education       Urban          US
##           3          56           9           2           2</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">str</span>(cleanCarSeats)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## &#39;data.frame&#39;:    400 obs. of  11 variables:
##  $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...
##  $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...
##  $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...
##  $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...
##  $ Population : num  276 260 269 466 340 501 45 425 108 131 ...
##  $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...
##  $ ShelveLoc  : Factor w/ 3 levels &#34;Bad&#34;,&#34;Good&#34;,&#34;Medium&#34;: 1 2 3 3 1 1 3 2 3 3 ...
##  $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...
##  $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...
##  $ Urban      : Factor w/ 2 levels &#34;No&#34;,&#34;Yes&#34;: 2 2 2 2 2 1 2 2 1 1 ...
##  $ US         : Factor w/ 2 levels &#34;No&#34;,&#34;Yes&#34;: 2 2 2 2 1 2 1 2 1 2 ...</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(cleanCarSeats)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##      Sales          CompPrice       Income        Advertising
##  Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000
##  1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000
##  Median : 7.490   Median :125   Median : 69.00   Median : 5.000
##  Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635
##  3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000
##  Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000
##    Population        Price        ShelveLoc        Age          Education
##  Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0
##  1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0
##  Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0
##  Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9
##  3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0
##  Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0
##  Urban       US
##  No :118   No :142
##  Yes:282   Yes:258
##
##
##
##</code></pre></div>
<p>We can see that:</p>

<ul>
<li><strong>Urban</strong>, <strong>US</strong> and <strong>ShelveLoc</strong> are factors with 2,2 and 3 levels
respectively</li>
<li>Education has only 9 unique values so we might as well consider it to
be a factor too if we need to</li>
</ul>

<p><strong>(a)</strong> Multiple Regression Model</p>

<p>Fit it to things</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(<span style="color:#a6e22e">lm</span>(Sales<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>cleanCarSeats))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = Sales ~ ., data = cleanCarSeats)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -2.8692 -0.6908  0.0211  0.6636  3.4115
##
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)      5.6606231  0.6034487   9.380  &lt; 2e-16 ***
## CompPrice        0.0928153  0.0041477  22.378  &lt; 2e-16 ***
## Income           0.0158028  0.0018451   8.565 2.58e-16 ***
## Advertising      0.1230951  0.0111237  11.066  &lt; 2e-16 ***
## Population       0.0002079  0.0003705   0.561    0.575
## Price           -0.0953579  0.0026711 -35.700  &lt; 2e-16 ***
## ShelveLocGood    4.8501827  0.1531100  31.678  &lt; 2e-16 ***
## ShelveLocMedium  1.9567148  0.1261056  15.516  &lt; 2e-16 ***
## Age             -0.0460452  0.0031817 -14.472  &lt; 2e-16 ***
## Education       -0.0211018  0.0197205  -1.070    0.285
## UrbanYes         0.1228864  0.1129761   1.088    0.277
## USYes           -0.1840928  0.1498423  -1.229    0.220
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 1.019 on 388 degrees of freedom
## Multiple R-squared:  0.8734, Adjusted R-squared:  0.8698
## F-statistic: 243.4 on 11 and 388 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(<span style="color:#a6e22e">lm</span>(Sales<span style="color:#f92672">~</span>US<span style="color:#f92672">*</span>Price<span style="color:#f92672">*</span>Urban,data<span style="color:#f92672">=</span>cleanCarSeats))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = Sales ~ US * Price * Urban, data = cleanCarSeats)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -6.7952 -1.6659 -0.0984  1.6119  7.2433
##
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)          13.456350   1.727210   7.791 6.03e-14 ***
## USYes                 2.049051   2.322591   0.882    0.378
## Price                -0.061657   0.014875  -4.145 4.17e-05 ***
## UrbanYes             -0.651545   2.071401  -0.315    0.753
## USYes:Price          -0.001567   0.019972  -0.078    0.937
## USYes:UrbanYes       -1.122034   2.759662  -0.407    0.685
## Price:UrbanYes        0.010793   0.017796   0.606    0.545
## USYes:Price:UrbanYes  0.001288   0.023619   0.055    0.957
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 2.473 on 392 degrees of freedom
## Multiple R-squared:  0.2467, Adjusted R-squared:  0.2333
## F-statistic: 18.34 on 7 and 392 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(<span style="color:#a6e22e">lm</span>(Sales<span style="color:#f92672">~</span>US<span style="color:#f92672">+</span>Price<span style="color:#f92672">+</span>Urban,data<span style="color:#f92672">=</span>cleanCarSeats))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = Sales ~ US + Price + Urban, data = cleanCarSeats)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -6.9206 -1.6220 -0.0564  1.5786  7.0581
##
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 13.043469   0.651012  20.036  &lt; 2e-16 ***
## USYes        1.200573   0.259042   4.635 4.86e-06 ***
## Price       -0.054459   0.005242 -10.389  &lt; 2e-16 ***
## UrbanYes    -0.021916   0.271650  -0.081    0.936
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 2.472 on 396 degrees of freedom
## Multiple R-squared:  0.2393, Adjusted R-squared:  0.2335
## F-statistic: 41.52 on 3 and 396 DF,  p-value: &lt; 2.2e-16</code></pre></div>
<p><strong>(b)</strong> Interpret stuff</p>

<p>To interpret the data, we need to determine which of the models fits the
data best, we will use <code>anova()</code> to test this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lmCarSAll<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">lm</span>(Sales<span style="color:#f92672">~</span>.,data<span style="color:#f92672">=</span>cleanCarSeats)
lmCarStimesPUU<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">lm</span>(Sales<span style="color:#f92672">~</span>US<span style="color:#f92672">*</span>Price<span style="color:#f92672">*</span>Urban,data<span style="color:#f92672">=</span>cleanCarSeats)
lmCarSplusPUU<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">lm</span>(Sales<span style="color:#f92672">~</span>US<span style="color:#f92672">+</span>Price<span style="color:#f92672">+</span>Urban,data<span style="color:#f92672">=</span>cleanCarSeats)
<span style="color:#a6e22e">anova</span>(lmCarSAll,lmCarStimesPUU,lmCarSplusPUU)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Analysis of Variance Table
##
## Model 1: Sales ~ CompPrice + Income + Advertising + Population + Price +
##     ShelveLoc + Age + Education + Urban + US
## Model 2: Sales ~ US * Price * Urban
## Model 3: Sales ~ US + Price + Urban
##   Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)
## 1    388  402.83
## 2    392 2397.10 -4  -1994.27 480.2082 &lt; 2.2e-16 ***
## 3    396 2420.83 -4    -23.73   5.7149 0.0001772 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">anova</span>(lmCarStimesPUU,lmCarSplusPUU)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Analysis of Variance Table
##
## Model 1: Sales ~ US * Price * Urban
## Model 2: Sales ~ US + Price + Urban
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1    392 2397.1
## 2    396 2420.8 -4   -23.734 0.9703 0.4236</code></pre></div>
<p><strong>Remember</strong> that it is not possible to use <code>anova()</code> unless the same
variables are present in all the models being tested, so it is
meaningless to use <code>anova</code> for <code>lmCarSAll</code> along with the others,
because we can&rsquo;t change the interaction model to get only the main
effects.</p>

<ul>
<li>We note that due to the low value of the F-statistic and the non-zero
value of the p-value we cannot disregard the null hypothesis, or in
other words, the models are basically the same in terms of their
performance.</li>
</ul>

<p>This means that I would like to continue with the simpler model, since
the increase in R squared is too small to account for dealing with the
additional factors.</p>

<ul>
<li>We see immediately, that there is a positive correlation only with
being in the US</li>
<li>Increases in price and being in an urban area actually decrease the
sales, which is not surprising since being in the an urban area is
probably correlated to a higher price, which we can check immediately</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(<span style="color:#a6e22e">lm</span>(Price<span style="color:#f92672">~</span>Urban,data<span style="color:#f92672">=</span>cleanCarSeats))</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = Price ~ Urban, data = cleanCarSeats)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -92.514 -15.514   1.205  14.595  74.486
##
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  114.076      2.180  52.330   &lt;2e-16 ***
## UrbanYes       2.438      2.596   0.939    0.348
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 23.68 on 398 degrees of freedom
## Multiple R-squared:  0.002211,   Adjusted R-squared:  -0.0002965
## F-statistic: 0.8817 on 1 and 398 DF,  p-value: 0.3483</code></pre></div>
<p>We see that our assumption is validated. Being in an urban area has a
low t-statistic for a positive increase on the slope</p>

<ul>
<li>Returning to our previous model, we note that there is a high value of
the p-value of the t-statistic for <code>Urban</code> being true, this means
there isn&rsquo;t a real relationship between being in an urban area and the
sales. This makes intuitive sense as well</li>
</ul>

<p><strong>note</strong> <code>t-test</code> is essentially a linear model with one variable, that
is, if we want to find out if there is a relation between having a store
in an urban area, we could sum all the urban yes and divide by the
number of observations and compare that to the sum of all the urban no
divided by the number of observations which is essentially the <code>t-test</code>
again.</p>

<ul>
<li><code>Price</code> is significant, and has an inverse relation with the sales, so
we should keep that in mind</li>
</ul>

<p><strong>&copy;</strong> In Equation Form:</p>

<p>\[ Sales=1.200573*USYes - 0.054459*Price - 0.021916*UrbanYes + 13.043469 \]</p>

<p><strong>(e)</strong> Other models</p>

<ul>
<li>We know from our case-study on testing the full multiple linear
regression for <code>Sales</code> that there are definitely more important
variables being ignored. However, we also know that <code>Urban</code> is not
significant, so we can use a smaller model.</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R">lmCarSplusPU<span style="color:#f92672">&lt;-</span><span style="color:#a6e22e">lm</span>(Sales<span style="color:#f92672">~</span>US<span style="color:#f92672">+</span>Price, data<span style="color:#f92672">=</span>cleanCarSeats)</code></pre></div>
<p><strong>(f)</strong> Comparison of models</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">summary</span>(lmCarSplusPU)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##
## Call:
## lm(formula = Sales ~ US + Price, data = cleanCarSeats)
##
## Residuals:
##     Min      1Q  Median      3Q     Max
## -6.9269 -1.6286 -0.0574  1.5766  7.0515
##
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 13.03079    0.63098  20.652  &lt; 2e-16 ***
## USYes        1.19964    0.25846   4.641 4.71e-06 ***
## Price       -0.05448    0.00523 -10.416  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##
## Residual standard error: 2.469 on 397 degrees of freedom
## Multiple R-squared:  0.2393, Adjusted R-squared:  0.2354
## F-statistic: 62.43 on 2 and 397 DF,  p-value: &lt; 2.2e-16</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">anova</span>(lmCarSplusPUU,lmCarSplusPU)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">## Analysis of Variance Table
##
## Model 1: Sales ~ US + Price + Urban
## Model 2: Sales ~ US + Price
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1    396 2420.8
## 2    397 2420.9 -1  -0.03979 0.0065 0.9357</code></pre></div>
<p>As expected, the low value of the F statistic and the high p-value for
the <code>anova()</code> test asserts that the null hypothesis cannot be neglected,
thus there are no differences between the model with the insignificant
parameter, which is also seen in the <code>R</code> squared value, which is the
same for both models</p>

<p><strong>(g)</strong> Confidence Intervals</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">confint</span>(lmCarSplusPU)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##                   2.5 %      97.5 %
## (Intercept) 11.79032020 14.27126531
## USYes        0.69151957  1.70776632
## Price       -0.06475984 -0.04419543</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">confint</span>(lmCarSplusPUU)</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">##                   2.5 %      97.5 %
## (Intercept) 11.76359670 14.32334118
## USYes        0.69130419  1.70984121
## Price       -0.06476419 -0.04415351
## UrbanYes    -0.55597316  0.51214085</code></pre></div>
<ul>
<li>☐ Look into trying to plot this with <code>ggplot</code></li>
</ul>

<p><strong>(h)</strong> Outliers</p>

<ul>
<li>We will first check the leverage plots</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#a6e22e">par</span>(mfrow<span style="color:#f92672">=</span><span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
<span style="color:#a6e22e">plot</span>(lmCarSplusPU)</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-41-1.png"   />

        
            <figcaption class="center" >Figure 12: Leverage Plots</figcaption>
        
    </figure>



<p>We can see there is a point with high leverage, but it has a low
residual. In any case we should check further.</p>

<ul>
<li>Now we will check the studentized residuals to see if they are greater
than 3</li>
</ul>

<!--listend-->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-R" data-lang="R"><span style="color:#75715e"># See residuals</span>
<span style="color:#a6e22e">plot</span>(xlab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Prediction&#34;</span>,ylab<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Studentized Residual&#34;</span>,x<span style="color:#f92672">=</span><span style="color:#a6e22e">predict</span>(lmCarSplusPU),y<span style="color:#f92672">=</span><span style="color:#a6e22e">rstudent</span>(lmCarSplusPU))</code></pre></div>

    <figure class="left" >
        <img src="/islr/sol01/unnamed-chunk-42-1.png"   />

        
            <figcaption class="center" >Figure 13: Studentized residuals</figcaption>
        
    </figure>



<p>Thus I would say there are no outliers in our dataset, as none of our
datapoints have an absolute studentized residual above 3.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1">James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science &amp; Business Media.
 <a class="footnote-return" href="#fnref:fn-1"><sup>[return]</sup></a></li>
</ol>
</div>
]]></content>
        </item>
        
    </channel>
</rss>
