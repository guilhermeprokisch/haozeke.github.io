[{"categories":["programming"],"contents":"  A short post on my current set-up for R with nixpkgs and emacs to auto-compile my system configuration.\n Background This is my third post on working with nixpkgs and R.\n Part I covered ways of working effectively with R and nixpkgs Part II dealt with composing dependent devtools packages in a per-package environment, with a focus on rethinking and tidybayes.rethinking  This final part is about automating the system-wide configuration using emacs. Specifically doom-emacs. Naturally, this is the most optimal way to work with nix packages as well.\nSystem Configuration After experimenting with a per-project layout, I decided to use the full system configuration instead of the per-project layout. So I simply set:\n# $HOME/.config/nixpkgs/config.nix { packageOverrides = super: let self = super.pkgs; rethinking = with self.rPackages; buildRPackage { name = \u0026#34;rethinking\u0026#34;; src = self.fetchFromGitHub { owner = \u0026#34;rmcelreath\u0026#34;; repo = \u0026#34;rethinking\u0026#34;; rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; }; propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; }; tidybayes_rethinking = with self.rPackages; buildRPackage { name = \u0026#34;tidybayes.rethinking\u0026#34;; src = self.fetchFromGitHub { owner = \u0026#34;mjskay\u0026#34;; repo = \u0026#34;tidybayes.rethinking\u0026#34;; rev = \u0026#34;df903c88f4f4320795a47c616eef24a690b433a4\u0026#34;; sha256 = \u0026#34;1jl3189zdddmwm07z1mk58hcahirqrwx211ms0i1rzbx5y4zak0c\u0026#34;; }; propagatedBuildInputs = [ dplyr tibble rlang MASS tidybayes rethinking rstan ]; }; in { rEnv = super.rWrapper.override { packages = with self.rPackages; [ tidyverse devtools modelr purrr forcats #################### # Machine Learning # #################### # MLR3 mlr3 mlr3viz mlr3learners mlr3pipelines # Plotting tools ggplot2 cowplot ggrepel RColorBrewer # Stan Stuff rstan tidybayes # Text Utilities orgutils latex2exp kableExtra knitr data_table printr # Devtools Stuff rethinking tidybayes_rethinking ]; }; }; } If any of these look strange, refer to the earlier posts.\nAutomation Pains direnv, lorri and niv (the heroes of Part II) are not really useful for working with the system-wide configuration, but an elegant solution still exists, which leverages firestarter and after-save-hooks in emacs.\nFirestarter Firestarter is my favorite method of working with shell commands after saving things. My setup is simply:\n; packages.el (package! firestarter) This is coupled with a simple configuration.\n; config.el (use-package! firestarter :ensure t :init (firestarter-mode) :config (setq firestarter-default-type t) ) The default type corresponds to demanding the shell outupt for the commands.\nNix-R Stuff To finalize this setup, we will need to modify our system configuration slightly. For brevity, we simply note the following local variables.\n# $HOME/.config/nixpkgs/config.nix # Local Variables: # firestarter: \u0026#34;nix-env -f \u0026#39;\u0026lt;nixpkgs\u0026gt;\u0026#39; -iA rEnv\u0026#34; # firestarter-default-type: (quote failure) # End: The firestarter-default-type used here is to ensure that errors are displayed in a buffer.\nTo check what is being installed (if anything) simply run:\nnix-env -f \u0026#34;\u0026lt;nixpkgs\u0026gt;\u0026#34; -iA rEnv --dry-run Conclusion This is my current setup. It works out better than most of my other attempts and seems to be an optimal approach. The packages are versioned, everything is automated, and I can reproduce changes across all my machines. Will stick with this.\n","permalink":"https://rgoswami.me/posts/emacs-nix-r/","tags":["tools","nix","workflow","R","emacs"],"title":"Emacs for Nix-R"},{"categories":["programming"],"contents":"  This post describes how to set up a transparent automated setup for reproducible R workflows using nixpkgs, niv, and lorri. The explanatory example used throughout the post is one of setting up the rethinking package and running some examples from the excellent second edition of \u0026ldquo;Statistical Rethinking\u0026rdquo; by Richard McElreath.\n Background As detailed in an earlier post1, I had set up Nix to work with non-CRAN packages. If the rest of this section is unclear, please refer back to the earlier post.\nSetup For the remainder of the post, we will set up a basic project structure:\nmkdir tryRnix/ Now we will create a shell.nix as2:\n# shell.nix { pkgs ? import \u0026lt;nixpkgs\u0026gt; { } }: with pkgs; let my-r-pkgs = rWrapper.override { packages = with rPackages; [ ggplot2 tidyverse tidybayes tidybayes.rethinking (buildRPackage { name = \u0026#34;rethinking\u0026#34;; src = fetchFromGitHub { owner = \u0026#34;rmcelreath\u0026#34;; repo = \u0026#34;rethinking\u0026#34;; rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; }; propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; }) ]; }; in mkShell { buildInputs = with pkgs; [ git glibcLocales openssl which openssh curl wget ]; inputsFrom = [ my-r-pkgs ]; shellHook = \u0026#39;\u0026#39; mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; \u0026#39;\u0026#39;; GIT_SSL_CAINFO = \u0026#34;${cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux \u0026#34;${glibcLocales}/lib/locale/locale-archive\u0026#34;; } So we have:\ntree tryRnix    tryRnix        └── shell.nix     0 directories, 1 file    Introspection At this point:\n I was able to install packages (system and R) arbitrarily I was able to use project specific folders Unlike npm, pipenv, poetry, conda and friends, my system was not bloated by downloading and setting up the same packages every-time I used them in different projects  However, though this is a major step up from being chained to RStudio and my system package manager, it is still perhaps not immediately obvious how this workflow is reproducible. Admittedly, I have defined my packages in a nice functional manner; but someone else might have a different upstream channel they are tracking, and thus will have different packages. Indeed the only packages which I could be sure of were the R packages I built from Github, since those were tied to a hash. Finally, the setup described for each project is pretty onerous, and it is not immediately clear how to leverage fantastic tools like direnv for working through this.\nTowards Reproducible Environments The astute reader will have noticed that I mentioned that the R packages were reproducible since they were tied to a hash, and might reasonable argue that the entire Nix ecosystem is about hashing in the first place. Once we realize that, the rest is relatively simple3.\nNiv and Pinning Niv essentially keeps track of the channel from which all the packages are installed. Setup is pretty minimal.\ncd tryRnix/ nix-env -i niv niv init At this point, we have:\ntree tryRnix    tryRnix        ├── nix     │  ├── sources.json    │  └── sources.nix    └── shell.nix           1 directory, 3 files    We will have to update our shell.nix to use the new sources.\nlet sources = import ./nix/sources.nix; pkgs = import sources.nixpkgs { }; stdenv = pkgs.stdenv; my-r-pkgs = pkgs.rWrapper.override { packages = with pkgs.rPackages; [ ggplot2 tidyverse tidybayes ]; }; in pkgs.mkShell { buildInputs = with pkgs;[ git glibcLocales openssl which openssh curl wget my-r-pkgs ]; shellHook = \u0026#39;\u0026#39; mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; \u0026#39;\u0026#39;; GIT_SSL_CAINFO = \u0026#34;${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux \u0026#34;${pkgs.glibcLocales}/lib/locale/locale-archive\u0026#34;; } We could inspect and edit these sources by hand, but it is much more convenient to simply use niv again when we need to update these.\ncd tryRnix/ niv update nixpkgs -b nixpkgs-unstable At this stage we have a reproducible set of packages ready to use. However it is still pretty annoying to have to go through the trouble of writing nix-shell and also waiting while it rebuilds when we change things.\nLorri and Direnv In the past, I have made my admiration for direnv very clear (especially for python-poetry). However, though direnv does allow us to include arbitrary bash logic into our projects, it would be nice to have something which has some defaults for nix. Thankfully, the folks at TweagIO developed lorri to scratch that itch.\nThe basic setup is simple:\nnix-env -i lorri cd tryRnix/ lorri inittree -a tryRnix/    tryRnix/        ├── .envrc     ├── nix     │  ├── sources.json    │  └── sources.nix    └── shell.nix           1 directory, 4 files    We can and should inspect the environment lorri wants us to load with direnv file:\ncat tryRnix/.envrc$(lorri direnv) In and of itself that is not too descriptive, so we should run that on our own first.\nEVALUATION_ROOT=\u0026#34;$HOME/.cache/lorri/gc_roots/407bd4df60fbda6e3a656c39f81c03c2/gc_root/shell_gc_root\u0026#34; watch_file \u0026#34;/run/user/1000/lorri/daemon.socket\u0026#34; watch_file \u0026#34;$EVALUATION_ROOT\u0026#34; #!/usr/bin/env bash # ^ shebang is unused as this file is sourced, but present for editor # integration. Note: Direnv guarantees it *will* be parsed using bash. function punt () { : } # move \u0026#34;origPreHook\u0026#34; \u0026#34;preHook\u0026#34; \u0026#34;$@\u0026#34;;; move() { srcvarname=$1 # example: varname might contain the string \u0026#34;origPATH\u0026#34; # drop off the source variable name shift destvarname=$1 # example: destvarname might contain the string \u0026#34;PATH\u0026#34; # drop off the destination variable name shift # like: export origPATH=\u0026#34;...some-value...\u0026#34; export \u0026#34;${@?}\u0026#34;; # set $original to the contents of the variable $srcvarname # refers to eval \u0026#34;$destvarname=\\\u0026#34;${!srcvarname}\\\u0026#34;\u0026#34; # mark the destvarname as exported so direnv picks it up # (shellcheck: we do want to export the content of destvarname!) # shellcheck disable=SC2163 export \u0026#34;$destvarname\u0026#34; # remove the export from above, ie: export origPATH... unset \u0026#34;$srcvarname\u0026#34; } function prepend() { varname=$1 # example: varname might contain the string \u0026#34;PATH\u0026#34; # drop off the varname shift separator=$1 # example: separator would usually be the string \u0026#34;:\u0026#34; # drop off the separator argument, so the remaining arguments # are the arguments to export shift # set $original to the contents of the the variable $varname # refers to original=\u0026#34;${!varname}\u0026#34; # effectfully accept the new variable\u0026#39;s contents export \u0026#34;${@?}\u0026#34;; # re-set $varname\u0026#39;s variable to the contents of varname\u0026#39;s # reference, plus the current (updated on the export) contents. # however, exclude the ${separator} unless ${original} starts # with a value eval \u0026#34;$varname=${!varname}${original:+${separator}${original}}\u0026#34; } function append() { varname=$1 # example: varname might contain the string \u0026#34;PATH\u0026#34; # drop off the varname shift separator=$1 # example: separator would usually be the string \u0026#34;:\u0026#34; # drop off the separator argument, so the remaining arguments # are the arguments to export shift # set $original to the contents of the the variable $varname # refers to original=\u0026#34;${!varname:-}\u0026#34; # effectfully accept the new variable\u0026#39;s contents export \u0026#34;${@?}\u0026#34;; # re-set $varname\u0026#39;s variable to the contents of varname\u0026#39;s # reference, plus the current (updated on the export) contents. # however, exclude the ${separator} unless ${original} starts # with a value eval \u0026#34;$varname=${original:+${original}${separator}}${!varname}\u0026#34; } varmap() { if [ -f \u0026#34;$EVALUATION_ROOT/varmap-v1\u0026#34; ]; then # Capture the name of the variable being set IFS=\u0026#34;=\u0026#34; read -r -a cur_varname \u0026lt;\u0026lt;\u0026lt; \u0026#34;$1\u0026#34; # With IFS=\u0026#39;\u0026#39; and the `read` delimiter being \u0026#39;\u0026#39;, we achieve # splitting on \\0 bytes while also preserving leading # whitespace: # # bash-3.2$ printf \u0026#39; \u0026lt;- leading space\\0bar\\0baz\\0\u0026#39; \\ # | (while IFS=\u0026#39;\u0026#39; read -d $\u0026#39;\\0\u0026#39; -r x; do echo \u0026#34;\u0026gt;$x\u0026lt;\u0026#34;; done) # \u0026gt; \u0026lt;- leading space\u0026lt; # \u0026gt;bar\u0026lt; # \u0026gt;baz\u0026lt;``` while IFS=\u0026#39;\u0026#39; read -r -d \u0026#39;\u0026#39; map_instruction \\  \u0026amp;\u0026amp; IFS=\u0026#39;\u0026#39; read -r -d \u0026#39;\u0026#39; map_variable \\  \u0026amp;\u0026amp; IFS=\u0026#39;\u0026#39; read -r -d \u0026#39;\u0026#39; map_separator; do unset IFS if [ \u0026#34;$map_variable\u0026#34; == \u0026#34;${cur_varname[0]}\u0026#34; ]; then if [ \u0026#34;$map_instruction\u0026#34; == \u0026#34;append\u0026#34; ]; then append \u0026#34;$map_variable\u0026#34; \u0026#34;$map_separator\u0026#34; \u0026#34;$@\u0026#34; return fi fi done \u0026lt; \u0026#34;$EVALUATION_ROOT/varmap-v1\u0026#34; fi export \u0026#34;${@?}\u0026#34; } function declare() { if [ \u0026#34;$1\u0026#34; == \u0026#34;-x\u0026#34; ]; then shift; fi # Some variables require special handling. # # - punt: don\u0026#39;t set the variable at all # - prepend: take the new value, and put it before the current value. case \u0026#34;$1\u0026#34; in # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L100 \u0026#34;HOME=\u0026#34;*) punt;; \u0026#34;USER=\u0026#34;*) punt;; \u0026#34;LOGNAME=\u0026#34;*) punt;; \u0026#34;DISPLAY=\u0026#34;*) punt;; \u0026#34;PATH=\u0026#34;*) prepend \u0026#34;PATH\u0026#34; \u0026#34;:\u0026#34; \u0026#34;$@\u0026#34;;; \u0026#34;TERM=\u0026#34;*) punt;; \u0026#34;IN_NIX_SHELL=\u0026#34;*) punt;; \u0026#34;TZ=\u0026#34;*) punt;; \u0026#34;PAGER=\u0026#34;*) punt;; \u0026#34;NIX_BUILD_SHELL=\u0026#34;*) punt;; \u0026#34;SHLVL=\u0026#34;*) punt;; # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L385 \u0026#34;TEMPDIR=\u0026#34;*) punt;; \u0026#34;TMPDIR=\u0026#34;*) punt;; \u0026#34;TEMP=\u0026#34;*) punt;; \u0026#34;TMP=\u0026#34;*) punt;; # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L421 \u0026#34;NIX_ENFORCE_PURITY=\u0026#34;*) punt;; # vars from: https://www.gnu.org/software/bash/manual/html_node/Bash-Variables.html (last checked: 2019-09-26) # reported in https://github.com/target/lorri/issues/153 \u0026#34;OLDPWD=\u0026#34;*) punt;; \u0026#34;PWD=\u0026#34;*) punt;; \u0026#34;SHELL=\u0026#34;*) punt;; # https://github.com/target/lorri/issues/97 \u0026#34;preHook=\u0026#34;*) punt;; \u0026#34;origPreHook=\u0026#34;*) move \u0026#34;origPreHook\u0026#34; \u0026#34;preHook\u0026#34; \u0026#34;$@\u0026#34;;; *) varmap \u0026#34;$@\u0026#34; ;; esac } export IN_NIX_SHELL=impure if [ -f \u0026#34;$EVALUATION_ROOT/bash-export\u0026#34; ]; then # shellcheck disable=SC1090 . \u0026#34;$EVALUATION_ROOT/bash-export\u0026#34; elif [ -f \u0026#34;$EVALUATION_ROOT\u0026#34; ]; then # shellcheck disable=SC1090 . \u0026#34;$EVALUATION_ROOT\u0026#34; fi unset declare Jun 06 19:02:32.368 INFO lorri has not completed an evaluation for this project yet, expr: $HOME/Git/Github/WebDev/Mine/haozeke.github.io/content-org/tryRnix/shell.nix Jun 06 19:02:32.368 WARN `lorri direnv` should be executed by direnv from within an `.envrc` file, expr: $HOME/Git/Github/WebDev/Mine/haozeke.github.io/content-org/tryRnix/shell.nix Upon inspection, that seems to check out. So now we can enable this.\ndirenv allow Additionally, we will need to stick to using a pure environment as much as possible to prevent unexpected situations. So we set:\n# .envrc eval \u0026#34;$(lorri direnv)\u0026#34; nix-shell --run bash --pure There\u0026rsquo;s still a catch though. We need to have lorri daemon running to make sure the packages are built automatically without us having to exit the shell and re-run things. We can turn to the documentation for this. Essentially, we need to have a user-level systemd socket file and service for lorri.\n# ~/.config/systemd/user/lorri.socket [Unit] Description=Socket for Lorri Daemon [Socket] ListenStream=%t/lorri/daemon.socket RuntimeDirectory=lorri [Install] WantedBy=sockets.target# ~/.config/systemd/user/lorri.service [Unit] Description=Lorri Daemon Requires=lorri.socket After=lorri.socket [Service] ExecStart=%h/.nix-profile/bin/lorri daemon PrivateTmp=true ProtectSystem=strict ProtectHome=read-only Restart=on-failure With that we are finally ready to start working with our auto-managed, reproducible environments.\nsystemctl --user daemon-reload \u0026amp;\u0026amp; \\ systemctl --user enable --now lorri.socket Rethinking As promised, we will first test the setup to see that everything is working. Now is also a good time to try the tidybayes.rethinking package. In order to use it, we will need to define the rethinking package in a way so we can pass it to the buildInputs for tidybayes.rethinking. We will modify new shell.nix as follows:\n# shell.nix let sources = import ./nix/sources.nix; pkgs = import sources.nixpkgs { }; stdenv = pkgs.stdenv; rethinking = with pkgs.rPackages; buildRPackage { name = \u0026#34;rethinking\u0026#34;; src = pkgs.fetchFromGitHub { owner = \u0026#34;rmcelreath\u0026#34;; repo = \u0026#34;rethinking\u0026#34;; rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; }; propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; }; tidybayes_rethinking = with pkgs.rPackages; buildRPackage { name = \u0026#34;tidybayes.rethinking\u0026#34;; src = pkgs.fetchFromGitHub { owner = \u0026#34;mjskay\u0026#34;; repo = \u0026#34;tidybayes.rethinking\u0026#34;; rev = \u0026#34;df903c88f4f4320795a47c616eef24a690b433a4\u0026#34;; sha256 = \u0026#34;1jl3189zdddmwm07z1mk58hcahirqrwx211ms0i1rzbx5y4zak0c\u0026#34;; }; propagatedBuildInputs = [ dplyr tibble rlang MASS tidybayes rethinking rstan ]; }; rEnv = pkgs.rWrapper.override { packages = with pkgs.rPackages; [ ggplot2 tidyverse tidybayes devtools modelr cowplot ggrepel RColorBrewer purrr forcats rstan rethinking tidybayes_rethinking ]; }; in pkgs.mkShell { buildInputs = with pkgs; [ git glibcLocales which ]; inputsFrom = [ rEnv ]; shellHook = \u0026#39;\u0026#39; mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; \u0026#39;\u0026#39;; GIT_SSL_CAINFO = \u0026#34;${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux \u0026#34;${pkgs.glibcLocales}/lib/locale/locale-archive\u0026#34;; } The main thing to note here is that we need the output of the derivation we create here, i.e. we need to use inputsFrom and NOT buildInputs for rEnv.\nLet us try to get a nice graphic for the conclusion.\nlibrary(magrittr) library(dplyr) library(purrr) library(forcats) library(tidyr) library(modelr) library(tidybayes) library(tidybayes.rethinking) library(ggplot2) library(cowplot) library(rstan) library(rethinking) library(ggrepel) library(RColorBrewer) theme_set(theme_tidybayes()) rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) set.seed(5) n = 10 n_condition = 5 ABC = tibble( condition = factor(rep(c(\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;), n)), response = rnorm(n * 5, c(0,1,2,1,-1), 0.5) ) mtcars_clean = mtcars %\u0026gt;% mutate(cyl = factor(cyl)) m_cyl = ulam(alist( cyl ~ dordlogit(phi, cutpoint), phi \u0026lt;- b_mpg*mpg, b_mpg ~ student_t(3, 0, 10), cutpoint ~ student_t(3, 0, 10) ), data = mtcars_clean, chains = 4, cores = parallel::detectCores(), iter = 2000 ) cutpoints = m_cyl %\u0026gt;% recover_types(mtcars_clean) %\u0026gt;% spread_draws(cutpoint[cyl]) # define the last cutpoint last_cutpoint = tibble( .draw = 1:max(cutpoints$.draw), cyl = \u0026#34;8\u0026#34;, cutpoint = Inf ) cutpoints = bind_rows(cutpoints, last_cutpoint) %\u0026gt;% # define the previous cutpoint (cutpoint_{j-1}) group_by(.draw) %\u0026gt;% arrange(cyl) %\u0026gt;% mutate(prev_cutpoint = lag(cutpoint, default = -Inf)) fitted_cyl_probs = mtcars_clean %\u0026gt;% data_grid(mpg = seq_range(mpg, n = 101)) %\u0026gt;% add_fitted_draws(m_cyl) %\u0026gt;% inner_join(cutpoints, by = \u0026#34;.draw\u0026#34;) %\u0026gt;% mutate(`P(cyl | mpg)` = # this part is logit^-1(cutpoint_j - beta*x) - logit^-1(cutpoint_{j-1} - beta*x) plogis(cutpoint - .value) - plogis(prev_cutpoint - .value) ) data_plot = mtcars_clean %\u0026gt;% ggplot(aes(x = mpg, y = cyl, color = cyl)) + geom_point() + scale_color_brewer(palette = \u0026#34;Dark2\u0026#34;, name = \u0026#34;cyl\u0026#34;) fit_plot = fitted_cyl_probs %\u0026gt;% ggplot(aes(x = mpg, y = `P(cyl | mpg)`, color = cyl)) + stat_lineribbon(aes(fill = cyl), alpha = 1/5) + scale_color_brewer(palette = \u0026#34;Dark2\u0026#34;) + scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;) png(filename=\u0026#34;../images/rethinking.png\u0026#34;) plot_grid(ncol = 1, align = \u0026#34;v\u0026#34;, data_plot, fit_plot ) dev.off Finally we will run this in our environment.\nRscript tesPlot.R   Conclusions This post was really more of an exploratory follow up to the previous post, and does not really work in isolation. Then again, at this point everything seems to have worked out well. R with Nix has finally become a truly viable combination for any and every analysis under the sun. Some parts of the workflow are still a bit janky, but will probably resolve themselves over time.\nUpdate: There is a final part detailing automated ways of reloading the system configuration\n My motivations were laid out in the aforementioned post, and will not be repeated [return] For why these are the way they are see the this is written, see the aforementioned post [return] Christine Dodrill has a great write up on using these tools as well [return]   ","permalink":"https://rgoswami.me/posts/rethinking-r-nix/","tags":["tools","nix","workflow","R"],"title":"Statistical Rethinking and Nix"},{"categories":["programming"],"contents":"  This post discusses briefly, the nix-shell environment for reproducible programming. In particular, there is an emphasis on extensions for installing and working with packages not in CRAN, i.e. packages off Github which are normally installed with devtools.\n Background The entire nix ecosystem is fantastic, and is the main packaging system used by d-SEAMS as well. Recently I began working through the excellent second edition of \u0026ldquo;Statistical Rethinking\u0026rdquo; by Richard McElreath1.\nUnfortunately, the rethinking package which is a major component of the book itself depends on the V8 engine for some reason. The reigning AUR2 package (V8-r) broke with a fun error message I couldn\u0026rsquo;t be bothered to deal with. Ominously, the rest of the logs prominently featured Warning: Running gclient on Python 3.. Given that older python versions have been permanently retired, this seemed like a bad thing to deal with3. In any case, having weaned off non-nix dependency tools for python and friends, it seemed strange to not do the same for R.\nThe standard installation for the package entails obtaining rstan (which is trivial with nixpkgs) and then using:\ninstall.packages(c(\u0026#34;coda\u0026#34;,\u0026#34;mvtnorm\u0026#34;,\u0026#34;devtools\u0026#34;,\u0026#34;loo\u0026#34;,\u0026#34;dagitty\u0026#34;)) library(devtools) devtools::install_github(\u0026#34;rmcelreath/rethinking\u0026#34;) We will break this down and work through this installation in Nix space.\nNix and R The standard approach to setting up a project shell.nix is simply by using the mkshell function. There are some common aspects to this workflow, with more language specific details documented here. A simple first version might be:\nlet pkgs = import \u0026lt;nixpkgs\u0026gt; { }; in pkgs.mkShell { buildInputs = with pkgs; [ zsh R rPackages.ggplot rPackages.data_table ]; shellHook = \u0026#39;\u0026#39; echo \u0026#34;hello\u0026#34; \u0026#39;\u0026#39;; LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux \u0026#34;${glibcLocales}/lib/locale/locale-archive\u0026#34;; } Where we note that we can install CRAN packages as easily as regular packages (like R), except for the fact that they are kept in a pkgs.rPackages environment, as opposed to pkgs. This is actually a common convention most languages with central repos. The most interesting thing to note is that, similar to the convention for nix-python setups, packages with a dot in the name will be converted to having an underscore, i.e. data.table -\u0026gt; data_table.\nHowever, for the rethinking package, and many others, there is no current CRAN package, and so the rPackages approach fails.\nThe LOCALE_ARCHIVE needs to be set for Linux machines, and is required for working with other packages.\nNix-R and Devtools To work with non-CRAN packages, we need to modify our package setup a little. We will also simplify our file to split the pkgs and the r-pkgs.\nNaive Approach The naive approach works by using the shellHook to set R_LIBS_USER to save user packages per-project.\n{ pkgs ? import \u0026lt;nixpkgs\u0026gt; { } }: with pkgs; let my-r-pkgs = rWrapper.override { packages = with rPackages; [ ggplot2 knitr rstan tidyverse V8 dagitty coda mvtnorm shape Rcpp tidybayes ]; }; in mkShell { buildInputs = = with pkgs;[ git glibcLocales openssl openssh curl wget ]; inputsFrom = [ my-r-pkgs ]; shellHook = \u0026#39;\u0026#39; mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; \u0026#39;\u0026#39;; GIT_SSL_CAINFO = \u0026#34;${cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux \u0026#34;${glibcLocales}/lib/locale/locale-archive\u0026#34;; } Note that here we will also need to set the GIT_SSL_CAINFO to prevent some errors during the build process\u0026nbsp;4.\nNative Approach The native approach essentially leverages the nix method for building R packages. This is the most reproducible of the lot, and also has the useful property of storing the files in the nix-store so re-using packages across different projects will not store, build or download the package again. The values required can be calculated from nix-prefetch-git as follows:\nnix-env -i nix-prefetch-git nix-prefetch-git https://github.com/rmcelreath/rethinking.git The crux of this approach is the following snippet\u0026nbsp;5:\n(buildRPackage { name = \u0026#34;rethinking\u0026#34;; src = fetchFromGitHub { owner = \u0026#34;rmcelreath\u0026#34;; repo = \u0026#34;rethinking\u0026#34;; rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; }; propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; }) Project Shell This formulation for some strange reason does not work from the shell or environment by default, but does work with nix-shell --run bash --pure.\n{ pkgs ? import \u0026lt;nixpkgs\u0026gt; { } }: with pkgs; let my-r-pkgs = rWrapper.override { packages = with rPackages; [ ggplot2 knitr rstan tidyverse V8 dagitty coda mvtnorm shape Rcpp tidybayes (buildRPackage { name = \u0026#34;rethinking\u0026#34;; src = fetchFromGitHub { owner = \u0026#34;rmcelreath\u0026#34;; repo = \u0026#34;rethinking\u0026#34;; rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; }; propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; }) ]; }; in mkShell { buildInputs = with pkgs; [ git glibcLocales openssl which openssh curl wget my-r-pkgs ]; shellHook = \u0026#39;\u0026#39; mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; echo ${my-r-pkgs}/bin/R \u0026#39;\u0026#39;; GIT_SSL_CAINFO = \u0026#34;${cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux \u0026#34;${glibcLocales}/lib/locale/locale-archive\u0026#34;; } The reason behind this is simply that rWrapper forms an extra package which has lower precedence than the user profile R, which is documented in more detail here on the NixOS wiki.\nUser Profile This is a more general approach which defines the environment for R with all the relevant libraries and is described in the nixpkgs manual. The following code should be placed in $HOME/.config/nixpkgs/config.nix:\n{ packageOverrides = super: let self = super.pkgs; in { rEnv = super.rWrapper.override { packages = with self.rPackages; [ ggplot2 knitr tidyverse tidybayes (buildRPackage { name = \u0026#34;rethinking\u0026#34;; src = self.fetchFromGitHub { owner = \u0026#34;rmcelreath\u0026#34;; repo = \u0026#34;rethinking\u0026#34;; rev = \u0026#34;d0978c7f8b6329b94efa2014658d750ae12b1fa2\u0026#34;; sha256 = \u0026#34;1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0\u0026#34;; }; propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ]; }) ]; }; }; } This snippet allows us to use our R as follows:\n# Install things nix-env -f \u0026#34;\u0026lt;nixpkgs\u0026gt;\u0026#34; -iA rEnv # Fix locale export LOCALE_ARCHIVE=\u0026#34;$(nix-build --no-out-link \u0026#34;\u0026lt;nixpkgs\u0026gt;\u0026#34; -A glibcLocales)/lib/locale/locale-archive\u0026#34; # Profit R Note that in this method, on Linux systems, the locale problem has to be fixed with the explicit export. This means that this should be used mostly with project level environments, instead of populating the global shell RC files.\nUpdate: There is another post with methods to reload this configuration automatically\nConclusions Of the methods described, the most useful method for working with packages not hosted on CRAN is through the user-profile, while the shell.nix method is useful in conjunction, for managing various projects. So the ideal approach is then to use the user profile for installing anything which normally uses devtools and then use shell.nix for the rest.\nNote that if the Project Shell is used with a User Profile as described in the next section, all packages defined there can be dropped and then the project shell does not need to execute R by default. The simplified shell.nix is then simply:\n{ pkgs ? import \u0026lt;nixpkgs\u0026gt; { } }: with pkgs; let my-r-pkgs = rWrapper.override { packages = with rPackages; [ ggplot2 ]; }; in mkShell { buildInputs = with pkgs;[ git glibcLocales openssl which openssh curl wget my-r-pkgs ]; inputsFrom = [ my-r-pkgs ]; shellHook = \u0026#39;\u0026#39; mkdir -p \u0026#34;$(pwd)/_libs\u0026#34; export R_LIBS_USER=\u0026#34;$(pwd)/_libs\u0026#34; \u0026#39;\u0026#39;; GIT_SSL_CAINFO = \u0026#34;${cacert}/etc/ssl/certs/ca-bundle.crt\u0026#34;; LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux \u0026#34;${glibcLocales}/lib/locale/locale-archive\u0026#34;; } The entire workflow for rethinking is continued here.\n As part of a summer course at the University of Iceland relating to their successful COVID-19 model [return] The Arch User Repository is the port of first call for most ArchLinux users [return] Though, like any good AUR user, I did post a bug report [return] This approach is also discussed here [return] As discussed on this issue, this stackoverflow question and also seen here [return]   ","permalink":"https://rgoswami.me/posts/nix-r-devtools/","tags":["tools","nix","workflow","R"],"title":"Nix with R and devtools"},{"categories":["notes"],"contents":" Background I recently had the opportunity to take part in an AMA (ask me anything) session for the CS106A students on Machine Learning for the Physical Sciences. This is a post about the technical issues, and also includes a video if you read through.\nZoom and LosslessCut Zoom recordings are one of the nicer ways to deal with switching windows and screen sharing, especially after fixing the dark screen glitch. However, though LosslessCut works really well to get cut-points, exporting and merging the file into one caused a bunch of glitches.\nEnter Handbrake To not beat around the bush, the solution was to simply encode the Zoom recording with Handbrake before using LosslessCut\u0026nbsp;1. Since the conversion takes a while, it is also neat to note that you can directly export the cut points made with LosslessCut on the original video, then import them onto the newly encoded file.\nConclusions I am not really sure how this will turn out, but it is a useful thing to keep in mind. The introductory video turned out to be:\n   For me, the Vimeo Youtube HQ 1080p60 preset worked out well [return]   ","permalink":"https://rgoswami.me/posts/losslesscut-zoom-ama/","tags":["teaching","cs106a","tools"],"title":"LosslessCut, Zoom and an AMA for CS106A"},{"categories":["notes"],"contents":" Background I have been leading the fantastic section 881 as a virtual section leader for the Stanford CS106A: Code in Place initiative for the past four weeks. I have also spent a lot of time on Zoom, sharing my screen. Fun fact. My screen shares look like this:\n  Figure 1: Zoom screen share with weird overlay\n  This post is about hunting down what caused this amazing zoom glitch1 and how I finally fixed it.\nTiling Windows and Compositors For reasons best left to another post, I use the fabulous i3 window manager, with colemak keybindings described here. Recall that, from Wikipedia:\n A compositing window manager is a window manager that provides applications with an off-screen buffer for each window. The window manager composites the window buffers into an image representing the screen and writes the result into the display memory.\n For reasons I can no longer recall, compton has been a traditional aspect of my workflow. As per my last update back in April last year; my configuration is here.\nCompton to Picom Some time ago (actually many months ago), compton itself transitioned over to picom, but remained largely compatible with my old configuration2. To be clear, the transition was largely painless, with ample warnings in the terminal showing up; along with very reasonable fallbacks. The key aspect of my compton.conf which caused the shadowing was:\nshadow = true; shadow-radius = 5; shadow-offset-x = -5; shadow-offset-y = -5; shadow-opacity = 0.5; The corrective measure was simply to set shadow-opacity to nothing; that is:\nshadow-opacity = 0.0; The rest of the configuration is here; and contains a lot more, mostly pertaining to opacity and other pretty effects3.\nConclusion Finally we have achieved the goal of having normal screen sharing capabilities; as seen below:\n  Figure 2: Just in time to see an excellent pun\n  The struggle was real, though the cause was trivial, and really highlights the need to always know your system packages. In this case, no doubt my students would have preferred not having to suffer through the darkness of my screen4. This has been a rather trivial post, but one to keep in mind none-the-less.\n To be clear, none of the windows were the glitch. The issue was the darkened overlay [return] As always, the ArchLinux Wiki is a great place for more information [return] The rest of my Dotfiles, managed by the excellent dotgit are also worth a look [return] Though it might have also served as a metaphor for darkness [return]   ","permalink":"https://rgoswami.me/posts/compton-zoom-shadow/","tags":["workflow","tools"],"title":"Compton to Picom and Zoom Glitches"},{"categories":["notes"],"contents":" Background One of the main reasons to use orgmode is definitely to get a better note taking workflow. Closely related to blogging or writing, the ideal note workflow is one which lets you keep a bunch of throwaway ideas and also somehow have access to them in a coherent manner. This will be a long post, and it is a work-in-progress, so, keep that in mind. Since this is mainly me1 work-shopping my technique, the philosophy will come in a later post probably. This workflow is documented more sparsely in my config file here, in the noteYoda section2. Some parts of this post also include mini video clips for clarity3.\nThe entire workflow will end up being something like this4:\n  Concept While working through ideas, it actually was more useful to describe the workflow I want, and then implement it, instead of relying on the canned approaches of each package. So the basics of the ideology are listed below.\nReference Management Reference management is one of the main reasons to consider a plain-text setup, and mine is no different. The options most commonly seen are:\n Mendeley This is a great option, and the most mobile friendly of the bunch. Sadly, the price tiers aren\u0026rsquo;t very friendly so I have to give it a hard pass. Jabref This is fun, but really more of a per-project management system, but it works well for that. The fact that it is Java based was a major issue for me. Zotero This is what I personally use and recommend. More on that in a later post.  Notes The idea is to be able to create notes for all kinds of content. Specifically, papers or books, along with webpages. This then requires a separate system for each which is described by:\n Search Engine The search engine is key, both in terms of accessibility and scalability. It is assumed that there will be many notes, and that they will have a wide variety of content. The search interface must then simply allow us to narrow down our candidates in a meaningful manner. Contextual Representation This aspect of the workflow deals with representations, which should transcend the usage of tags or categories. In particular, it would be nice to be able to visualize the flow of ideas, each represented by a note. Backlinks In particular, by backlinks at this point we are referring to the ability to link to a pdf or a website with a unique key such that notes can be added or removed at will. Storage Not actually part of the workflow in the same way, since it will be handled at the system level, it is worth nothing, that in this workflow Zotero is used to export a master bib file and keeps it updated, while the notes themselves are version controlled5.  The concepts above will be handled by the following packages.\n   Concept Package Note     Search deft Has a great interface   Context org-roam Allows the export of graphiz mindmaps   Backlinks org-roam, org-ref, org-noter Covers websites, bibliographies, and pdfs respectively    A key component in this workflow is actually facilitated by the fabulous org-roam-bibtex or ORB. The basic idea is to ensure meaningful templates which interpolate smoothly with org-roam, org-ref, helm-bibtex, and org-capture.\nBasic Variables Given the packages we will be using, some variable settings are in order, namely:\n(setq org_notes (concat (getenv \u0026#34;HOME\u0026#34;) \u0026#34;/Git/Gitlab/Mine/Notes/\u0026#34;) zot_bib (concat (getenv \u0026#34;HOME\u0026#34;) \u0026#34;/GDrive/zotLib.bib\u0026#34;) org-directory org_notes deft-directory org_notes org-roam-directory org_notes ) Search For the search setup, the doom-emacs deft setup, by adding +deft in my init.el, worked out of the box for me. For those who do not use doom6, the following should suffice:\n(use-package deft :commands deft :init (setq deft-default-extension \u0026#34;org\u0026#34; ;; de-couples filename and note title: deft-use-filename-as-title nil deft-use-filter-string-for-filename t ;; disable auto-save deft-auto-save-interval -1.0 ;; converts the filter string into a readable file-name using kebab-case: deft-file-naming-rules \u0026#39;((noslash . \u0026#34;-\u0026#34;) (nospace . \u0026#34;-\u0026#34;) (case-fn . downcase))) :config (add-to-list \u0026#39;deft-extensions \u0026#34;tex\u0026#34;) ) For more about the doom-emacs defaults, check the Github repo. The other aspect of interacting with the notes is via the org-roam interface and will be covered below.\nBibliography Since I will be using org-ref, it makes no sense to load or work with the +biblio module at the moment. Thus this section is actually doom agnostic. The basic tools of bibliographic management from the emacs end are the venerable helm-bibtex (repo here) and org-ref (repo here). In order to make this guide complete, I will also describe the Zotero settings I have.\nZotero Without getting too deep into the weeds here, the basic requirements are:\n Zotero The better bibtex extension  The idea is to then have one top level .bib file in some handy location which you will set up to sync automatically. To make life easier, there is a tiny recording of the next steps.\n  Helm-Bibtex This venerable package is really good at interfacing with a variety of externally formatted bibliographic managers.\n(setq bibtex-completion-notes-path \u0026#34;/home/haozeke/Git/Gitlab/Mine/Notes/\u0026#34; bibtex-completion-bibliography \u0026#34;/home/haozeke/GDrive/zotLib.bib\u0026#34; bibtex-completion-pdf-field \u0026#34;file\u0026#34; bibtex-completion-notes-template-multiple-files (concat \u0026#34;#+TITLE: ${title}\\n\u0026#34; \u0026#34;#+ROAM_KEY: cite:${=key=}\\n\u0026#34; \u0026#34;* TODO Notes\\n\u0026#34; \u0026#34;:PROPERTIES:\\n\u0026#34; \u0026#34;:Custom_ID: ${=key=}\\n\u0026#34; \u0026#34;:NOTER_DOCUMENT: %(orb-process-file-field \\\u0026#34;${=key=}\\\u0026#34;)\\n\u0026#34; \u0026#34;:AUTHOR: ${author-abbrev}\\n\u0026#34; \u0026#34;:JOURNAL: ${journaltitle}\\n\u0026#34; \u0026#34;:DATE: ${date}\\n\u0026#34; \u0026#34;:YEAR: ${year}\\n\u0026#34; \u0026#34;:DOI: ${doi}\\n\u0026#34; \u0026#34;:URL: ${url}\\n\u0026#34; \u0026#34;:END:\\n\\n\u0026#34; ) ) doom-emacs users like me might want to wrap the above in a nice after! org-ref expression, but it doesn\u0026rsquo;t really matter.\nExplanation To break-down aspects of the configuration snippet above:\n The template includes the orb-process-file-field function to allow selecting the pdf to be used with org-noter The file field is specified to work with the .bib file generated by Zotero helm-bibtex allows for any of the keys in a .bib file to be used in a template, and an overly expressive one is more useful The ROAM_KEY is defined to ensure that cite backlinks work correctly with org-roam As I prefer to have one notes file per pdf, I have only configured the bibtex-completion-notes-template-multiple-files variable  Org-Ref As discussed above, this just makes citations much more meaningful in orgmode.\n(use-package org-ref :config (setq org-ref-completion-library \u0026#39;org-ref-ivy-cite org-ref-get-pdf-filename-function \u0026#39;org-ref-get-pdf-filename-helm-bibtex org-ref-default-bibliography (list \u0026#34;/home/haozeke/GDrive/zotLib.bib\u0026#34;) org-ref-bibliography-notes \u0026#34;/home/haozeke/Git/Gitlab/Mine/Notes/bibnotes.org\u0026#34; org-ref-note-title-format \u0026#34;* TODO %y - %t\\n :PROPERTIES:\\n :Custom_ID: %k\\n :NOTER_DOCUMENT: %F\\n :ROAM_KEY: cite:%k\\n :AUTHOR: %9a\\n :JOURNAL: %j\\n :YEAR: %y\\n :VOLUME: %v\\n :PAGES: %p\\n :DOI: %D\\n :URL: %U\\n :END:\\n\\n\u0026#34; org-ref-notes-directory \u0026#34;/home/haozeke/Git/Gitlab/Mine/Notes/\u0026#34; org-ref-notes-function \u0026#39;orb-edit-notes )) An essential aspect of this configuration is just that most of heavy lifting in terms of the notes are palmed off to helm-bibtex.\nExplanation To break-down aspects of the configuration snippet above:\n The org-ref-get-pdf-filename-function simply uses the helm-bibtex settings to find the pdf The default bibliography and notes directory are set to the same location as all the org-roam files, to encourage a flat hierarchy The org-ref-notes-function simply ensures that, like the helm-bibtex settings, I expect one file per pdf, and that I would like to use my org-roam template instead of the org-ref or helm-bibtex one  Note that for some reason, the format specifiers for org-ref are not the keys in .bib but are instead, the following7:\nIn the format, the following percent escapes will be expanded. %l The BibTeX label of the citation. %a List of author names, see also `reftex-cite-punctuation\u0026#39;. %2a Like %a, but abbreviate more than 2 authors like Jones et al. %A First author name only. %e Works like %a, but on list of editor names. (%2e and %E work as well) It is also possible to access all other BibTeX database fields: %b booktitle %c chapter %d edition %h howpublished %i institution %j journal %k key %m month %n number %o organization %p pages %P first page %r address %s school %u publisher %t title %v volume %y year %B booktitle, abbreviated %T title, abbreviated %U url %D doi %S series %N note %f pdf filename %F absolute pdf filename Usually, only %l is needed. The other stuff is mainly for the echo area display, and for (setq reftex-comment-citations t). %\u0026lt; as a special operator kills punctuation and space around it after the string has been formatted. A pair of square brackets indicates an optional argument, and RefTeX will prompt for the values of these arguments. Indexing Notes This part of the workflow builds on the concepts best known as the Zettelkasten method. More details about the philosophy behind org-roam is here.\nOrg-Roam The first part of this interface is essentially just the doom-emacs configuration, adapted for those who don\u0026rsquo;t believe in the dark side below.\n(use-package org-roam :hook (org-load . org-roam-mode) :commands (org-roam-buffer-toggle-display org-roam-find-file org-roam-graph org-roam-insert org-roam-switch-to-buffer org-roam-dailies-date org-roam-dailies-today org-roam-dailies-tomorrow org-roam-dailies-yesterday) :preface ;; Set this to nil so we can later detect whether the user has set a custom ;; directory for it, and default to `org-directory\u0026#39; if they haven\u0026#39;t. (defvar org-roam-directory nil) :init :config (setq org-roam-directory (expand-file-name (or org-roam-directory \u0026#34;roam\u0026#34;) org-directory) org-roam-verbose nil ; https://youtu.be/fn4jIlFwuLU org-roam-buffer-no-delete-other-windows t ; make org-roam buffer sticky org-roam-completion-system \u0026#39;default ;; Normally, the org-roam buffer doesn\u0026#39;t open until you explicitly call ;; `org-roam\u0026#39;. If `+org-roam-open-buffer-on-find-file\u0026#39; is non-nil, the ;; org-roam buffer will be opened for you when you use `org-roam-find-file\u0026#39; ;; (but not `find-file\u0026#39;, to limit the scope of this behavior). (add-hook \u0026#39;find-file-hook (defun +org-roam-open-buffer-maybe-h () (and +org-roam-open-buffer-on-find-file (memq \u0026#39;org-roam-buffer--update-maybe post-command-hook) (not (window-parameter nil \u0026#39;window-side)) ; don\u0026#39;t proc for popups (not (eq \u0026#39;visible (org-roam-buffer--visibility))) (with-current-buffer (window-buffer) (org-roam-buffer--get-create))))) ;; Hide the mode line in the org-roam buffer, since it serves no purpose. This ;; makes it easier to distinguish among other org buffers. (add-hook \u0026#39;org-roam-buffer-prepare-hook #\u0026#39;hide-mode-line-mode)) ;; Since the org module lazy loads org-protocol (waits until an org URL is ;; detected), we can safely chain `org-roam-protocol\u0026#39; to it. (use-package org-roam-protocol :after org-protocol) (use-package company-org-roam :after org-roam :config (set-company-backend! \u0026#39;org-mode \u0026#39;(company-org-roam company-yasnippet company-dabbrev))) Once again, for more details, check the Github repo.\nOrg-Roam-Bibtex The configuration required is:\n(use-package org-roam-bibtex :after (org-roam) :hook (org-roam-mode . org-roam-bibtex-mode) :config (setq org-roam-bibtex-preformat-keywords \u0026#39;(\u0026#34;=key=\u0026#34; \u0026#34;title\u0026#34; \u0026#34;url\u0026#34; \u0026#34;file\u0026#34; \u0026#34;author-or-editor\u0026#34; \u0026#34;keywords\u0026#34;)) (setq orb-templates \u0026#39;((\u0026#34;r\u0026#34; \u0026#34;ref\u0026#34; plain (function org-roam-capture--get-point) \u0026#34;\u0026#34; :file-name \u0026#34;${slug}\u0026#34; :head \u0026#34;#+TITLE: ${=key=}: ${title}\\n#+ROAM_KEY: ${ref} - tags :: - keywords :: ${keywords} \\n* ${title}\\n :PROPERTIES:\\n :Custom_ID: ${=key=}\\n :URL: ${url}\\n :AUTHOR: ${author-or-editor}\\n :NOTER_DOCUMENT: %(orb-process-file-field \\\u0026#34;${=key=}\\\u0026#34;)\\n :NOTER_PAGE: \\n :END:\\n\\n\u0026#34; :unnarrowed t)))) Where most of the configuration is essentially the template again. Like helm-bibtex, ORB allows taking arbitrary keys from the .bib file.\nOrg Noter The final aspect of a pdf workflow is simply ensuring that every pdf is associated with notes. The philosophy of org-noter is best described here. Only minor tweaks should be required to get this working with interleave as well.\n(use-package org-noter :after (:any org pdf-view) :config (setq ;; The WM can handle splits org-noter-notes-window-location \u0026#39;other-frame ;; Please stop opening frames org-noter-always-create-frame nil ;; I want to see the whole file org-noter-hide-other nil ;; Everything is relative to the main notes file org-noter-notes-search-path (list org_notes) ) ) Evidently, from my configuration, it appears that I decided to use org-noter over the more commonly described interleave because it has better support for working with multiple documents linked to one file.\nOrg-Protocol I will only cover the bare minimum relating to the use of org-capture here, because eventually I intend to handle a lot more cases with orca. Note that this part of the workflow has more to do with using org-roam with websites than pdf files.\nTemplates This might get complicated but I am only trying to get the bare minimum for org-protocol right now.\n;; Actually start using templates (after! org-capture ;; Firefox and Chrome (add-to-list \u0026#39;org-capture-templates \u0026#39;(\u0026#34;P\u0026#34; \u0026#34;Protocol\u0026#34; entry ; key, name, type (file+headline +org-capture-notes-file \u0026#34;Inbox\u0026#34;) ; target \u0026#34;* %^{Title}\\nSource: %u, %c\\n #+BEGIN_QUOTE\\n%i\\n#+END_QUOTE\\n\\n\\n%?\u0026#34; :prepend t ; properties :kill-buffer t)) (add-to-list \u0026#39;org-capture-templates \u0026#39;(\u0026#34;L\u0026#34; \u0026#34;Protocol Link\u0026#34; entry (file+headline +org-capture-notes-file \u0026#34;Inbox\u0026#34;) \u0026#34;* %? [[%:link][%(transform-square-brackets-to-round-ones \\\u0026#34;%:description\\\u0026#34;)]]\\n\u0026#34; :prepend t :kill-buffer t)) ) Conclusions At this point, many might argue that since by the end, only one template is called, defining the rest were pointless. They would be right, however, this is just how my configuration evolved. Feel free to cannibalize this for your personal benefit. Eventually I plan to expand this into something with org-journal as well, but not right now.\n Rohit Goswami that is, from the landing page; obviously [return] This is a reference to my fantastic pet, named Yoda [return] Recorded with SimpleScreenRecorder, cut with LosslessCut, uploaded to YouTube, and embedded with a Hugo shortcode [return] The video uses org-ref-notes-function-many-files as the org-ref-notes-function so the template looks a little different [return] For some strange reason a lot of online posts suggested Dropbox for syncing notes, which makes no sense to me, it is always better to have version control and ignore rules [return] Therefore clearly proving that the cookies of the dark side have no power in the holy text editor war [return] Where these are from the org-ref documentation [return]   ","permalink":"https://rgoswami.me/posts/org-note-workflow/","tags":["ramblings","explanations"],"title":"An Orgmode Note Workflow"},{"categories":["programming"],"contents":" Background I have, in the past written about how I made the switch to Colemak. However, until recently, I was still trying to mimic the VIM keybindings from QWERTY. This is a post where I discuss the changes I made to ensure that I never have to stretch my fingers in odd ways again. The main idea is expressed well by vim-colemak.\nColemak layout: | QWERTY layout: `12345 67890-= Move around: | (instead of) `12345 67890-= qwfpg jluy;[]\\  e | k qwert yuiop[]\\  arstd HNEIo\u0026#39; h i | h l asdfg HJKL;\u0026#39; zxcvb km,./ n | j zxcvb nm,./ Emacs Though I have mentioned publicly, that I was using the regular QWERTY motion keys, I realized I had actually started to use the mouse more often, simply because it was a pain to navigate. Thankfully, emacs has evil-colemak-basics, which is fabulous. For reference, these make it really easy for QWERTY users to make the switch if they\u0026rsquo;re previously used to VIM bindings.\n   Colemak Qwerty Action States At Qwerty position? Remarks     h, n, e, i h, j, k, l navigate mnvo yes    k, K n, N search next/previous mnvo yes    u, U i, I insert _nv_ yes    l u undo _nv_ yes    N J join lines _nv_ yes    E K lookup mnv_ yes    u i inner text object keymap ___o yes    f, F e, E jump to end of word mnvo yes with t-f-j rotation   t, T f, f jump to character mnvo yes with t-f-j rotation   j, J t, T jump until character mnvo no with t-f-j rotation   j, J e, E jump to end of word mnvo no without t-f-j rotation    Where the table above is from the fantastic readme.\nI still had some issues, mostly relating to searching in buffers, so I ended using swiper-isearch more which is a bonus too.\nVisual Lines Since I tend to keep visual-line-mode all the time, it makes sense to actually swap working with lines and visual lines. To work this through this needs evil-better-visual-line.\n(use-package! evil-better-visual-line :after evil-colemak-basics :config (evil-better-visual-line-on) (map! :map evil-colemak-basics-keymap (:nvm \u0026#34;n\u0026#34; \u0026#39;evil-better-visual-line-next-line :nvm \u0026#34;e\u0026#34; \u0026#39;evil-better-visual-line-previous-line :nvm \u0026#34;g n\u0026#34; \u0026#39;evil-next-line :nvm \u0026#34;g e\u0026#34; \u0026#39;evil-previous-line)) ) Pdf-Tools For my doom-emacs configuration, I also set the following map:\n(after! pdf-view (add-hook! \u0026#39;pdf-view-mode-hook (evil-colemak-basics-mode -1)) (map! :map pdf-view-mode-map :n \u0026#34;g g\u0026#34; #\u0026#39;pdf-view-first-page :n \u0026#34;G\u0026#34; #\u0026#39;pdf-view-last-page :n \u0026#34;N\u0026#34; #\u0026#39;pdf-view-next-page-command :n \u0026#34;E\u0026#34; #\u0026#39;pdf-view-previous-page-command :n \u0026#34;e\u0026#34; #\u0026#39;evil-collection-pdf-view-previous-line-or-previous-page :n \u0026#34;n\u0026#34; #\u0026#39;evil-collection-pdf-view-next-line-or-next-page ) Where the most important thing is the hook which removes the evil-colemak-basics binding. Since it is a single mode and hook, after-hook! is the same as after-hook1.\nWindow Management Somehow these are not part of the evil-colemak defaults.\n(after! evil (map! :map evil-window-map (:leader (:prefix (\u0026#34;w\u0026#34; . \u0026#34;Select Window\u0026#34;) :n :desc \u0026#34;Left\u0026#34; \u0026#34;h\u0026#34; \u0026#39;evil-window-left :n :desc \u0026#34;Up\u0026#34; \u0026#34;e\u0026#34; \u0026#39;evil-window-up :n :desc \u0026#34;Down\u0026#34; \u0026#34;n\u0026#34; \u0026#39;evil-window-down :n :desc \u0026#34;Right\u0026#34; \u0026#34;i\u0026#34; \u0026#39;evil-window-right )) )) Search Harmonizing with Vimium.\n(after! evil (map! :map evil-motion-state-map (:n :desc \u0026#34;Previous match\u0026#34; \u0026#34;K\u0026#34; \u0026#39;evil-ex-search-previous :n :desc \u0026#34;Next match\u0026#34; \u0026#34;k\u0026#34; \u0026#39;evil-ex-search-next :n :desc \u0026#34;Forward search\u0026#34; \u0026#34;/\u0026#34; \u0026#39;evil-search-forward ) )) Page Movement Though this is more of a personal preference, I find it more natural to bind N and E to page-wise movement instead of join lines and lookup, since I almost never use those commands, and the movement keys echo what I expect elsewhere.\n(after! evil (map! :map evil-colemak-basics-keymap :nv \u0026#34;N\u0026#34; \u0026#39;evil-scroll-page-up :nv \u0026#34;E\u0026#34; \u0026#39;evil-scroll-page-down) ) Evil Org Annoyingly, evil-org-mode had a map which kept overriding all my other settings. Thankfully it has a helper variable to set movement. I also do not need this anyway, at-least not by default.\n(after! org (remove-hook \u0026#39;org-mode-hook \u0026#39;evil-org-mode) (setq evil-org-movement-bindings \u0026#39;((up . \u0026#34;e\u0026#34;) (down . \u0026#34;n\u0026#34;) (left . \u0026#34;h\u0026#34;) (right . \u0026#34;i\u0026#34;)) ) ) Vimium I use the excellent vimium to make Chrome be a little less annoying. Luckily the Wiki seems to have a reasonable suggestion for colemak. The basic idea is to migrate the underlying keys directly to ensure very few manual changes are required.\nmapkey n j mapkey N J mapkey e k mapkey E K mapkey i l mapkey I L mapkey k n mapkey K N mapkey l i mapkey L I mapkey j e mapkey J E Zsh To ensure uniform bindings, I used to use bindkey -v but will need some minor changes to that set up. I based this part of my configuration off the bindings of bunnyfly.\nbindkey -v # Colemak. bindkey -M vicmd \u0026#34;h\u0026#34; backward-char bindkey -M vicmd \u0026#34;n\u0026#34; down-line-or-history bindkey -M vicmd \u0026#34;e\u0026#34; up-line-or-history bindkey -M vicmd \u0026#34;i\u0026#34; forward-char bindkey -M vicmd \u0026#34;s\u0026#34; vi-insert bindkey -M vicmd \u0026#34;S\u0026#34; vi-insert-bol bindkey -M vicmd \u0026#34;k\u0026#34; vi-repeat-search bindkey -M vicmd \u0026#34;K\u0026#34; vi-rev-repeat-search bindkey -M vicmd \u0026#34;l\u0026#34; beginning-of-line bindkey -M vicmd \u0026#34;L\u0026#34; end-of-line bindkey -M vicmd \u0026#34;j\u0026#34; vi-forward-word-end bindkey -M vicmd \u0026#34;J\u0026#34; vi-forward-blank-word-end # Sane Undo, Redo, Backspace, Delete. bindkey -M vicmd \u0026#34;u\u0026#34; undo bindkey -M vicmd \u0026#34;U\u0026#34; redo bindkey -M vicmd \u0026#34;^?\u0026#34; backward-delete-char bindkey -M vicmd \u0026#34;^[[3~\u0026#34; delete-char # Keep ctrl+r searching bindkey -M viins \u0026#39;^R\u0026#39; history-incremental-pattern-search-forward bindkey -M viins \u0026#39;^r\u0026#39; history-incremental-pattern-search-backward Zathura There is no better pdf viewer than zathura, and it also works for djvu and friends. As a plus point, it normally has very reasonable vim bindings, and an excellent configuration system, so we will leverage that. The best part is that we can just add to it using include zathuraColemak or whatever so as to be minimally invasive.\nmap h scroll leftmap n scroll downmap e scroll upmap i scroll rightmap N scroll half-downmap E scroll half-upmap k search forwardmap K search backward# For TOC navigationmap [index] o toggle_index# hjkl → hneimap [index] n navigate_index downmap [index] e navigate_index upmap [index] h navigate_index collapsemap [index] i navigate_index expandmap [index] H navigate_index collapse-allmap [index] I navigate_index expand-all Zathura is a complicated beast, however, and my full configuration contains a lot more information.\ni3 I have some bindings set up in terms of $left $right $up and $down, so it was simple to re-bind them.\nset $left hset $down nset $up eset $right i Conclusions That seems to be it for now. If I think of more programs I use regularly which allow VIM bindings, or keybindings in general, I\u0026rsquo;ll probably just update this post. My full dotfiles are present here, and now include a colemak target.\n The hook fix was suggested by the fantastic hlissner on the super friendly doom Discord server. [return]   ","permalink":"https://rgoswami.me/posts/colemak-dots-refactor/","tags":["workflow"],"title":"Refactoring Dotfiles For Colemak"},{"categories":["programming"],"contents":" Background One of the best things about writing in orgmode is that we can embed and execute arbitrary code snippets. However, not all languages have an exporter, for obvious reasons. Somewhat surprisingly, there is no way to call pandoc on embedded snippets, which feels like a waste, especially when a whole bunch of documentation formats can be converted to orgmode with it.\nConsider the following beautifully highlighted snippet of an rst (ReStructured Text) list table.\n.. list-table:: Title :widths: 25 25 50 :header-rows: 1 * - Heading row 1, column 1 - Heading row 1, column 2 - Heading row 1, column 3 * - Row 1, column 1 - - Row 1, column 3 * - Row 2, column 1 - Row 2, column 2 - Row 2, column 3 Trying to run this will generate the sort of obvious error:\norg-babel-execute-src-block: No org-babel-execute function for rst! Writing an Exporter For this post, I will be focusing on rst, but this can be defined for any of the pandoc back-ends. The approach was inspired by ob-markdown.\n(defun org-babel-execute:rst (body params) \u0026#34;Execute a block of rst code with org-babel. This function is called by `org-babel-execute-src-block\u0026#39;.\u0026#34; (let* ((result-params (split-string (or (cdr (assoc :results params)) \u0026#34;\u0026#34;))) (in-file (org-babel-temp-file \u0026#34;rst-\u0026#34;)) (cmdline (cdr (assoc :cmdline params))) (to (cdr (assoc :to params))) (template (cdr (assoc :template params))) (cmd (concat \u0026#34;pandoc\u0026#34; \u0026#34; -t org\u0026#34; \u0026#34; -i \u0026#34; (org-babel-process-file-name in-file) \u0026#34; -f rst \u0026#34; \u0026#34; \u0026#34; cmdline))) (with-temp-file in-file (insert body)) (message cmd) (shell-command-to-string cmd))) ;; Send to results (defun org-babel-prep-session:rst (session params) \u0026#34;Return an error because rst does not support sessions.\u0026#34; (error \u0026#34;rst does not support sessions\u0026#34;)) Trying it out With that done, it is pretty trivial to re-run the above example.\n.. list-table:: Title :widths: 25 25 50 :header-rows: 1 * - Heading row 1, column 1 - Heading row 1, column 2 - Heading row 1, column 3 * - Row 1, column 1 - - Row 1, column 3 * - Row 2, column 1 - Row 2, column 2 - Row 2, column 3    Heading row 1, column 1 Heading row 1, column 2 Heading row 1, column 3     Row 1, column 1  Row 1, column 3   Row 2, column 1 Row 2, column 2          Note that we have used rst :exports both :results raw as the header argument.\nConclusions Will probably follow this up with an actual package, which should handle the entire spectrum of pandoc back-ends.\n","permalink":"https://rgoswami.me/posts/org-pandoc-babel/","tags":["tools","emacs","workflow","orgmode"],"title":"Pandoc to Orgmode with Babel"},{"categories":["programming"],"contents":" Background I have been wanting to find a workflow which allows me to bypass writing a lot of TeX by hand for a while now. To that end I looked into using a computer algebra system (CAS). Naturally, my first choice was the FOSS Maxima (also because it uses Lisp under the hood). However, for all the reasons listed here, relating to its accuracy, which have not been fixed even though the post was over 5 years ago, I ended up having to go with the closed source Mathematica.\nPackages Support for Mathematica in modern orgmode is mainly through the use of ob-mathematica, which is the official org-babel extension (from contrib) for working with Mathematica. However, ob-mathematica relies on the now-defunct mma package for font-locking, which is less than ideal. Thankfully, there exists the excellent wolfram-mode package which happens to be in MELPA as well. Finally, since the default return type of a mathematica block is an input-string meant to be used in another mathematica block, which is not useful when we work with org-babel, we will use the excellent mash.pl utility from here, as suggested by the ob-mathematica package to sanitize our output and set a unifying path.\nSo to recap, use your favorite manager to get:\n ob-mathematica (in contrib) wolfram-mode (MELPA) mash.pl (from here)\u0026nbsp;1  After obtaining the packages, the configuration is then simply\u0026nbsp;2:\n;; Load mathematica from contrib (org-babel-do-load-languages \u0026#39;org-babel-load-languages (append org-babel-load-languages \u0026#39;((mathematica . t)) )) ;; Sanitize output and deal with paths (setq org-babel-mathematica-command \u0026#34;~/.local/bin/mash\u0026#34;) ;; Font-locking (add-to-list \u0026#39;org-src-lang-modes \u0026#39;(\u0026#34;mathematica\u0026#34; . wolfram)) ;; For wolfram-mode (setq mathematica-command-line \u0026#34;~/.local/bin/mash\u0026#34;) Results LaTeX Now we are in a position to simply evaluate content with font-locking. We will test our set up with an example lifted from the ob-mathematica source-code.\n Table 1: A table \n   1 4     2 4   3 6   4 8   7 0    (1+Transpose@x)//TeXForm Where our header-line (with #+begin_src) is:\nmathematica :var x=example-table :results latex Sanity Checks We can also test the example from the blog post earlier to test basic mathematical sanity.\nLimit[Log[b-a+Ieta],eta-\u0026gt;0,Direction-\u0026gt;-1,Assumptions-\u0026gt;{a\u0026gt;0,b\u0026gt;0,a\u0026gt;b}]TeXForm[Limit[Log[b-a+Ieta],eta-\u0026gt;0,Direction-\u0026gt;1,Assumptions-\u0026gt;{a\u0026gt;0,b\u0026gt;0,a\u0026gt;b}]] \\((I*Pi + Log[a - b])*\\log (a-b)-i \\pi\\)\nInline Math Note that we can now also write fractions, integrals and other cumbersome TeX objects a lot faster with this syntax, like \\(\\frac{x^3}{3}\\). Where we are using the following snippet:\nsrc_mathematica[:exports none :results raw]{Integrate[x^2,x] // TeXForm} Plots For plots, the standard orgmode rules apply, that is, we have to export to a file and return the name through our code snippet. Consider:\np=Plot[Sin[x],{x,0,6Pi},Frame-\u0026gt;True];Export[\u0026#34;images/sine.png\u0026#34;,p];Print[\u0026#34;images/sine.png\u0026#34;]   Figure 1: An exported Mathematica image\n  Where we have used mathematica :results file as our header line.\n As noted in the comments, it is nicer to rename mash.pl to mash [return] For reference, my whole config is here [return]   ","permalink":"https://rgoswami.me/posts/org-mathematica/","tags":["tools","emacs","workflow","orgmode"],"title":"Using Mathematica with Orgmode"},{"categories":["notes"],"contents":" Background As I mentioned earlier, I\u0026rsquo;m leading a section for Stanford CS106A: Code in Place. This post relates to the notes and thoughts garnered during the small group training session\u0026nbsp;1.\nReflections Demographics Redacted. Did not use breakout meetings due to privacy issues.\nEngagement and Participation  Some people were more active (skewed responses) Some of the more rudimentary questions might have been suppressed  Highlighted Moments  Covering multiple perspectives Different mental models  Challenges and Transformations  Technical debt was an issue Lack of engagement Went on for too long  For me in particular:\n It took over two hours, and though most people stayed on, not everyone was engaged.\n Scenarios These are to be dealt with as per the guidelines here. Since different groups covered different scenarios, not all of these have answers here.\nEnsuring Engagement  You have some students who didn\u0026rsquo;t participate at all in the section. What do you do?\n Effective Communication  What might not be effective about the policy, “Students should just tell me if I say something that offends them”?\n Sharing Experiences  You just finished your section and are staying behind to answer questions from your students. A couple students asked what it’s like studying/working in an engineering/tech field.\nWhat things might you want to keep in mind when answering their questions?\n Time Management  Section went way over time due to lots of questions being asked by students. What are some time management strategies you can use moving forward?\n Homework Assists  A sectionee posts in your Ed group, “I am a little bit frustrated because I don\u0026rsquo;t really know where to start on the first assignment. A little hint would be very helpful.” How do you respond?\n Debugging  A sectionee shows you the following buggy code for printing all the elements in a list:\nmy_lst = [\u0026lsquo;apple\u0026rsquo;, \u0026lsquo;banana\u0026rsquo;, \u0026lsquo;carrot\u0026rsquo;] i = 0 while len(my_lst) \u0026gt; 0: print(my_lst[i]) i = i + 1\nThey explain that the code works (it prints all the elements in the right order) but then throws a weird error: “IndexError: list index out of range.” How would you help them find their bug?\n Quitting  You have a student who is already discouraged by how difficult the first assignment is and has told you they don’t feel cut out for CS. What do you say to them?\n  Provide encouragement Give examples of hardship faced Be positive and make sure they don’t feel worse, even if they do follow through and quit “You’re not the first” Takes a lot of time. Doesn’t happen overnight Ask them why they don’t feel cut out and try to solve that problem  Looking up issues  Why might it be problematic to say something like, “It’s easy to download X or look up the answer to Y”? Why might those statements not be true?\n  Difficulty in backgrounds (language barriers) They might not be able to understand stackoverflow.com until they learn more CS They might not know where to look online (lack of domain expertise) Dependencies (for downloads) Makes them feel bad if they don’t end up finding it easy   This post was created on the day of training, 21-04-20, but will be posted later [return]   ","permalink":"https://rgoswami.me/posts/scp-smallgrp-trainig/","tags":["teaching","cs106a"],"title":"CS106A Small Group Training"},{"categories":["notes"],"contents":" Background As I mentioned earlier, I\u0026rsquo;m leading a section for Stanford CS106A: Code in Place. I did also mention I\u0026rsquo;d try to keep a set of short notes on the process. I finally had my first section meeting!\nPreparation I went through the following:\n Sent out a welcome message Detailed the workflow Set up a HackMD instance Set up some slides in beamer\u0026nbsp;1  However, after that, I was still concerned since I didn\u0026rsquo;t get much of a response on the ice-breakers for EdStem. Thankfully, everyone showed up.\nTeaching  I had a fabulous session, and we went through a variety of concepts. Didn\u0026rsquo;t spend much time on icebreakers, but did get a feel for where the students stand on the functional vs imperative programming paradigms Possibly because of working through two different approaches, the 40 minute long session went on for two hours and fifteen minutes. Some students had more of a background than the others, thankfully computational thinking is not normally taught very well  Conclusion  The notes are visible here, and the session was recorded here2 It was fun, and I hope the students enjoyed it as much as I did. I will probably expand this in terms of the concepts covered, to give the students more of an overview of what was covered   Even though most of the session was supposed to be live, it was still helpful to show I was interested enough to set up slides [return] As always, advice is much appreciated (and moderated) [return]   ","permalink":"https://rgoswami.me/posts/scp-smallgrp-meet1/","tags":["teaching","cs106a"],"title":"CS106A Section Meeting I"},{"categories":["notes"],"contents":" Background As I mentioned in my last post, I\u0026rsquo;m leading a section for Stanford CS106A: Code in Place. I did also mention I\u0026rsquo;d try to keep a set of short notes on the process. So there1.\nThe Training Given the overwhelming number of students, and section leaders, the small groups are for fostering a community of teachers.\nConsider allowing for daisy chaining during introductions Discussions are the primary take-away Only the instructor should be coding during the session  Core components  Clarity Content Atmosphere Section management Correctness  Sectional Details  Check in at the start Notice the space Check in regularly Avoid negative phrases Establish norms and the general culture  Zoom Norms  Have people introduce themselves Mute people when they aren\u0026rsquo;t talking Raise hands Try to use icebreakers which respect privacy  Materials Here\u0026rsquo;s some of the stuff which, being as it was open-sourced, I suppose is OK to put here2.\n Section Leader Training Section Leaders\u0026rsquo; Guide to Virtual Sections Some Zoom Icebreakers   As you may know, the official playlist is here [return] If you know otherwise, let me know in the comments [return]   ","permalink":"https://rgoswami.me/posts/scp-smallgrp/","tags":["teaching","cs106a"],"title":"Small Section On-boarding"},{"categories":["notes"],"contents":" Background A few weeks ago, I ended up recording a video for the Stanford CS106A: Code in Place initiative (which can be found here). I heard back a while ago, and am now to lead a section for the course!\nI\u0026rsquo;ll probably be making a series of short posts as this process continues.\nOn-Boarding This was very reminiscent of the Carpentries instructor training, which makes sense, given how well thought out that experience was.\nWe started out with a pre-presentation where people were able to just spitball and connect, which is pretty neat.\nOne of the interesting parts of this, was the idea of interactive recorded lectures, where the professors will be watching lectures with the students. The entire slide deck is here.\nThe other great idea for this kind of long course was the idea of having a Tea room and a Teachers lounge where people can just tune in to chat.\nCaveats A couple of things which keep cropping up for online teaching in general are the following:\n Zoom does not have persistent chats, so an auxiliary tool like an Etherpad is great  ","permalink":"https://rgoswami.me/posts/scp-onboarding/","tags":["ideas","teaching","cs106a"],"title":"On-boarding for Code in Place"},{"categories":["notes"],"contents":" Background Like a lot of my tech based rants, this was brought on by a recent Hacker News post. I won\u0026rsquo;t go into why the product listed there is a hollow faux FOSS rip-off. I won\u0026rsquo;t discuss how that \u0026lsquo;free\u0026rsquo; analytics option, like many others are just hobby projects taking pot shots at other projects. Or how insanely overpriced most alternatives are.\nI will however discuss why and how I transitioned to using the awesome Goat Counter.\nGoogle Analytics I would like to point out that it is OK to start out with Google Analytics. It is easy, and free, and scales well. There are reasons not to, but it is a good starting point.\nPros  Google Analytics is free, truly free The metrics are very detailed It is easy to set up  Cons  Privacy concerns Blocked by people Easy to obsess over metrics  Goat Counter As with most Hacker News posts, the article itself was nothing compared to the excellent comment thread. It was there that I came across people praising Goat Counter.\nPros  Is open sourced (here on Github) Super lightweight Anonymous statistics Easy to share  Cons  Has an upper limit on free accounts (10k a month) I am not very fond of Go  Conclusions I might eventually go back to GA, if I go over the 10k page view limit. Then again, I might not. It might be more like, I only care about the first 10k people who make it to my site.\n","permalink":"https://rgoswami.me/posts/goat-google/","tags":["tools","rationale","workflow","ideas"],"title":"Analytics: Google to Goat"},{"categories":["notes"],"contents":"  Explain why using bagging for prediction trees generally improves predictions over regular prediction trees.\n Introduction Bagging (or Bootstrap Aggregation) is one of the most commonly used ensemble method for improving the prediction of trees. We will broadly follow a historical development trend to understand the process. That is, we will begin by considering the Bootstrap method. This in turn requires knowledge of the Jacknife method, which is understandable from a simple bias variance perspective. Finally we will close out the discussion by considering the utility and trade-offs of the Bagging technique, and will draw attention to the fact that the Bagging method was contrasted to another popular ensemble method, namely the Random Forest method, in the previous section.\nBefore delving into the mathematics, recall that the approach taken by bagging is given as per Cichosz (2015) to be:\n create base models with bootstrap samples of the training set combine models by unweighted voting (for classification) or by averaging (for regression)  The reason for covering the Jacknife method is to develop an intuition relating to the sampling of data described in the following table:\n   Data-set Size per sample Estimator     Reduces Jacknife   Remains the same Bootstrap   Increases data-augmentation    Bias Variance Trade-offs We will recall, for this discussion, the bias variance trade off which is the basis of our model accuracy estimates (for regression) as per the formulation of James et al. (2013).\n\\begin{equation} E(y₀-\\hat{f}(x₀))²=\\mathrm{Var}(\\hat{f}(x₀))+[\\mathrm{Bias(\\hat{f(x₀)})}]²+\\mathrm{Var}(ε) \\end{equation}\nWhere:\n \\(E(y_{0}-\\hat{f}(x_{0}))²\\) is the expected test MSE, or the average test MSE if \\(f\\) is estimated with a large number of training sets and tested at each \\(x₀\\) The variance is the amount by which our approximation \\(\\hat{f}\\) will change if estimated by a different training set, or the flexibility error The bias is the (reducible) approximation error, caused by not fitting to the training set exactly \\(\\mathrm{Var}(ε)\\) is the irreducible error  We will also keep in mind, going forward the following requirements of a good estimator:\n Low variance AND low bias Typically, the variance increases while the bias decreases as we use more flexible methods (i.e. methods which fit the training set better1)  Also for the rest of this section, we will need to recall from Hastie, Tibshirani, and Friedman (2009), that the bias is given by:\n\\begin{equation} [E(\\hat{f_{k}}(x₀)-f(x₀)]² \\end{equation}\nWhere the expectation averages over the randomness in the training data.\nTo keep things in perspective, recall from Hastie, Tibshirani, and Friedman (2009):\n  Figure 1: Test and training error as a function of model complexity\n  Jacknife Estimates We will model our discussion on the work of Efron (1982). Note that:\n The \\(\\hat{θ}\\) symbol is an estimate of the true quantity \\(θ\\) This is defined by the estimate being \\(\\hat{θ}=θ(\\hat{F})\\) \\(\\hat{F}\\) is the empirical probability distribution, defined by mass \\(1/n\\) at \\(xᵢ ∀ i∈I\\), i is from 1 to n  The points above establishes our bias to be given by \\(E_Fθ(\\hat{F})-θ(F)\\) such that \\(E_F\\) is the expectation under x₁⋯xₙ~F.\nTo derive the Jacknife estimate \\((\\tilde{θ})\\) we will simply sequentially delete points xᵢ (changing \\(\\hat{F}\\)), and recompute our estimate \\(\\hat{θ}\\), which then simplifies to:\n\\begin{equation} \\tilde{θ}\\equiv n\\hat{θ}-(\\frac{n-1}{n})∑_{i=1}ⁿ\\hat{θ} \\end{equation}\nIn essence, the Jacknife estimate is obtained by making repeated estimates on increasingly smaller data-sets. This intuition lets us imagine a method which actually makes estimates on larger data-sets (which is the motivation for data augmentation) or, perhaps not so intuitively, on estimates on data-sets of the same size.\nBootstrap Estimates Continuing with the same notation, we will note that the bootstrap is obtained by draw random data-sets with replacement from the training data, where each sample is the same size as the original; as noted by Hastie, Tibshirani, and Friedman (2009).\nWe will consider the bootstrap estimate for the standard deviation of the \\(\\hat{θ}\\) operator, which is denoted by \\(σ(F,n,\\hat{\\theta})=σ(F)\\)\nThe bootstrap is simple the standard deviation at the approximate F, i.e., at \\(F=\\hat{F}\\):\n\\begin{equation} \\hat{\\mathrm{SD}}=\\sigma(\\hat{F}) \\end{equation}\nSince we generally have no closed form analytical form for \\(σ(F)\\) we must use a Monte Carlo algorithm:\n Fit a non parametric maximum likelihood estimate (MLE) of F, i.e. \\(\\hat{F}\\) Draw a sample from \\(\\hat{F}\\) and calculate the estimate of \\(\\hat{θ}\\) on that sample, say, \\(\\hat{θ}\\^*\\) Repeat 2 to get multiple (say B) replications of \\(\\hat{θ}\\^*\\)  Now we know that as \\(B→∞\\) then our estimate would match \\(σ(\\hat{F})\\) perfectly, however, since that itself is an estimate of the value we are actually interested in, in practice there is no real point using a very high B value.\nNote that in actual practice we simply use the given training data with repetition and do not actually use an MLE of the approximate true distribution to generate samples. This causes the bootstrap estimate to be unreasonably good, since there is always significant overlap between the training and test samples during the model fit. This is why cross validation demands non-overlapping data partitions.\nConnecting Estimates The somewhat surprising result can be proved when \\(\\hat{θ}=θ(\\hat{F}\\) is a quadratic functional, namely:\n\\begin{equation}\\hat{\\mathrm{Bias}}_{boot}=\\frac{n-1}{n} \\hat{\\mathrm{Bias}}_{jack}\\end{equation}\nIn practice however, we will simply recall that the Jacknife tends to overestimate, and the Bootstrap tends to underestimation.\nBagging Bagging, is motivated by using the bootstrap methodology to improve the estimate or prediction directly, instead of using it as a method to asses the accuracy of an estimate. It is a representative of the so-called parallel ensemble methods where the base learners are generated in parallel. As such, the motivation is to reduce the error by exploiting the independence of base learners (true for mathematically exact bootstrap samples, but not really true in practice).\nMathematically the formulation of Hastie, Tibshirani, and Friedman (2009) establishes a connection between the Bayesian understanding of the bootstrap mean as a posterior average, however, here we will use a more heuristic approach.\nWe have noted above that the bagging process simply involves looking at different samples in differing orders. This has some stark repercussions for tree-based methods, since the trees are grown with a greedy approach.\n Bootstrap samples may cause different trees to be produced This causes a reduction in the variance, especially when not too many samples are considered Averaging, reduces variance while leaving bias unchanged  Practically, these separate trees being averaged allows for varying importance values of the variables to be calculated.\nIn particular, following Hastie, Tibshirani, and Friedman (2009), it is possible to see that the MSE tends to decrease by bagging.\n\\begin{align} E_P[Y-\\hat{f}^*(x)]² \u0026amp; = \u0026amp; E_P[Y-f*{ag}(x)+f^*_{ag}(x)-\\hat{f}^*(x)]² \\\\\n\u0026amp; = \u0026amp; E_P[Y-f^*_{ag}(x)]²+E_P[\\hat{f}^*(x)-f^*_{ag}(x)]² ≥ E_P[Y-f^*_{ag}(x)]² \\end{align}\nWhere:\n The training observations are independently drawn from a distribution \\(P\\) \\(f_{ag}(x)=E_P\\hat{f}\\^*(x)\\) is the ideal aggregate estimator  For the formulation above, we assume that \\(f_{ag}\\) is a true bagging estimate, which draws samples from the actual population. The upper bound is obtained from the variance of the \\(\\hat{f}\\^*(x)\\) around the mean, \\(f_{ag}\\)\nPractically, we should note the following:\n The regression trees are deep The greedy algorithm growing the trees cause them to be unstable (sensitive to changes in input data) Each tree has a high variance, and low bias Averaging these trees reduces the variance  Missing from the discussion above is how exactly the training and test sets are used in a bagging algorithm, as well as an estimate for the error for each base learner. This has been reported in the code above as the OOB error, or out of bag error. We have, as noted by Zhou (2012) and Breiman (1996) the following considerations.\n Given \\(m\\) training samples, the probability that the iᵗʰ sample is selected 0,1,2\u0026hellip; times is approximately Poisson distributed with \\(λ=1\\) The probability of the iᵗʰ example will occur at least once is then \\(1-(1/e)≈0.632\\) This means for each base learner, there are around \\(36.8\\) % original training samples which have not been used in its training process  The goodness can thus be estimated using these OOB error, which is simply an estimate of the error of the base tree on the OOB samples.\nAs a final note, random forests are conceptually easily understood by combining bagging with subspace sampling, which is why in most cases and packages, we used bagging as a special case of random forests, i.e. when no subspace sampling is performed, random forests algorithms perform bagging.\nReferences   Breiman, Leo. 1996. \u0026ldquo;Bagging Predictors.\u0026rdquo; Machine Learning 24 (2): 123\u0026ndash;40. https://doi.org/10.1023/A:1018054314350.\n\n Cichosz, Pawel. 2015. Data Mining Algorithms: Explained Using R. Chichester, West Sussex ; Malden, MA: John Wiley \u0026amp; Sons Inc.\n\n Efron, Bradley. 1982. The Jackknife, the Bootstrap, and Other Resampling Plans. CBMS-NSF Regional Conference Series in Applied Mathematics 38. Philadelphia, Pa: Society for Industrial and Applied Mathematics.\n\n Hastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer.\n\n James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7.\n\n Zhou, Zhi-Hua. 2012. Ensemble Methods: Foundations and Algorithms. 0th ed. Chapman and Hall/CRC. https://doi.org/10.1201/b12207.\n\n\n This is mostly true for reasonably smooth true functions [return]   ","permalink":"https://rgoswami.me/posts/trees-and-bags/","tags":["theory","statistics","math"],"title":"Trees and Bags"},{"categories":["notes"],"contents":" Background  I have had a lot of discussions regarding the teaching of git This is mostly as a part of the SoftwareCarpentries, or in view of my involvement with univ.ai, or simply in every public space I am associated with Without getting into my views, I just wanted to keep this resource in mind  The site  Learning git is a highly contentious thing People seem to be fond of GUI tools, especially since on non *nix systems, it seems that there is a lot of debate surrounding obtaining the git utility in the first place  One of the best ways of understanding (without installing stuff) the mental models required for working with git is this site\n  Figure 1: A screenshot of the site\n   However, as is clear, this is not exactly a replacement for a good old command-line.\n It does make for a good resource for teaching with slides, or for generating other static visualizations, where live coding is not an option\n  ","permalink":"https://rgoswami.me/posts/d3git/","tags":["tools","rationale","workflow","ideas"],"title":"D3 for Git"},{"categories":["notes"],"contents":" Background Sometime this year, I realized that I no longer have access to a lot of my older communication. This included, a lot of resources I enjoyed and shared with the people who were around me at that point in time. To counter this, I have decided to opt for shorter posts, even if they don\u0026rsquo;t always include the same level of detail I would prefer to provide.\nAlternatives  I have an automated system based around IFTTT combined with Twitter, Diigo, and even Pocket However, that doesn\u0026rsquo;t really tell me much, and trawling through a massive glut of data is often pointless as well There\u0026rsquo;s always Twitter, but I don\u0026rsquo;t really care to hear the views of others when I want to revisit my own ideas  Conclusions  I will be making shorter posts here, like the random one on octobox  ","permalink":"https://rgoswami.me/posts/shortpost/","tags":["tools","rationale","workflow","ideas"],"title":"Shorter Posts"},{"categories":["programming"],"contents":" Background My dotfiles turned 4 years old a few months ago (since 9th Jan 2017) and remains one of my most frequently updated projects for obvious reasons. Going through the changes reminds me of a whole of posts I never got around to writing.\nAnyway, recently I gained access to another HPC cluster, with a standard configuration (bash, old CentOS) and decided to track my provisioning steps. This is really a very streamlined experience by now, since I\u0026rsquo;ve used the same setup across scores of machines. This is actually also a generic intro to configuring user setups on HPC (high performance cluster) machines, if one is inclined to read it in that manner. To that end, sections of this post involve restrictions relating to user privileges which aren\u0026rsquo;t normally part of most Dotfile setups.\nAside  Dotfiles define most people who maintain them No two sets are ever exactly alike They fall somewhere between winging it for each machine and using something like Chef or Ansible Tracking dotfiles is really close to having a sort of out-of-context journal  Before I settled on using the fabulous dotgit, I considered several alternatives, most notably GNU stow.\nPreliminaries It is important to note the environment into which I had to get my setup.\nSSH Setup  The very first thing to do is to use a new ssh-key   export myKey=\u0026#34;someName\u0026#34; ssh-keygen -f $HOME/.ssh/$myKey # I normally don\u0026#39;t set a password ssh-add $HOME/.ssh/$myKey ssh-copy-id $myHPC # myHPC being an IP address I more often than not tend to back this up with a cutesy alias, also because I do not always get my username of choice on these machines. So in $HOME/.ssh/config I use:\nHost myHPC Hostname 127.0.0.1 User somethingIgot IdentityFile ~/.ssh/myKey Harvesting Information  I normally use neofetch on new machines   mkdir -p $HOME/Git/Github cd $HOME/Git/Github git clone https://github.com/dylanaraps/neofetch.git cd neofetch ./neofetch   Figure 1: Neofetch Output\n  Where the top has been tastefully truncated. Just for context, the latest bash as of this writing is v5.0.16 so, that\u0026rsquo;s not too bad, given that neofetch works for bash ≥ 3.2\nTODO Circumventing User Restrictions with Nix  A post in and of itself would be required to explain why and how users are normally restricted from activities in cluster nodes Here, we leverage the nix-package management system to circumvent these User installation of nix is sadly non-trivial, so this might be of some use\u0026nbsp;1  Testing nix-user-chroot  We will first check namespace support   # Errored out unshare --user --pid echo YES # Worked! zgrep CONFIG_USER_NS /boot/config-$(uname -r) # CONFIG_USER_NS=y Thankfully we have support for namespaces, so we can continue with nix-user-chroot.\n Since we definitely do not have rustup or rustc on the HPC, we will use a prebuilt binary of nix-user-chroot   cd $HOME \u0026amp;\u0026amp; wget -O nix-user-chroot https://github.com/nix-community/nix-user-chroot/releases/download/1.0.2/nix-user-chroot-bin-1.0.2-x86_64-unknown-linux-musl  Similar to the wiki example, we will use $HOME/.nix   cd ~/ chmod +x nix-user-chroot mkdir -m 0755 ~/.nix ./nix-user-chroot ~/.nix bash -c \u0026#39;curl https://nixos.org/nix/install | sh\u0026#39;  Only, this doesn\u0026rsquo;t work  Turns out that since unshare is too old, nix-user-chroot won\u0026rsquo;t work either.\nUsing PRoot PRoot is pretty neat in general, they even have a nice website describing it.\n Set a folder up for local installations (this is normally done by my Dotfiles, but we might as well have one here too)   mkdir -p $HOME/.local/bin export PATH=$PATH:$HOME/.local/bin  Get a binary from the GitLab artifacts   cd $HOME mkdir tmp cd tmp wget -O artifacts.zip https://gitlab.com/proot/proot/-/jobs/452350181/artifacts/download unzip artifacts.zip mv dist/proot $HOME/.local/bin  Bind and install nix   mkdir ~/.nix export PROOT_NO_SECCOMP=1 proot -b ~/.nix:/nix export PROOT_NO_SECCOMP=1 curl https://nixos.org/nix/install | sh If you\u0026rsquo;re very unlucky, like I was, you may be greeted by a lovely little error message along the lines of:\n/nix/store/ddmmzn4ggz1f66lwxjy64n89864yj9w9-nix-2.3.3/bin/nix-store: /opt/ohpc/pub/compiler/gcc/5.4.0/lib64/libstdc++.so.6: version `GLIBCXX_3.4.22\u0026#39; not found (required by /nix/store/c0b76xh2za9r9r4b0g3iv4x2lkw1zzcn-aws-sdk-cpp-1.7.90/lib/libaws-cpp-sdk-core.so) Which basically is as bad as it sounds. At this stage, we need a newer compiler to even get nix up and running, but can\u0026rsquo;t without getting an OS update. This chicken and egg situation calls for the drastic measure of leveraging brew first2.\nsh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)\u0026#34; Note that nothing in this section suggests the best way is not to lobby your sys-admin to install nix system-wide in multi-user mode.\nGiving Up with Linuxbrew  Somewhere around this point, linuxbrew is a good idea More on this later  Shell Stuff zsh is my shell of choice, and is what my Dotfiles expect and work best with.\n I did end up making a quick change to update the dotfiles with a target which includes a snippet to transition to zsh from the default bash shell  Dotfiles The actual installation steps basically tracks the readme instructions.\ngit clone https://github.com/kobus-v-schoor/dotgit.git mkdir -p ~/.bin cp -r dotgit/bin/dotgit* ~/.bin cat dotgit/bin/bash_completion \u0026gt;\u0026gt; ~/.bash_completion rm -rf dotgit # echo \u0026#39;export PATH=\u0026#34;$PATH:$HOME/.bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export PATH=\u0026#34;$PATH:$HOME/.bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc Much of this section is directly adapted from the NixOS wiki [return] This used to be called linuxbrew, but the new site makes it clear that it\u0026rsquo;s all one brew now. [return]   ","permalink":"https://rgoswami.me/posts/prov-dots/","tags":["workflow","projects","hpc"],"title":"Provisioning Dotfiles on an HPC"},{"categories":["personal"],"contents":" Background I just realized that it has been over two years since I switched from QWERTY to Colemak but somehow never managed to write about it. It was a major change in my life, and it took forever to get acclimatized to. I do not think I\u0026rsquo;ll ever again be in a position to make such a change in my life again, but it was definitely worth it.\nTouch Typing My interest in touch typing in I decided to digitize my notes for posterity, during the last two years of my undergraduate studies back in Harcourt Butler Technical Institute (HBTI) Kanpur, India. in one of my many instances of yak shaving, I realized I could probably consume and annotate a lot more content by typing faster. Given that at that stage I was already a fast talker, it seemed like a natural extension. There was probably an element of nostalgia involved as well. That and the end of a bachelors involves the thesis, which generally involves a lot of typing.\nThere were (and are) some fantastic resources for learning to touch type nowadays, I personally used:\n Typing.com This is short, but a pretty good basic setup. The numbering and special characters are a bit much to take in at the level of practice you get by completing all the previous exercises, but eventually they make for a good workout. TypingClub This is what I ended up working my way through. It is comprehensive, beautiful, and fun.  Also, later, I ended up using keybr a lot, simply because typing gibberish is a good way of practicing, and it is independent of the keyboard layout.\nJust to foreshadow things, the enemy facing me at this point was the layout itself1.\n  Alternate layouts Having finally broken into the giddy regimes of 150+ wpm, I was ecstatic, and decided to start working my way through some longer reports. However, I quickly realized I was unable to type for more than a couple of minutes without getting terribly cramped. Once it got to the point of having to visit a physiotherapist, I had to call it quits. At that stage, relearning the entire touch typing corpus, given that I already was used to QWERTY, seemed pretty bleak.\nIt took forever, and I ended up applying my choices to my phone keyboard as well, which presumably helped me in terms of increasing familiarity, had the unintended effect of making me seem distant to people I was close to, since my verbose texts suddenly devolved to painful one-liners.\nThe alternative layouts I tried were:\n DVORAK At the time, TypingClub only supported QWERTY and DVORAK, so it was pretty natural for me to try it out. There are also some very nice comics about it. I remember that it was pretty neat, with a good even distribution, until I tried coding. The placement of the semicolons make it impossible to use while programming. I would still say it makes for a comfortable layout, as long as special characters are not required.     CarpalX I experimented with the entire carpalx family, but I was unable to get used to it. I liked QFMLWY best. I do recommend reading the training methodology, especially if anyone is interested in numerical optimization in general. More importantly, though it was relatively easy to set up on my devices and operating systems, the fact that it wasn\u0026rsquo;t natively supported meant a lot of grief whenever I inevitably had to use a public computer.     Colemak Eventually I decided to go with Colemak, especially since it is widely available. Nothing is easier than setxkbmap us -variant colemak -option grp:alt_shift_toggle on public machines and it\u0026rsquo;s easy on Windows as well. Colemak seems like a good compromise. I personally have not been able to reach the same speeds I managed with QWERTY, even after a year, but then again, I can be a lot more consistent, and it hurts less. Nowadays, Colemak has made its way onto most typing sites as well, including TypingClub    What about VIM?  DVORAK makes it impossible, so do most other layouts, but there are some tutorials purporting to help use vim movement with DVORAK Colemak isn\u0026rsquo;t any better, but the fact of the matter is that once you know VIM on QWERTY, and have separately internalized colemak or something else, hitting keys is just hitting keys  All that said, I still occasionally simply remap HJKL (QWERTY movement) to HNEI (Colemak analog) when it is feasible. update: I actually ended up refactoring my entire Dotfiles to use more Colemak native bindings, as described in this post.\nConclusion Changing layouts was a real struggle. Watching my WPM drop back to lower than hunt and peck styles was pretty humiliating, especially since the reports kept coming in, and more than once I switched to QWERTY. However, since then, I have managed to stay on course. I guess if I think about it, it boils down to a few scattered thoughts:\n Typing is kinda like running a marathon, knowing how it is done and doing it are two different things Tell everyone, so people can listen to you lament your reduced speed and not hate you for replying slowly Practice everyday, because, well, it works out in the long run, even when you plateau Alternate shifts! That\u0026rsquo;s really something which should show up more in tutorials, especially for listicles, not changing the shifts will really hurt Try and get a mechanical keyboard (like the Anne Pro 2 or the Coolermaster Masterkeys), they\u0026rsquo;re fun and easy to change layouts on   The images are from here, where there\u0026rsquo;s also an effort based metric used to score keyboard layouts. [return]   ","permalink":"https://rgoswami.me/posts/colemak-switch/","tags":["workflow","explanations"],"title":"Switching to Colemak"},{"categories":["personal"],"contents":" Background For a while I was worried about writing about a TV show here. I thought it might be frivolous, or worse, might outweigh the other kinds of articles I would like to write. However, like most things, that which is ignored just grows, so it is easier to just write and forget about it.\nThe Show Much has been said about how Bojack Horseman is one of the best shows ever, and they\u0026rsquo;re all correct. For that matter I won\u0026rsquo;t be going into the details of how every episode ties together a tapestry of lives in a meaningful way, or any of that. The show was amazingly poignant. The characters felt real. Which actually leads me to the real issue.\nThe End The end of Bojack was good. It was the way it was meant to be. For a slice-of-life show, it is a natural conclusion. It isn\u0026rsquo;t necessary that any catharsis occurs or that the characters change or become better or all that jazz. It isn\u0026rsquo;t about giving the viewers closure. It is simply about a window onto the lives of (fictional) characters being shut. To that end, I disliked attempts to bring closure in the show itself.\nOne of the main reasons why I felt strongly enough to write this, is simply because when I looked around, the prevailing opinion was that the main character should have been killed off, for his sins. This strikes me as a very flippant attitude to take. It reeks of people trying to make the show a cautionary tale, which is frankly speaking a weird approach to take towards any fictional story. The idea that the character should be redeemed also seemed equally weak, for much the same reasons.\nThe fact that the characters are hypocrites, and that none of them are as good or bad as they make themselves out to be is one of the best parts of the show.\nConclusion That\u0026rsquo;s actually all I have to say about this. I thought of adding relevant memes or listing episodes or name dropping sites, but this isn\u0026rsquo;t buzzfeed. The show is incredible, and there are far better ways of proving that. Bust out your favorite search engine + streaming content provider / digital piracy eye-patch and give it a whirl. The only thing I\u0026rsquo;d suggest is watching everything in order, it\u0026rsquo;s just that kind of show.\n","permalink":"https://rgoswami.me/posts/bojack-horseman/","tags":["thoughts","random","review","TV"],"title":"Bojack Horseman"},{"categories":["personal"],"contents":" Background  Pandora doesn\u0026rsquo;t work outside the states I keep forgetting how to set-up proxychains  Proxychains Technically this article expects proxychains-ng, which seems to be the more up-to-date fork of the original proxychains.\n Install proxychains-ng\n# I am on archlinux.. sudo pacman -S proxychains-ng Copy the configuration to the $HOME directory\ncp /etc/proxychains.conf . Edit said configuration to add some US-based proxy\n  In my particular case, I don\u0026rsquo;t keep the tor section enabled.\ntail $HOME/proxychains.conf# # proxy types: http, socks4, socks5 # ( auth types supported: \u0026#34;basic\u0026#34;-http \u0026#34;user/pass\u0026#34;-socks ) # [ProxyList] # add proxy here ... # meanwile # defaults set to \u0026#34;tor\u0026#34; # socks4 127.0.0.1 9050 I actually use Windscribe for my VPN needs, and they have a neat SOCKS5 proxy setup. This works out to a line like socks5 $IP $PORT $USERNAME $PASS being added. The default generator gives you a pretty server name, but to get the IP I use ping $SERVER and put that in the conf file.\nPandora I use the excellent pianobar frontend.\n Get pianobar\nsudo pacman -S pianobar Use it with proxychains\nproxychains pianobar Profit\n  I also like setting up some defaults to make life easier:\nmkdir -p ~/.config/pianobar vim ~/.config/pianobar/config I normally set the following (inspired by the ArchWiki):\naudio_quality = {high, medium, low} autostart_station = $ID password = \u0026#34;$PASS\u0026#34; user = \u0026#34;$emailID\u0026#34; The autostart_station ID can be obtained by inspecting the terminal output during an initial run. I usually set it to the QuickMix station.\n","permalink":"https://rgoswami.me/posts/pandora-proxychains/","tags":["tools","workflow"],"title":"Pandora and Proxychains"},{"categories":["programming"],"contents":" Background  I dislike Jupyter notebooks (and JupyterHub) a lot EIN is really not much of a solution either  In the past I have written some posts on TeX with JupyterHub and discussed ways to use virtual Python with JupyterHub in a more reasonable manner.\nHowever, I personally found that EIN was a huge pain to work with, and I mostly ended up working with the web-interface anyway.\nIt is a bit redundant to do so, given that at-least for my purposes, the end result was a LaTeX document. Breaking down the rest of my requirements went a bit like this:\n What exports well to TeX? Org, Markdown, anything which goes into pandoc What displays code really well? LaTeX, Markdown, Org What allows easy visualization of code snippets? Rmarkdown, RStudio, JupyterHub, Org with babel  Clearly, orgmode is the common denominator, and ergo, a perfect JupyterHub alternative.\nSetup Throughout this post I will assume the following structure:\ntree tmp mkdir -p tmp/images touch tmp/myFakeJupyter.org    tmp        ├── images     └── myFakeJupyter.org     1 directory, 1 file    As is evident, we have a folder tmp which will have all the things we need for dealing with our setup.\nVirtual Python Without waxing too eloquent on the whole reason behind doing this, since I will rant about virtual python management systems elsewhere, here I will simply describe my preferred method, which is using poetry.\n# In a folder above tmp poetry init poetry add numpy matplotlib scipy pandas The next part is optional, but a good idea if you figure out using direnv and have configured layout_poetry as described here:\n# Same place as the poetry files echo \u0026#34;layout_poetry()\u0026#34; \u0026gt;\u0026gt; .envrc Note:\n We can nest an arbitrary number of the tmp structures under a single place we define the poetry setup I prefer using direnv to ensure that I never forget to hook into the right environment  Orgmode This is not an introduction to org, however in particular, there are some basic settings to keep in mind to make sure the set-up works as expected.\nIndentation Python is notoriously weird about whitespace, so we will ensure that our export process does not mangle whitespace and offend the python interpreter. We will have the following line at the top of our orgmode file:\n# -*- org-src-preserve-indentation: t; org-edit-src-content: 0; -*- Note:\n this post is actually generating the file being discussed here by  tangling the file\n You can get the whole file here  TeX Settings These are also basically optional, but at the very least you will need the following:\n#+author: Rohit Goswami #+title: Whatever #+subtitle: Wittier line about whatever #+date: \\today #+OPTIONS: toc:nil I actually use a lot of math using the TeX input mode in Emacs, so I like the following settings for math:\n# For math display #+LATEX_HEADER: \\usepackage{amsfonts} #+LATEX_HEADER: \\usepackage{unicode-math} There are a bunch of other settings which may be used, but these are the bare minimum, more on that would be in a snippet anyway.\nNote:\n rendering math in the orgmode file in this manner requires that we use XeTeX to compile the final file  Org-Python We essentially need to ensure that:\n Babel uses our virtual python The same session is used for each block  We will get our poetry python pretty easily:\nwhich python Now we will use this as a common header-arg passed into the property drawer to make sure we don\u0026rsquo;t need to set them in every code block.\nWe can use the following structure in our file:\n\\* Python Stuff :PROPERTIES: :header-args: :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both :END: Now we can simply work with code as we normally would \\#+BEGIN_SRC python print(\u0026#34;Hello World\u0026#34;) \\#+END_SRC Note:\n For some reason, this property needs to be set on every heading (as of Feb 13 2020) In the actual file you will want to remove extraneous \\ symbols:  \\* → * \\#+BEGIN_SRC → #+BEGIN_SRC \\#+END_SRC → #+END_SRC   Python Images and Orgmode To view images in orgmode as we would in a JupyterLab notebook, we will use a slight trick.\n We will ensure that the code block returns a file object with the arguments The code block should end with a print statement to actually generate the file name\nSo we want a code block like this:\n   #+BEGIN_SRC python :results output file :exports both import matplotlib.pyplot as plt from sklearn.datasets.samples_generator import make_circles X, y = make_circles(100, factor=.1, noise=.1) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\u0026#39;autumn\u0026#39;) plt.xlabel(\u0026#39;x1\u0026#39;) plt.ylabel(\u0026#39;x2\u0026#39;) plt.savefig(\u0026#39;images/plotCircles.png\u0026#39;, dpi = 300) print(\u0026#39;images/plotCircles.png\u0026#39;) # return filename to org-mode #+end_src Which would give the following when executed:\n#+RESULTS: [[file:images/plotCircles.png]] Since that looks pretty ugly, this will actually look like this:\nimport matplotlib.pyplot as plt from sklearn.datasets.samples_generator import make_circles X, y = make_circles(100, factor=.1, noise=.1) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\u0026#39;autumn\u0026#39;) plt.xlabel(\u0026#39;x1\u0026#39;) plt.ylabel(\u0026#39;x2\u0026#39;) plt.savefig(\u0026#39;images/plotCircles.png\u0026#39;, dpi = 300) print(\u0026#39;images/plotCircles.png\u0026#39;) # return filename to org-mode   Bonus A better way to simulate standard jupyter workflows is to just specify the properties once at the beginning.\n#+PROPERTY: header-args:python :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both This setup circumvents having to set the properties per sub-tree, though for very large projects, it is useful to use different processes.\nConclusions  The last step is of course to export the file as to a TeX file and then compile that with something like latexmk -pdfxe -shell-escape file.tex  There are a million and one variations of this of course, but this is enough to get started.\nThe whole file is also reproduced here.\n","permalink":"https://rgoswami.me/posts/jupyter-orgmode/","tags":["tools","emacs","workflow","orgmode"],"title":"Replacing Jupyter with Orgmode"},{"categories":["programming"],"contents":" Background  I end up writing about using poetry a lot I almost always use direnv in real life too I don\u0026rsquo;t keep writing mini scripts in my .envrc  Honestly there\u0026rsquo;s nothing here anyone using the direnv wiki will find surprising, but then it is still neat to link back to.\nSetting Up Poetry This essentially works by simply modifying the global .direnvrc which essentially gets sourced by every local .envrc anyway.\nvim $HOME/.direnvrc So what we put in there is the following snippet derived from other snippets on the wiki, and is actually now there too.\n# PUT this here layout_poetry() { if [[ ! -f pyproject.toml ]]; then log_error \u0026#39;No pyproject.toml found. Use `poetry new` or `poetry init` to create one first.\u0026#39; exit 2 fi local VENV=$(dirname $(poetry run which python)) export VIRTUAL_ENV=$(echo \u0026#34;$VENV\u0026#34; | rev | cut -d\u0026#39;/\u0026#39; -f2- | rev) export POETRY_ACTIVE=1 PATH_add \u0026#34;$VENV\u0026#34; } Now we can just make .envrc files with layout_poetry and everything will just work™.\n","permalink":"https://rgoswami.me/posts/poetry-direnv/","tags":["tools","direnv","workflow","python"],"title":"Poetry and Direnv"},{"categories":["notes"],"contents":" Background As a member of several large organizations, I get a lot of github notifications. Not all of these are of relevance to me. This is especially true of psuedo-monorepo style repositories like the JOSS review system and especially the exercism community.\n I recently (re-)joined the exercism community as a maintainer for the C++ lessons after having been a (sporadic) teacher This was largely in response to a community call to action as the group needed new blood to usher in v3 of the exercism project  Anyway, I have since found that at the small cost of possibly much of my public repo data, I can manage my notifications better with Octobox\nOctobox  It appears to be free for now It syncs on demand (useful) I can search things quite easily They have a neat logo There appear to be many features I probably won\u0026rsquo;t use  It looks like this:\n  Figure 1: Octobox Stock Photo\n  ","permalink":"https://rgoswami.me/posts/ghnotif/","tags":["tools","github","workflow"],"title":"Taming Github Notifications"},{"categories":["personal"],"contents":" Why this site exists I have a lot of online presences. I have been around (or at-least, lurking) for over ten years. Almost as long as I have been programming. Anyway, I have a penchant lately for using emacs and honestly there isn\u0026rsquo;t very good support for org-mode files. There are options recently with gatsby as well, but this seemed kinda neat.\nWhat \u0026lsquo;this\u0026rsquo; is  This site is built by Hugo The posts are generated with ox-hugo The theme is based of this excellent one by Djordje Atlialp, which in turn is based off of this theme by panr  My modifications are here   What is here  Mostly random thoughts I don\u0026rsquo;t mind people knowing Some tech stuff which isn\u0026rsquo;t coherent enough to be put in any form with references Emacs specific workflows which I might want to write about more than short notes on the config  What isn\u0026rsquo;t here  Some collections should and will go to my grimoire My doom-emacs configuration Academic stuff is better tracked on Publons or Google Scholar or my pages hosted by my favorite IITK group or UI group  ","permalink":"https://rgoswami.me/posts/rationale/","tags":["ramblings","explanations"],"title":"Site Rationale"},{"categories":null,"contents":" Hi.\nI\u0026rsquo;m Rohit Goswami, better known across the web as HaoZeke. I\u0026rsquo;m not the first of my name, which is why instead of rgoswami, I occasionally use rg0swami when I need to be identified by something closer to my name.\nThe actual username is a throwback to back when people liked being anonymous (and with multiple personalities) online, so that ought to give an idea of how old I am. A curriculum vitae is available here.\nIt is difficult to keep this section short and not let it spill into an unstructured memoir. For a while I considered trying to consolidate my online presences but that turned out to be completely impossible without a series of posts and avatars1. I did however eventually set up a sporadically updated collection of web-links involving me.\nIntangible Positions This is a set of things which are primarily online and/or voluntary in a non-academic sense.\n I administer and design a bunch of websites, mostly verified on Keybase I am a certified Software Carpentries instructor I officially maintain, for the Software Carpentries, the lesson on R (r-novice-inflammation) I also maintain some packages on the AUR (ArchLinux User Repository) I hone coursework development and teaching with univ.ai I maintain(ed) the official LineageOS image for the Xperia Z5 Dual  Historical Places What follows is a more informal set of places I am or have been associated with or are of significance to me2.\nReykjavík  I am associated with the reputed Jonsson group of the Science Institute at the University of Iceland, where I benefit from the guidance of the erudite and inspiring Prof. Hannes Jonsson My doctoral committee is here, which includes the very excellent inputs of Dr. Elvar Jonsson I have also benefited from sitting in on some formal coursework here, which has been a fascinatingly useful experience  Kanpur  I retain a close association with the fantastic Femtolab at IIT Kanpur under Prof. Debabrata Goswami, who has provided constant guidance throughout my career I am the co-lead developer of the FOSS scientific d-SEAMS software suite for graph theoretic approaches to structure determination of molecular dynamics simulations, along with my exceptional co-lead Amrita Goswami of the CNS Lab under Prof. Jayant K. Singh at IITK I worked with the Nair group as part of the Summer Undergraduate in Research Excellence (SURGE) program, also at IITK Harcourt Butler Technical Institute (HBTI) Kanpur, or the Harcourt Butler Technological University, as it is now called, was where I trained to be a chemical engineer  Bombay  I spent a formative summer under Prof. Rajarshi Chakrabarti of the IIT Bombay Chemistry department, who has been instrumental in developing my interests I also spent some time discussing experiments with Prof. Rajdip Bandyopadhyaya of the IIT Bombay Chemical Engineering department during an industrial internship in fragnance compounding at the R\u0026amp;D department of KEVA Ltd. under Dr. Debojit Chakrabarty  Bangalore  At IISc, I had the good fortune to meet Prof. Hannes Jonsson at a summer workshop on Rare events At the BIC, I undertook formal machine learning and artificial intelligence training under Harvard\u0026rsquo;s Dr. Rahul Dave and Dr. Pavlos Protopapas as part of the univ.ai summer course  Chennai  I spent a very fruitful summer on quantum tomography under Prof. Sibashish Ghosh at the Institute for Mathematical Sciences (IMSc Chennai)  Avatars I thought it might be of use to list a few of my more official visages. This is mostly to ensure people do not confuse me with a Sasquatch3. These mugshots are exactly that, mugshots for profile icons4.\n  Figure 1: A collage of mugshots, shuffled and not ordered by date to confuse people trying to kill me\n  Donations If you\u0026rsquo;ve gotten this far, you might also want to check out the following\u0026nbsp;5:\n Patreon Librepay   I didn\u0026rsquo;t think it would be necessary, but just in case it isn\u0026rsquo;t clear, people listed here are not necessarily all references or anything, this is a personal list of people associated with each city, not a cover letter [return] I grew up on the verdant and beautiful TIFR Mumbai campus, and completed high school and undergraduate stuff while playing with peacocks and things on the IIT Kanpur campus [return] This is not a replacement for an Instagram feed or a Facebook wall, or even a ResearchGate or Publons or ORCID page; all of which I do sporadically remember I have [return] Made with the Mountain Tapir Collage Maker [return] There won\u0026rsquo;t ever be any content behind paywalls though [return]   ","permalink":"https://rgoswami.me/about/","tags":null,"title":"About"},{"categories":["personal"],"contents":"  An attempt to re-claim and verify my digital presence.\n Background I mentioned on my about page, that it is nigh impossible to keep track of every digital trace there is of me. That said, it is really not even a countable infinite set yet, so it is a good idea to get started before it gets much worse. This is minimally curated, and will only be sporadically updated, so take everything here with a grain of salt. I honestly have no idea why anyone who is not me would like to see this, other than to prove one of these with respect to the rest1.\nProfiles Professional  University of Iceland FemtoLab  Voluntary  The Carpentries  Instructor, Maintainer  IGDORE Univ.ai  Academic  Publons PeerJ Google Scholar OrCiD Loop OSF  Societies  American Institute of Chemical Engineers IEEE  Communities  Figshare  Misc  Github Gitlab Goodreads Keybase  Pages and Articles By Me  Everything on any of my many websites Write-up on research with a ChemE undergraduate degree for the American Institute of Chemical Engineers (AIChE) Young Professionals Committee (YPC) Hackernoon article on Locking and Encrypting Apps with Encfs  Mentioning Me Lists  Mathematical Methods of Modern Statistics 2 (info) Gallery of RARE 2019 at IISc 2020 MegaCodeRefinery (helper) Probablistic Data Analysis (University of Turku)  Quotes  Quoted in a Stanford Daily Article on CS106A Code in Place Fortran Monthly Newsletter (June 2020) Emacs News  2020-05-11: for An Orgmode Note Workflow 2020-05-04: for Pandoc to Orgmode with Babel 2020-04-27: for Using Mathematica with Orgmode 2020-04-06: for Replacing Jupyter with Orgmode   Videos Of Me  Everything on my YouTube channel Discussion session for the CS196A Code in Place AMA with the students  Including Me This category involves recordings where I asked questions, and therefore technically involve me in a sense.\n Code in Place AMA with Stanford CS Lecturers  IAS TML Lecture Questions I\u0026rsquo;ve been sitting in on these for a while thanks to Ke Li, but this section lists the lectures I asked a question in\n \u0026ldquo;What Do Models Learn?\u0026rdquo; by Aleksander Mądry \u0026ldquo;Langevin Dynamics in Machine Learning\u0026rdquo; by Michael Jordan   If you do think you have seen me somewhere not on this list, drop me an email [return]   ","permalink":"https://rgoswami.me/posts/rg-collection-weblinks/","tags":["ramblings"],"title":"Collection of WebLinks"},{"categories":null,"contents":"","permalink":"https://rgoswami.me/search/","tags":null,"title":"Search"}]