[{"categories":["programming"],"contents":" Background I have been wanting to find a workflow which allows me to bypass writing a lot of TeX by hand for a while now. To that end I looked into using a computer algebra system (CAS). Naturally, my first choice was the FOSS Maxima (also because it uses Lisp under the hood). However, for all the reasons listed here, relating to its accuracy, which have not been fixed even though the post was over 5 years ago, I ended up having to go with the closed source Mathematica.\nPackages Support for Mathematica in modern orgmode is mainly through the use of ob-mathematica, which is the official org-babel extension (from contrib) for working with Mathematica. However, ob-mathematica relies on the now-defunct mma package for font-locking, which is less than ideal. Thankfully, there exists the excellent wolfram-mode package which happens to be in MELPA as well. Finally, since the default return type of a mathematica block is an input-string meant to be used in another mathematica block, which is not useful when we work with org-babel, we will use the excellent mash.pl utility from here, as suggested by the ob-mathematica package to sanitize our output and set a unifying path.\nSo to recap, use your favorite manager to get:\n ob-mathematica (in contrib) wolfram-mode (MELPA) mash.pl (from here)  After obtaining the packages, the configuration is then simply\u0026nbsp;1:\n;; Load mathematica from contrib (org-babel-do-load-languages \u0026#39;org-babel-load-languages (append org-babel-load-languages \u0026#39;(mathematica . t) )) ;; Sanitize output and deal with paths (setq org-babel-mathematica-command \u0026#34;~/.local/bin/mash\u0026#34;) ;; Font-locking (add-to-list \u0026#39;org-src-lang-modes \u0026#39;(\u0026#34;mathematica\u0026#34; . wolfram)) ;; For wolfram-mode (setq mathematica-command-line \u0026#34;~/.local/bin/mash\u0026#34;) Results LaTeX Now we are in a position to simply evaluate content with font-locking. We will test our set up with an example lifted from the ob-mathematica source-code.\n Table 1: A table \n   1 4     2 4   3 6   4 8   7 0    (1+Transpose@x) // TeXForm Where our header-line (with #+begin_src) is:\nmathematica :var x=example-table :results latex Sanity Checks We can also test the example from the blog post earlier to test basic mathematical sanity.\nLimit[Log[b - a + I eta], eta -\u0026gt; 0, Direction -\u0026gt; -1,Assumptions -\u0026gt; {a \u0026gt; 0, b \u0026gt; 0, a \u0026gt; b}] TeXForm[Limit[Log[b - a + I eta], eta -\u0026gt; 0, Direction -\u0026gt; 1,Assumptions -\u0026gt; {a \u0026gt; 0, b \u0026gt; 0, a \u0026gt; b}]] \\((I*Pi + Log[a - b])*\\log (a-b)-i \\pi\\)\nInline Math Note that we can now also write fractions, integrals and other cumbersome TeX objects a lot faster with this syntax, like \\(\\frac{x^3}{3}\\). Where we are using the following snippet:\nsrc_mathematica[:exports none :results raw]{Integrate[x^2,x] // TeXForm} Plots For plots, the standard orgmode rules apply, that is, we have to export to a file and return the name through our code snippet. Consider:\np=Plot[Sin[x], {x, 0, 6 Pi},Frame-\u0026gt;True]; Export[\u0026#34;images/sine.png\u0026#34;,p]; Print[\u0026#34;images/sine.png\u0026#34;] Figure 1: An exported Mathematica image  Where we have used mathematica :results file as our header line.\n For reference, my whole config is here [return]   ","permalink":"https://rgoswami.me/posts/org-mathematica/","tags":["tools","emacs","workflow","orgmode"],"title":"Using Mathematica with Orgmode"},{"categories":["notes"],"contents":" Background As I mentioned earlier, I\u0026rsquo;m leading a section for Stanford CS106A: Code in Place. This post relates to the notes and thoughts garnered during the small group training session\u0026nbsp;1.\nReflections Demographics Redacted. Did not use breakout meetings due to privacy issues.\nEngagement and Participation  Some people were more active (skewed responses) Some of the more rudimentary questions might have been suppressed  Highlighted Moments  Covering multiple perspectives Different mental models  Challenges and Transformations  Technical debt was an issue Lack of engagement Went on for too long  For me in particular:\n It took over two hours, and though most people stayed on, not everyone was engaged.\n Scenarios These are to be dealt with as per the guidelines here. Since different groups covered different scenarios, not all of these have answers here.\nEnsuring Engagement  You have some students who didn\u0026rsquo;t participate at all in the section. What do you do?\n Effective Communication  What might not be effective about the policy, “Students should just tell me if I say something that offends them”?\n Sharing Experiences  You just finished your section and are staying behind to answer questions from your students. A couple students asked what it’s like studying/working in an engineering/tech field.\nWhat things might you want to keep in mind when answering their questions?\n Time Management  Section went way over time due to lots of questions being asked by students. What are some time management strategies you can use moving forward?\n Homework Assists  A sectionee posts in your Ed group, “I am a little bit frustrated because I don\u0026rsquo;t really know where to start on the first assignment. A little hint would be very helpful.” How do you respond?\n Debugging  A sectionee shows you the following buggy code for printing all the elements in a list:\nmy_lst = [\u0026lsquo;apple\u0026rsquo;, \u0026lsquo;banana\u0026rsquo;, \u0026lsquo;carrot\u0026rsquo;] i = 0 while len(my_lst) \u0026gt; 0: print(my_lst[i]) i = i + 1\nThey explain that the code works (it prints all the elements in the right order) but then throws a weird error: “IndexError: list index out of range.” How would you help them find their bug?\n Quitting  You have a student who is already discouraged by how difficult the first assignment is and has told you they don’t feel cut out for CS. What do you say to them?\n  Provide encouragement Give examples of hardship faced Be positive and make sure they don’t feel worse, even if they do follow through and quit “You’re not the first” Takes a lot of time. Doesn’t happen overnight Ask them why they don’t feel cut out and try to solve that problem  Looking up issues  Why might it be problematic to say something like, “It’s easy to download X or look up the answer to Y”? Why might those statements not be true?\n  Difficulty in backgrounds (language barriers) They might not be able to understand stackoverflow.com until they learn more CS They might not know where to look online (lack of domain expertise) Dependencies (for downloads) Makes them feel bad if they don’t end up finding it easy   This post was created on the day of training, 21-04-20, but will be posted later [return]   ","permalink":"https://rgoswami.me/posts/scp-smallgrp-trainig/","tags":["teaching","cs106a"],"title":"CS106A Small Group Training"},{"categories":["notes"],"contents":" Background As I mentioned earlier, I\u0026rsquo;m leading a section for Stanford CS106A: Code in Place. I did also mention I\u0026rsquo;d try to keep a set of short notes on the process. I finally had my first section meeting!\nPreparation I went through the following:\n Sent out a welcome message Detailed the workflow Set up a HackMD instance Set up some slides in beamer\u0026nbsp;1  However, after that, I was still concerned since I didn\u0026rsquo;t get much of a response on the ice-breakers for EdStem. Thankfully, everyone showed up.\nTeaching  I had a fabulous session, and we went through a variety of concepts. Didn\u0026rsquo;t spend much time on icebreakers, but did get a feel for where the students stand on the functional vs imperative programming paradigms Possibly because of working through two different approaches, the 40 minute long session went on for two hours and fifteen minutes. Some students had more of a background than the others, thankfully computational thinking is not normally taught very well  Conclusion  The notes are visible here, and the session was recorded here2 It was fun, and I hope the students enjoyed it as much as I did. I will probably expand this in terms of the concepts covered, to give the students more of an overview of what was covered   Even though most of the session was supposed to be live, it was still helpful to show I was interested enough to set up slides [return] As always, advice is much appreciated (and moderated) [return]   ","permalink":"https://rgoswami.me/posts/scp-smallgrp-meet1/","tags":["teaching","cs106a"],"title":"CS106A Section Meeting I"},{"categories":["notes"],"contents":" Background As I mentioned in my last post, I\u0026rsquo;m leading a section for Stanford CS106A: Code in Place. I did also mention I\u0026rsquo;d try to keep a set of short notes on the process. So there1.\nThe Training Given the overwhelming number of students, and section leaders, the small groups are for fostering a community of teachers.\nConsider allowing for daisy chaining during introductions Discussions are the primary take-away Only the instructor should be coding during the session  Core components  Clarity Content Atmosphere Section management Correctness  Sectional Details  Check in at the start Notice the space Check in regularly Avoid negative phrases Establish norms and the general culture  Zoom Norms  Have people introduce themselves Mute people when they aren\u0026rsquo;t talking Raise hands Try to use icebreakers which respect privacy  Materials Here\u0026rsquo;s some of the stuff which, being as it was open-sourced, I suppose is OK to put here2.\n Section Leader Training Section Leaders\u0026rsquo; Guide to Virtual Sections Some Zoom Icebreakers   As you may know, the official playlist is here [return] If you know otherwise, let me know in the comments [return]   ","permalink":"https://rgoswami.me/posts/scp-smallgrp/","tags":["ideas","teaching","cs106a"],"title":"Small Section On-boarding"},{"categories":["notes"],"contents":" Background A few weeks ago, I ended up recording a video for the Stanford CS106A: Code in Place initiative (which can be found here). I heard back a while ago, and am now to lead a section for the course!\nI\u0026rsquo;ll probably be making a series of short posts as this process continues.\nOn-Boarding This was very reminiscent of the Carpentries instructor training, which makes sense, given how well thought out that experience was.\nWe started out with a pre-presentation where people were able to just spitball and connect, which is pretty neat.\nOne of the interesting parts of this, was the idea of interactive recorded lectures, where the professors will be watching lectures with the students. The entire slide deck is here.\nThe other great idea for this kind of long course was the idea of having a Tea room and a Teachers lounge where people can just tune in to chat.\nCaveats A couple of things which keep cropping up for online teaching in general are the following:\n Zoom does not have persistent chats, so an auxiliary tool like an Etherpad is great  ","permalink":"https://rgoswami.me/posts/scp-onboarding/","tags":["ideas","teaching","cs106a"],"title":"On-boarding for Code in Place"},{"categories":["notes"],"contents":" Background Like a lot of my tech based rants, this was brought on by a recent Hacker News post. I won\u0026rsquo;t go into why the product listed there is a hollow faux FOSS rip-off. I won\u0026rsquo;t discuss how that \u0026lsquo;free\u0026rsquo; analytics option, like many others are just hobby projects taking pot shots at other projects. Or how insanely overpriced most alternatives are.\nI will however discuss why and how I transitioned to using the awesome Goat Counter.\nGoogle Analytics I would like to point out that it is OK to start out with Google Analytics. It is easy, and free, and scales well. There are reasons not to, but it is a good starting point.\nPros  Google Analytics is free, truly free The metrics are very detailed It is easy to set up  Cons  Privacy concerns Blocked by people Easy to obsess over metrics  Goat Counter As with most Hacker News posts, the article itself was nothing compared to the excellent comment thread. It was there that I came across people praising Goat Counter.\nPros  Is open sourced (here on Github) Super lightweight Anonymous statistics Easy to share  Cons  Has an upper limit on free accounts (10k a month) I am not very fond of Go  Conclusions I might eventually go back to GA, if I go over the 10k page view limit. Then again, I might not. It might be more like, I only care about the first 10k people who make it to my site.\n","permalink":"https://rgoswami.me/posts/goat-google/","tags":["tools","rationale","workflow","ideas"],"title":"Analytics: Google to Goat"},{"categories":["notes"],"contents":"  Explain why using bagging for prediction trees generally improves predictions over regular prediction trees.\n Introduction Bagging (or Bootstrap Aggregation) is one of the most commonly used ensemble method for improving the prediction of trees. We will broadly follow a historical development trend to understand the process. That is, we will begin by considering the Bootstrap method. This in turn requires knowledge of the Jacknife method, which is understandable from a simple bias variance perspective. Finally we will close out the discussion by considering the utility and trade-offs of the Bagging technique, and will draw attention to the fact that the Bagging method was contrasted to another popular ensemble method, namely the Random Forest method, in the previous section.\nBefore delving into the mathematics, recall that the approach taken by bagging is given as per Cichosz (2015) to be:\n create base models with bootstrap samples of the training set combine models by unweighted voting (for classification) or by averaging (for regression)  The reason for covering the Jacknife method is to develop an intuition relating to the sampling of data described in the following table:\n   Data-set Size per sample Estimator     Reduces Jacknife   Remains the same Bootstrap   Increases data-augmentation    Bias Variance Trade-offs We will recall, for this discussion, the bias variance trade off which is the basis of our model accuracy estimates (for regression) as per the formulation of James et al. (2013).\n\\begin{equation} E(y₀-\\hat{f}(x₀))²=\\mathrm{Var}(\\hat{f}(x₀))+[\\mathrm{Bias(\\hat{f(x₀)})}]²+\\mathrm{Var}(ε) \\end{equation}\nWhere:\n \\(E(y_{0}-\\hat{f}(x_{0}))²\\) is the expected test MSE, or the average test MSE if \\(f\\) is estimated with a large number of training sets and tested at each \\(x₀\\) The variance is the amount by which our approximation \\(\\hat{f}\\) will change if estimated by a different training set, or the flexibility error The bias is the (reducible) approximation error, caused by not fitting to the training set exactly \\(\\mathrm{Var}(ε)\\) is the irreducible error  We will also keep in mind, going forward the following requirements of a good estimator:\n Low variance AND low bias Typically, the variance increases while the bias decreases as we use more flexible methods (i.e. methods which fit the training set better1)  Also for the rest of this section, we will need to recall from Hastie, Tibshirani, and Friedman (2009), that the bias is given by:\n\\begin{equation} [E(\\hat{f_{k}}(x₀)-f(x₀)]² \\end{equation}\nWhere the expectation averages over the randomness in the training data.\nTo keep things in perspective, recall from Hastie, Tibshirani, and Friedman (2009):\nFigure 1: Test and training error as a function of model complexity  Jacknife Estimates We will model our discussion on the work of Efron (1982). Note that:\n The \\(\\hat{θ}\\) symbol is an estimate of the true quantity \\(θ\\) This is defined by the estimate being \\(\\hat{θ}=θ(\\hat{F})\\) \\(\\hat{F}\\) is the empirical probability distribution, defined by mass \\(1/n\\) at \\(xᵢ ∀ i∈I\\), i is from 1 to n  The points above establishes our bias to be given by \\(E_Fθ(\\hat{F})-θ(F)\\) such that \\(E_F\\) is the expectation under x₁⋯xₙ~F.\nTo derive the Jacknife estimate \\((\\tilde{θ})\\) we will simply sequentially delete points xᵢ (changing \\(\\hat{F}\\)), and recompute our estimate \\(\\hat{θ}\\), which then simplifies to:\n\\begin{equation} \\tilde{θ}\\equiv n\\hat{θ}-(\\frac{n-1}{n})∑_{i=1}ⁿ\\hat{θ} \\end{equation}\nIn essence, the Jacknife estimate is obtained by making repeated estimates on increasingly smaller data-sets. This intuition lets us imagine a method which actually makes estimates on larger data-sets (which is the motivation for data augmentation) or, perhaps not so intuitively, on estimates on data-sets of the same size.\nBootstrap Estimates Continuing with the same notation, we will note that the bootstrap is obtained by draw random data-sets with replacement from the training data, where each sample is the same size as the original; as noted by Hastie, Tibshirani, and Friedman (2009).\nWe will consider the bootstrap estimate for the standard deviation of the \\(\\hat{θ}\\) operator, which is denoted by \\(σ(F,n,\\hat{\\theta})=σ(F)\\)\nThe bootstrap is simple the standard deviation at the approximate F, i.e., at \\(F=\\hat{F}\\):\n\\begin{equation} \\hat{\\mathrm{SD}}=\\sigma(\\hat{F}) \\end{equation}\nSince we generally have no closed form analytical form for \\(σ(F)\\) we must use a Monte Carlo algorithm:\n Fit a non parametric maximum likelihood estimate (MLE) of F, i.e. \\(\\hat{F}\\) Draw a sample from \\(\\hat{F}\\) and calculate the estimate of \\(\\hat{θ}\\) on that sample, say, \\(\\hat{θ}\\^*\\) Repeat 2 to get multiple (say B) replications of \\(\\hat{θ}\\^*\\)  Now we know that as \\(B→∞\\) then our estimate would match \\(σ(\\hat{F})\\) perfectly, however, since that itself is an estimate of the value we are actually interested in, in practice there is no real point using a very high B value.\nNote that in actual practice we simply use the given training data with repetition and do not actually use an MLE of the approximate true distribution to generate samples. This causes the bootstrap estimate to be unreasonably good, since there is always significant overlap between the training and test samples during the model fit. This is why cross validation demands non-overlapping data partitions.\nConnecting Estimates The somewhat surprising result can be proved when \\(\\hat{θ}=θ(\\hat{F}\\) is a quadratic functional, namely:\n\\begin{equation}\\hat{\\mathrm{Bias}}_{boot}=\\frac{n-1}{n} \\hat{\\mathrm{Bias}}_{jack}\\end{equation}\nIn practice however, we will simply recall that the Jacknife tends to overestimate, and the Bootstrap tends to underestimation.\nBagging Bagging, is motivated by using the bootstrap methodology to improve the estimate or prediction directly, instead of using it as a method to asses the accuracy of an estimate. It is a representative of the so-called parallel ensemble methods where the base learners are generated in parallel. As such, the motivation is to reduce the error by exploiting the independence of base learners (true for mathematically exact bootstrap samples, but not really true in practice).\nMathematically the formulation of Hastie, Tibshirani, and Friedman (2009) establishes a connection between the Bayesian understanding of the bootstrap mean as a posterior average, however, here we will use a more heuristic approach.\nWe have noted above that the bagging process simply involves looking at different samples in differing orders. This has some stark repercussions for tree-based methods, since the trees are grown with a greedy approach.\n Bootstrap samples may cause different trees to be produced This causes a reduction in the variance, especially when not too many samples are considered Averaging, reduces variance while leaving bias unchanged  Practically, these separate trees being averaged allows for varying importance values of the variables to be calculated.\nIn particular, following Hastie, Tibshirani, and Friedman (2009), it is possible to see that the MSE tends to decrease by bagging.\n\\begin{align} E_P[Y-\\hat{f}^*(x)]² \u0026amp; = \u0026amp; E_P[Y-f*{ag}(x)+f^*_{ag}(x)-\\hat{f}^*(x)]² \\\\\n\u0026amp; = \u0026amp; E_P[Y-f^*_{ag}(x)]²+E_P[\\hat{f}^*(x)-f^*_{ag}(x)]² ≥ E_P[Y-f^*_{ag}(x)]² \\end{align}\nWhere:\n The training observations are independently drawn from a distribution \\(P\\) \\(f_{ag}(x)=E_P\\hat{f}\\^*(x)\\) is the ideal aggregate estimator  For the formulation above, we assume that \\(f_{ag}\\) is a true bagging estimate, which draws samples from the actual population. The upper bound is obtained from the variance of the \\(\\hat{f}\\^*(x)\\) around the mean, \\(f_{ag}\\)\nPractically, we should note the following:\n The regression trees are deep The greedy algorithm growing the trees cause them to be unstable (sensitive to changes in input data) Each tree has a high variance, and low bias Averaging these trees reduces the variance  Missing from the discussion above is how exactly the training and test sets are used in a bagging algorithm, as well as an estimate for the error for each base learner. This has been reported in the code above as the OOB error, or out of bag error. We have, as noted by Zhou (2012) and Breiman (1996) the following considerations.\n Given \\(m\\) training samples, the probability that the iᵗʰ sample is selected 0,1,2\u0026hellip; times is approximately Poisson distributed with \\(λ=1\\) The probability of the iᵗʰ example will occur at least once is then \\(1-(1/e)≈0.632\\) This means for each base learner, there are around \\(36.8\\) % original training samples which have not been used in its training process  The goodness can thus be estimated using these OOB error, which is simply an estimate of the error of the base tree on the OOB samples.\nAs a final note, random forests are conceptually easily understood by combining bagging with subspace sampling, which is why in most cases and packages, we used bagging as a special case of random forests, i.e. when no subspace sampling is performed, random forests algorithms perform bagging.\nReferences   Breiman, Leo. 1996. \u0026ldquo;Bagging Predictors.\u0026rdquo; Machine Learning 24 (2): 123\u0026ndash;40. https://doi.org/10.1023/A:1018054314350.\n\n Cichosz, Pawel. 2015. Data Mining Algorithms: Explained Using R. Chichester, West Sussex ; Malden, MA: John Wiley \u0026amp; Sons Inc.\n\n Efron, Bradley. 1982. The Jackknife, the Bootstrap, and Other Resampling Plans. CBMS-NSF Regional Conference Series in Applied Mathematics 38. Philadelphia, Pa: Society for Industrial and Applied Mathematics.\n\n Hastie, Trevor, Robert Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer Series in Statistics. New York, NY: Springer.\n\n James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 103. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7138-7.\n\n Zhou, Zhi-Hua. 2012. Ensemble Methods: Foundations and Algorithms. 0th ed. Chapman and Hall/CRC. https://doi.org/10.1201/b12207.\n\n\n This is mostly true for reasonably smooth true functions [return]   ","permalink":"https://rgoswami.me/posts/trees-and-bags/","tags":["theory","statistics"],"title":"Trees and Bags"},{"categories":["programming"],"contents":" Background  I recently read this post written by the now deceased Prof. David MacKay\u0026nbsp;1 It should be read widely, however, given that it is distributed as a ps.gz which is then a .ps file, and thus probably inaccessible to many of the people who should read it, I decided to rework it for native online consumption (there is also a pdf) THIS IS NOT MY CONTENT\u0026nbsp;2 Now, enjoy the post  Everyone Should Get an A Imagine a University – call it Camwick – where all students arrive with straight A grades. They are successful, enthusiastic, and curious. By the time they leave, only one third still receive straight As. The other two thirds get lower grades, do not enjoy their studies, and are not fun to teach. Is Camwick University a success? Camwick could point to its excellent teaching assessment scores and argue that it is ‘adding value’: students emerge knowing more. Future employers love the University’s policy of assigning grades – the University ranks its students, saving companies the bother of assessing job applicants themselves. But should a University be a sorting service? Isn’t something wrong with an institution that takes in mainly A-quality input and turns out less than half A-quality output? If a University fails to turn out as much A-quality enthusiasts as come in, is it in fact a place of intellectual destruction, throwing away the potential of the majority of its students? What are the roots of this destruction?\nExams I would recommend that Camwick consider abolishing traditional exams. In the current system, Camwick teaches Anna, Bob, and Charlie, who are all smart, then examines them; Anna comes \u0026lsquo;top\u0026rsquo;, Bob \u0026lsquo;second\u0026rsquo; and Charlie \u0026lsquo;third\u0026rsquo;. Perhaps Charlie, given a little more time, would have figured out the material, but he wasn\u0026rsquo;t quite ready when the exam arrived - perhaps because other courses consumed his attention.\nBob\u0026rsquo;s response to his \u0026lsquo;failure\u0026rsquo; is to adopt strategies of tlittle educational value: he parrot learns, he crams, and he asks lecturers to tell him what\u0026rsquo;s going to be on the exam. The exams become the focus of attention, even though the purpose of Bob\u0026rsquo;s going to the University was learning.\nCharlie\u0026rsquo;s response is to give up on doing \u0026lsquo;well\u0026rsquo;, and coast through University, no longer understanding everything. He loses self-worth and resents the University for making him feel bad.\nSome courses at Camwick assign grades using continuous assessment instead of exams. But continuous assessment has the same effect as exams on Bob and Charlie. So course grades based on continuous assessment should be abolished at the same time as exams.\nFigure 1: Everyone can get an A, regardless of learning rate, if their education is not halted by exams. Traditional system on the left, with an educational system on the right.  If Camwick had no exams, the focus of attention would have to be elsewhere. How about education, for example? Students could spend their time at Camwick exploring subjects that interest them, and attending classes that offer something they want to know about, free from the stress and misdirection of the exam system. Lecturers would at all times be friends rather than adversaries. [When I was an undergraduate at Cambridge, I asked a physis lecturer to clarify topic \\(N\\), which I felt had not been covered clearly. His response: \u0026ldquo;That\u0026rsquo;s what I love about \\(N\\): some students get it, some don\u0026rsquo;t - so we get beautiful bell shaped curves in the exam.\u0026ldquo;]\nOf course the extreme suggestion of abolishing all exams will not go down well: \u0026ldquo;What about standards?\u0026rdquo; \u0026ldquo;How can we get funding if we do not\u0026rdquo; \u0026ldquo;How do we award degrees that people will respect?\u0026rdquo; Traditionalists might say that students appreciate exams for the targets and feedback. Well, there\u0026rsquo;s nothing to stop us giving students targets or feedback. We can provide events just like exams, if students want them - self-administered tests, for example, would allow students to check how well they have assimilated all the material in a course. Other systems of targets and feedback that students enjoy include project work, problem-based learning, and portfolio-based assessment.\nAs a compromise, let\u0026rsquo;s modify our proposal a little: Camwick should become a place where the only achievable grade is an A. I\u0026rsquo;m not recommending that we simply give everyone an A. It\u0026rsquo;s a crime to let standards slip. When I say everyone should get an A, I mean that everyone should be allowed to get to an A.\nThink back to Alice, Bob, and Charlie. Alice grasped most of the material in the course and achieved an A. Given a little more time and little less stress, Bob and Charlie could probably have grasped it all too, and become equally strong masters of the material. What good does it do Bob and Charlie to record the fact that they were a little slower than alice? Wouldn\u0026rsquo;t it have been better, educationally, to give Bob and Charlie a little more time and help, so that they achieved the same A standard?\nDoes a bus-driver-training school rank its graduating drivers? No, it ensures that all attain the standard required of a bus-driver. Would you like to be treated by a C-grade doctor? No, everyone wants an A-grade doctor! So doctors and drivers are (I hope!) trained and trained and not let out until they are A-grade in standard. Why should other professions be treated differently?\nFigure 1a shows the command of the material of each student as a function of time in the traditional system. A traditional exam interrupts the learning process, and Bob and Charlie are recorded as having achieved a lower standard. Figure 1b shows the same students in an exam-free system, assuming they learn at the same rate as in the old system. Each student takes a different time to achieve full command of the course material. Every student has the satisfaction of achieving full command of the material.\nFigure 2: Everyone can get an A, regardless of learning rate, if their education is not halted by exams. Traditional system on the left, with an educational system on the right.  The difference between the two systems is also striking if we assume that students start the course at different levels of ability. In Figure 2, albert comes from a privileged background and already knows half the course material when he arrives. Brenda and Catharine arrive at a lower educational level. Brenda and Catharine are actually faster learners than Albert, but, as Figure 2a shows, the traditional exam system rewards Albert with the A grade (\u0026lsquo;congratulations, you started first!\u0026rsquo;), and brands Brenda and Catharine failures. In the \u0026lsquo;Only A-grades\u0026rsquo; system, everyone attains an A-grade in due course; and Albert isn\u0026rsquo;t actually first to finish.\nThe information about \u0026lsquo;who finished when\u0026rsquo; could in principle be retained in order to provide some sort of student-ranking service to employers, but I would strongly urge the destruction of all such records. Only the achieving of an A grade should be recorded, nothing else. Why?\n Because being ranked creates stress. Because students who are competing with each other for ranks may be reluctant to help each other learn. In contrast, in the \u0026lsquo;Only A-grades\u0026rsquo; system, the top students lose nothing if they help their peers; indeed, they may gain in several ways: peer-teaching strengthens the students\u0026rsquo; grasp on material, and often speeds up the whole class. Evidence that a student is a quick learner may well make itself evident in her transcript without rankings being made: Alice, covering material quickly, will have time to take extra courses. So in one year she\u0026rsquo;ll accumulate a slightly fatter sheaf of A-grade qualifications. What value are rankings? If future employers want students to be formally evaluated, they can pay for an evaluation service. Why ruin a great institution? The very best students might like grades too, as they enjoy being congratulated. But the \u0026lsquo;only A-grades\u0026rsquo; system will congratualte them too.  These ideas are not new, nor are they unprecedented. In many German Universities, first- and second-year courses have no grades, no obligatory coursework, and no obligatory exams. End-of-course exams are provided only as a service to students, to help them find out if they have indeed grasped the material and are ready progress to the next stage.\nIn practice, how should we organize courses so that everyone reaches 100% mastery? For Bob and Charlie\u0026rsquo;s benefit, the average pace probably has to be reduced. Figure 3 shows one way of organizing the material in stages, so that a class is kept together. Whenever Alice has completed the material in a stage, she can spend time on other interests, or can help other members of the class.\nFigure 3: Possible course plan. This scheme assumes that the students have rates of progress ranging from A (fastest) to C (slowest). Every two weeks, a consolidation period is inserted to ensure that C has assimilated all the learning objectives. Alice can use the consolidation period to pursue others interests or act as a peer-teacher.  Camwick staff who say \u0026ldquo;we can\u0026rsquo;t possibly cover a full degree course if we reduce the pace!\u0026rdquo; should bear in mind that, had Bob and charlie gone to a less prestigious University, they probably would have got first-class degrees. How can this paradox - going slowing and arriving at almost the same time - be explained? I suspect an important factor is this: struggling students get ever slower if we pile on new material before they have assimilated the old. For example, 2ⁿᵈ-year Lagrangian dynamics is difficult to absorb if one hasn\u0026rsquo;t grasped 1ˢᵗ-year Newtonian dynamics. So the steady linear progress assumed in Figures 1 to 3 is a poor model of Carlie. The more Charlie is left behind, the slower he learns. This means that the true difference in pace between Alice and Charlie need not be very big. If Charlie gets lost and left behind, we are wasting everyone\u0026rsquo;s time by having him sit in classes where new material is presented. A stitch in time saves nine (Figure 4).\nFigure 4: A stitch in time saves nine. Curve C shows Charlie\u0026#39;s progress in a course taught at the pace that is ideal for Alice. The more Charlie is left behind, the slower he learns. By the end of the course, there is a big gap between A and C. Curve C′ shows Charlie\u0026#39;s progress in a course taught at the pace that is ideal for him. Just a small decrease in class pace allows the big gap between Alice and Charlie to be eliminated.  Teaching methods must be modified to ensure that everyone in the class benefits. I advocate interactive teaching: students are asked questions and encouraged to ask questions and to be active participants in their own learning. It\u0026rsquo;s not enough to ask a question and let one person in the class (Alice!) answer it. The whole class must have the chance to think, puzzle and discuss; the teacher must ascertain the level of understanding of the whole class. In large classes, I find Mazur\u0026rsquo;s voting method works well: a lecture is centered on two or three carefully chosen questions with multiple-choice answers. Students discuss a question with their neighbors, then all vote. The vote informs the lecturer whether previous material has been understood. Diversity of votes can seed a useful discussion.\nTo conclude, here are a few further advantages of the educational approach advocated here:\n Happy, curious, and self-motivated students are fun to teach. At present, British students have little choice of university teaching and assessment style: all universities give out grades. Shouldn\u0026rsquo;t we offer them a choice? Some students would like the chance to go to a place with high standards where only A-grades are awarded. If some universities adopt student-centered educational policies and stop ranking students, perhaps these attitudes will spread to schools, with consequent benefits to pupils, and in due course, to universities. Dumbed-down A levels could be replaced by educational programmes that ensure that everyone attains their maximum potential and feels happy about it. Happy graduates who get A grades are likely to become grateful alumni donors.   Also known for the fabulous free book called Information Theory, Inference, and Learning Algorithms [return] If you have a good reason why this should not be distributed here in this manner, please contact me and I will do the needful [return]   ","permalink":"https://rgoswami.me/posts/mackay-all-a/","tags":["academics","teaching","evaluation","ideas"],"title":"Everyone Should Get an A - David MacKay"},{"categories":["notes"],"contents":" Background  I have had a lot of discussions regarding the teaching of git This is mostly as a part of the SoftwareCarpentries, or in view of my involvement with univ.ai, or simply in every public space I am associated with Without getting into my views, I just wanted to keep this resource in mind  The site  Learning git is a highly contentious thing People seem to be fond of GUI tools, especially since on non *nix systems, it seems that there is a lot of debate surrounding obtaining the git utility in the first place  One of the best ways of understanding (without installing stuff) the mental models required for working with git is this site\nFigure 1: A screenshot of the site   However, as is clear, this is not exactly a replacement for a good old command-line.\n It does make for a good resource for teaching with slides, or for generating other static visualizations, where live coding is not an option\n  ","permalink":"https://rgoswami.me/posts/d3git/","tags":["tools","rationale","workflow","ideas"],"title":"D3 for Git"},{"categories":["notes"],"contents":" Background Sometime this year, I realized that I no longer have access to a lot of my older communication. This included, a lot of resources I enjoyed and shared with the people who were around me at that point in time. To counter this, I have decided to opt for shorter posts, even if they don\u0026rsquo;t always include the same level of detail I would prefer to provide.\nAlternatives  I have an automated system based around IFTTT combined with Twitter, Diigo, and even Pocket However, that doesn\u0026rsquo;t really tell me much, and trawling through a massive glut of data is often pointless as well There\u0026rsquo;s always Twitter, but I don\u0026rsquo;t really care to hear the views of others when I want to revisit my own ideas  Conclusions  I will be making shorter posts here, like the random one on octobox  ","permalink":"https://rgoswami.me/posts/shortpost/","tags":["tools","rationale","workflow","ideas"],"title":"Shorter Posts"},{"categories":["programming"],"contents":" Background My dotfiles turned 4 years old a few months ago (since 9th Jan 2017) and remains one of my most frequently updated projects for obvious reasons. Going through the changes reminds me of a whole of posts I never got around to writing.\nAnyway, recently I gained access to another HPC cluster, with a standard configuration (bash, old CentOS) and decided to track my provisioning steps. This is really a very streamlined experience by now, since I\u0026rsquo;ve used the same setup across scores of machines. This is actually also a generic intro to configuring user setups on HPC (high performance cluster) machines, if one is inclined to read it in that manner. To that end, sections of this post involve restrictions relating to user privileges which aren\u0026rsquo;t normally part of most Dotfile setups.\nAside  Dotfiles define most people who maintain them No two sets are ever exactly alike They fall somewhere between winging it for each machine and using something like Chef or Ansible Tracking dotfiles is really close to having a sort of out-of-context journal  Before I settled on using the fabulous dotgit, I considered several alternatives, most notably GNU stow.\nPreliminaries It is important to note the environment into which I had to get my setup.\nSSH Setup  The very first thing to do is to use a new ssh-key   export myKey=\u0026#34;someName\u0026#34; ssh-keygen -f $HOME/.ssh/$myKey # I normally don\u0026#39;t set a password ssh-add $HOME/.ssh/$myKey ssh-copy-id $myHPC # myHPC being an IP address I more often than not tend to back this up with a cutesy alias, also because I do not always get my username of choice on these machines. So in $HOME/.ssh/config I use:\nHost myHPC Hostname 127.0.0.1 User somethingIgot IdentityFile ~/.ssh/myKey Harvesting Information  I normally use neofetch on new machines   mkdir -p $HOME/Git/Github cd $HOME/Git/Github git clone https://github.com/dylanaraps/neofetch.git cd neofetch ./neofetch Figure 1: Neofetch Output  Where the top has been tastefully truncated. Just for context, the latest bash as of this writing is v5.0.16 so, that\u0026rsquo;s not too bad, given that neofetch works for bash ≥ 3.2\nTODO Circumventing User Restrictions with Nix  A post in and of itself would be required to explain why and how users are normally restricted from activities in cluster nodes Here, we leverage the nix-package management system to circumvent these User installation of nix is sadly non-trivial, so this might be of some use\u0026nbsp;1  Testing nix-user-chroot  We will first check namespace support   # Errored out unshare --user --pid echo YES # Worked! zgrep CONFIG_USER_NS /boot/config-$(uname -r) # CONFIG_USER_NS=y Thankfully we have support for namespaces, so we can continue with nix-user-chroot.\n Since we definitely do not have rustup or rustc on the HPC, we will use a prebuilt binary of nix-user-chroot   cd $HOME \u0026amp;\u0026amp; wget -O nix-user-chroot https://github.com/nix-community/nix-user-chroot/releases/download/1.0.2/nix-user-chroot-bin-1.0.2-x86_64-unknown-linux-musl  Similar to the wiki example, we will use $HOME/.nix   cd ~/ chmod +x nix-user-chroot mkdir -m 0755 ~/.nix ./nix-user-chroot ~/.nix bash -c \u0026#39;curl https://nixos.org/nix/install | sh\u0026#39;  Only, this doesn\u0026rsquo;t work  Turns out that since unshare is too old, nix-user-chroot won\u0026rsquo;t work either.\nUsing PRoot PRoot is pretty neat in general, they even have a nice website describing it.\n Set a folder up for local installations (this is normally done by my Dotfiles, but we might as well have one here too)   mkdir -p $HOME/.local/bin export PATH=$PATH:$HOME/.local/bin  Get a binary from the GitLab artifacts   cd $HOME mkdir tmp cd tmp wget -O artifacts.zip https://gitlab.com/proot/proot/-/jobs/452350181/artifacts/download unzip artifacts.zip mv dist/proot $HOME/.local/bin  Bind and install nix   mkdir ~/.nix export PROOT_NO_SECCOMP=1 proot -b ~/.nix:/nix export PROOT_NO_SECCOMP=1 curl https://nixos.org/nix/install | sh If you\u0026rsquo;re very unlucky, like I was, you may be greeted by a lovely little error message along the lines of:\n/nix/store/ddmmzn4ggz1f66lwxjy64n89864yj9w9-nix-2.3.3/bin/nix-store: /opt/ohpc/pub/compiler/gcc/5.4.0/lib64/libstdc++.so.6: version `GLIBCXX_3.4.22\u0026#39; not found (required by /nix/store/c0b76xh2za9r9r4b0g3iv4x2lkw1zzcn-aws-sdk-cpp-1.7.90/lib/libaws-cpp-sdk-core.so) Which basically is as bad as it sounds. At this stage, we need a newer compiler to even get nix up and running, but can\u0026rsquo;t without getting an OS update. This chicken and egg situation calls for the drastic measure of leveraging brew first2.\nsh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)\u0026#34; Note that nothing in this section suggests the best way is not to lobby your sys-admin to install nix system-wide in multi-user mode.\nGiving Up with Linuxbrew  Somewhere around this point, linuxbrew is a good idea More on this later  Shell Stuff zsh is my shell of choice, and is what my Dotfiles expect and work best with.\n I did end up making a quick change to update the dotfiles with a target which includes a snippet to transition to zsh from the default bash shell  Dotfiles The actual installation steps basically tracks the readme instructions.\ngit clone https://github.com/kobus-v-schoor/dotgit.git mkdir -p ~/.bin cp -r dotgit/bin/dotgit* ~/.bin cat dotgit/bin/bash_completion \u0026gt;\u0026gt; ~/.bash_completion rm -rf dotgit # echo \u0026#39;export PATH=\u0026#34;$PATH:$HOME/.bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export PATH=\u0026#34;$PATH:$HOME/.bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc Much of this section is directly adapted from the NixOS wiki [return] This used to be called linuxbrew, but the new site makes it clear that it\u0026rsquo;s all one brew now. [return]   ","permalink":"https://rgoswami.me/posts/prov-dots/","tags":["programming","workflow","projects","hpc"],"title":"Provisioning Dotfiles on an HPC"},{"categories":["notes"],"contents":" Background I just realized that it has been over two years since I switched from QWERTY to COLEMAK but somehow never managed to write about it. It was a major change in my life, and it took forever to get acclimatized to. I do not think I\u0026rsquo;ll ever again be in a position to make such a change in my life again, but it was definitely worth it.\nTouch Typing My interest in touch typing in I decided to digitize my notes for posterity, during the last two years of my undergraduate studies back in Harcourt Butler Technical Institute (HBTI) Kanpur, India. in one of my many instances of yak shaving, I realized I could probably consume and annotate a lot more content by typing faster. Given that at that stage I was already a fast talker, it seemed like a natural extension. There was probably an element of nostalgia involved as well. That and the end of a bachelors involves the thesis, which generally involves a lot of typing.\nThere were (and are) some fantastic resources for learning to touch type nowadays, I personally used:\n Typing.com This is short, but a pretty good basic setup. The numbering and special characters are a bit much to take in at the level of practice you get by completing all the previous exercises, but eventually they make for a good workout. TypingClub This is what I ended up working my way through. It is comprehensive, beautiful, and fun.  Also, later, I ended up using keybr a lot, simply because typing gibberish is a good way of practicing, and it is independent of the keyboard layout.\nJust to foreshadow things, the enemy facing me at this point was the layout itself1.\n Alternate layouts Having finally broken into the giddy regimes of 150+ wpm, I was ecstatic, and decided to start working my way through some longer reports. However, I quickly realized I was unable to type for more than a couple of minutes without getting terribly cramped. Once it got to the point of having to visit a physiotherapist, I had to call it quits. At that stage, relearning the entire touch typing corpus, given that I already was used to QWERTY, seemed pretty bleak.\nIt took forever, and I ended up applying my choices to my phone keyboard as well, which presumably helped me in terms of increasing familiarity, had the unintended effect of making me seem distant to people I was close to, since my verbose texts suddenly devolved to painful one-liners.\nThe alternative layouts I tried were:\n DVORAK At the time, TypingClub only supported QWERTY and DVORAK, so it was pretty natural for me to try it out. There are also some very nice comics about it. I remember that it was pretty neat, with a good even distribution, until I tried coding. The placement of the semicolons make it impossible to use while programming. I would still say it makes for a comfortable layout, as long as special characters are not required.    CarpalX I experimented with the entire carpalx family, but I was unable to get used to it. I liked QFMLWY best. I do recommend reading the training methodology, especially if anyone is interested in numerical optimization in general. More importantly, though it was relatively easy to set up on my devices and operating systems, the fact that it wasn\u0026rsquo;t natively supported meant a lot of grief whenever I inevitably had to use a public computer.    Colemak Eventually I decided to go with Colemak, especially since it is widely available. Nothing is easier than setxkbmap us -variant colemak -option grp:alt_shift_toggle on public machines and it\u0026rsquo;s easy on Windows as well. Colemak seems like a good compromise. I personally have not been able to reach the same speeds I managed with QWERTY, even after a year, but then again, I can be a lot more consistent, and it hurts less. Nowadays, Colemak has made its way onto most typing sites as well, including TypingClub   What about VIM?  DVORAK makes it impossible, so do most other layouts, but there are some tutorials purporting to help use vim movement with DVORAK Colemak isn\u0026rsquo;t any better, but the fact of the matter is that once you know VIM on QWERTY, and have separately internalized colemak or something else, hitting keys is just hitting keys  All that said, I still occasionally simply remap HJKL (QWERTY movement) to HNEI (Colemak analog) when it is feasible.\nConclusion Changing layouts was a real struggle. Watching my WPM drop back to lower than hunt and peck styles was pretty humiliating, especially since the reports kept coming in, and more than once I switched to QWERTY. However, since then, I have managed to stay on course. I guess if I think about it, it boils down to a few scattered thoughts:\n Typing is kinda like running a marathon, knowing how it is done and doing it are two different things Tell everyone, so people can listen to you lament your reduced speed and not hate you for replying slowly Practice everyday, because, well, it works out in the long run, even when you plateau Alternate shifts! That\u0026rsquo;s really something which should show up more in tutorials, especially for listicles, not changing the shifts will really hurt Try and get a mechanical keyboard (like the Anne Pro 2 or the Coolermaster Masterkeys), they\u0026rsquo;re fun and easy to change layouts on   The images are from here, where there\u0026rsquo;s also an effort based metric used to score keyboard layouts. [return]   ","permalink":"https://rgoswami.me/posts/colemak-switch/","tags":["workflow","programming","personal"],"title":"Switching to Colemak"},{"categories":["personal"],"contents":" Background For a while I was worried about writing about a TV show here. I thought it might be frivolous, or worse, might outweigh the other kinds of articles I would like to write. However, like most things, that which is ignored just grows, so it is easier to just write and forget about it.\nThe Show Much has been said about how Bojack Horseman is one of the best shows ever, and they\u0026rsquo;re all correct. For that matter I won\u0026rsquo;t be going into the details of how every episode ties together a tapestry of lives in a meaningful way, or any of that. The show was amazingly poignant. The characters felt real. Which actually leads me to the real issue.\nThe End The end of Bojack was good. It was the way it was meant to be. For a slice-of-life show, it is a natural conclusion. It isn\u0026rsquo;t necessary that any catharsis occurs or that the characters change or become better or all that jazz. It isn\u0026rsquo;t about giving the viewers closure. It is simply about a window onto the lives of (fictional) characters being shut. To that end, I disliked attempts to bring closure in the show itself.\nOne of the main reasons why I felt strongly enough to write this, is simply because when I looked around, the prevailing opinion was that the main character should have been killed off, for his sins. This strikes me as a very flippant attitude to take. It reeks of people trying to make the show a cautionary tale, which is frankly speaking a weird approach to take towards any fictional story. The idea that the character should be redeemed also seemed equally weak, for much the same reasons.\nThe fact that the characters are hypocrites, and that none of them are as good or bad as they make themselves out to be is one of the best parts of the show.\nConclusion That\u0026rsquo;s actually all I have to say about this. I thought of adding relevant memes or listing episodes or name dropping sites, but this isn\u0026rsquo;t buzzfeed. The show is incredible, and there are far better ways of proving that. Bust out your favorite search engine + streaming content provider / digital piracy eye-patch and give it a whirl. The only thing I\u0026rsquo;d suggest is watching everything in order, it\u0026rsquo;s just that kind of show.\n","permalink":"https://rgoswami.me/posts/bojack-horseman/","tags":["thoughts","random","review","TV"],"title":"Bojack Horseman"},{"categories":["programming"],"contents":" Chapter VII - Moving Beyond Linearity All the questions are as per the ISL seventh printing.\nCommon libsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;, \u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;,\u0026#34;MASS\u0026#34;, \u0026#34;gridExtra\u0026#34;, \u0026#34;pls\u0026#34;,\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;) invisible(lapply(libsUsed, library, character.only = TRUE))## ## Attaching package: \u0026#39;dplyr\u0026#39;## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──## ✔ tibble 2.1.3 ✔ purrr 0.3.3 ## ✔ tidyr 1.0.0 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag()## Loading required package: lattice## ## Attaching package: \u0026#39;caret\u0026#39;## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## lift## ## Attaching package: \u0026#39;MASS\u0026#39;## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## select## ## Attaching package: \u0026#39;gridExtra\u0026#39;## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## combine## ## Attaching package: \u0026#39;pls\u0026#39;## The following object is masked from \u0026#39;package:caret\u0026#39;: ## ## R2## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## loadings## ## Attaching package: \u0026#39;data.table\u0026#39;## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## transpose## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## between, first, last Question 7.6 - Page 299 In this exercise, you will further analyze the Wage data set considered throughout this chapter.\n(a) Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.\n(b) Fit a step function to predict wage using age, and perform cross-validation to choose the optimal number of cuts. Make a plot of the fit obtained. In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\nAnswer Lets get the data.\nset.seed(1984) wageDat\u0026lt;-ISLR::Wage wageDat %\u0026gt;% str %\u0026gt;% print## \u0026#39;data.frame\u0026#39;: 3000 obs. of 11 variables: ## $ year : int 2006 2004 2003 2003 2005 2008 2009 2008 2006 2004 ... ## $ age : int 18 24 45 43 50 54 44 30 41 52 ... ## $ maritl : Factor w/ 5 levels \u0026#34;1. Never Married\u0026#34;,..: 1 1 2 2 4 2 2 1 1 2 ... ## $ race : Factor w/ 4 levels \u0026#34;1. White\u0026#34;,\u0026#34;2. Black\u0026#34;,..: 1 1 1 3 1 1 4 3 2 1 ... ## $ education : Factor w/ 5 levels \u0026#34;1. \u0026lt; HS Grad\u0026#34;,..: 1 4 3 4 2 4 3 3 3 2 ... ## $ region : Factor w/ 9 levels \u0026#34;1. New England\u0026#34;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ jobclass : Factor w/ 2 levels \u0026#34;1. Industrial\u0026#34;,..: 1 2 1 2 2 2 1 2 2 2 ... ## $ health : Factor w/ 2 levels \u0026#34;1. \u0026lt;=Good\u0026#34;,\u0026#34;2. \u0026gt;=Very Good\u0026#34;: 1 2 1 2 1 2 2 1 2 2 ... ## $ health_ins: Factor w/ 2 levels \u0026#34;1. Yes\u0026#34;,\u0026#34;2. No\u0026#34;: 2 2 1 1 1 1 1 1 1 1 ... ## $ logwage : num 4.32 4.26 4.88 5.04 4.32 ... ## $ wage : num 75 70.5 131 154.7 75 ... ## NULLwageDat %\u0026gt;% summary %\u0026gt;% print## year age maritl race ## Min. :2003 Min. :18.00 1. Never Married: 648 1. White:2480 ## 1st Qu.:2004 1st Qu.:33.75 2. Married :2074 2. Black: 293 ## Median :2006 Median :42.00 3. Widowed : 19 3. Asian: 190 ## Mean :2006 Mean :42.41 4. Divorced : 204 4. Other: 37 ## 3rd Qu.:2008 3rd Qu.:51.00 5. Separated : 55 ## Max. :2009 Max. :80.00 ## ## education region jobclass ## 1. \u0026lt; HS Grad :268 2. Middle Atlantic :3000 1. Industrial :1544 ## 2. HS Grad :971 1. New England : 0 2. Information:1456 ## 3. Some College :650 3. East North Central: 0 ## 4. College Grad :685 4. West North Central: 0 ## 5. Advanced Degree:426 5. South Atlantic : 0 ## 6. East South Central: 0 ## (Other) : 0 ## health health_ins logwage wage ## 1. \u0026lt;=Good : 858 1. Yes:2083 Min. :3.000 Min. : 20.09 ## 2. \u0026gt;=Very Good:2142 2. No : 917 1st Qu.:4.447 1st Qu.: 85.38 ## Median :4.653 Median :104.92 ## Mean :4.654 Mean :111.70 ## 3rd Qu.:4.857 3rd Qu.:128.68 ## Max. :5.763 Max. :318.34 ##wageDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print## year age maritl race education region jobclass ## 7 61 5 4 5 1 2 ## health health_ins logwage wage ## 2 2 508 508library(boot)## ## Attaching package: \u0026#39;boot\u0026#39;## The following object is masked from \u0026#39;package:lattice\u0026#39;: ## ## melanoma a) Polynomial regression all.deltas = rep(NA, 10) for (i in 1:10) { glm.fit = glm(wage~poly(age, i), data=Wage) all.deltas[i] = cv.glm(Wage, glm.fit, K=10)$delta[2] } plot(1:10, all.deltas, xlab=\u0026#34;Degree\u0026#34;, ylab=\u0026#34;CV error\u0026#34;, type=\u0026#34;l\u0026#34;, pch=20, lwd=2, ylim=c(1590, 1700)) min.point = min(all.deltas) sd.points = sd(all.deltas) abline(h=min.point + 0.2 * sd.points, col=\u0026#34;red\u0026#34;, lty=\u0026#34;dashed\u0026#34;) abline(h=min.point - 0.2 * sd.points, col=\u0026#34;red\u0026#34;, lty=\u0026#34;dashed\u0026#34;) legend(\u0026#34;topright\u0026#34;, \u0026#34;0.2-standard deviation lines\u0026#34;, lty=\u0026#34;dashed\u0026#34;, col=\u0026#34;red\u0026#34;)  # ANOVA fits=list() for (i in 1:10) { fits[[i]]=glm(wage~poly(age,i),data=wageDat) } anova(fits[[1]],fits[[2]],fits[[3]],fits[[4]],fits[[5]], fits[[6]],fits[[7]],fits[[8]],fits[[9]],fits[[10]])## Analysis of Deviance Table ## ## Model 1: wage ~ poly(age, i) ## Model 2: wage ~ poly(age, i) ## Model 3: wage ~ poly(age, i) ## Model 4: wage ~ poly(age, i) ## Model 5: wage ~ poly(age, i) ## Model 6: wage ~ poly(age, i) ## Model 7: wage ~ poly(age, i) ## Model 8: wage ~ poly(age, i) ## Model 9: wage ~ poly(age, i) ## Model 10: wage ~ poly(age, i) ## Resid. Df Resid. Dev Df Deviance ## 1 2998 5022216 ## 2 2997 4793430 1 228786 ## 3 2996 4777674 1 15756 ## 4 2995 4771604 1 6070 ## 5 2994 4770322 1 1283 ## 6 2993 4766389 1 3932 ## 7 2992 4763834 1 2555 ## 8 2991 4763707 1 127 ## 9 2990 4756703 1 7004 ## 10 2989 4756701 1 3  The 4th degree looks the best at the moment   # 3rd or 4th degrees look best based on ANOVA test # let\u0026#39;s go with 4th degree fit plot(wage~age, data=wageDat, col=\u0026#34;darkgrey\u0026#34;) agelims = range(wageDat$age) age.grid = seq(from=agelims[1], to=agelims[2]) lm.fit = lm(wage~poly(age, 4), data=wageDat) lm.pred = predict(lm.fit, data.frame(age=age.grid)) lines(age.grid, lm.pred, col=\u0026#34;blue\u0026#34;, lwd=2)  b) Step function and cross-validation # cross-validation cv.error \u0026lt;- rep(0,9) for (i in 2:10) { wageDat$age.cut \u0026lt;- cut(wageDat$age,i) glm.fit \u0026lt;- glm(wage~age.cut, data=wageDat) cv.error[i-1] \u0026lt;- cv.glm(wageDat, glm.fit, K=10)$delta[1] # [1]:std, [2]:bias-corrected } cv.error## [1] 1732.337 1682.978 1636.736 1635.600 1624.174 1610.688 1604.081 1612.005 ## [9] 1607.022cv.error## [1] 1732.337 1682.978 1636.736 1635.600 1624.174 1610.688 1604.081 1612.005 ## [9] 1607.022plot(2:10, cv.error, type=\u0026#34;b\u0026#34;)  cut.fit \u0026lt;- glm(wage~cut(age,8), data=wageDat) preds \u0026lt;- predict(cut.fit, newdata=list(age=age.grid), se=TRUE) se.bands \u0026lt;- preds$fit + cbind(2*preds$se.fit, -2*preds$se.fit) plot(wageDat$age, wageDat$wage, xlim=agelims, cex=0.5, col=\u0026#34;darkgrey\u0026#34;) title(\u0026#34;Fit with 8 Age Bands\u0026#34;) lines(age.grid, preds$fit, lwd=2, col=\u0026#34;blue\u0026#34;) matlines(age.grid, se.bands, lwd=1, col=\u0026#34;blue\u0026#34;, lty=3)  Question 7.8 - Page 299 Fit some of the non-linear models investigated in this chapter to the Auto data set. Is there evidence for non-linear relationships in this data set? Create some informative plots to justify your answer.\nAnswer autoDat\u0026lt;-ISLR::AutoautoDat %\u0026gt;% pivot_longer(-c(mpg,name),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=mpg,y=Value)) + geom_point() + facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;)  Very clearly there is a lot of non-linearity in the mpg data, especially for acceleration, weight, displacement, horsepower.\nrss = rep(NA, 10) fits = list() for (d in 1:10) { fits[[d]] = lm(mpg ~ poly(displacement, d), data = autoDat) rss[d] = deviance(fits[[d]]) } rss %\u0026gt;% print## [1] 8378.822 7412.263 7392.322 7391.722 7380.838 7270.746 7089.716 6917.401 ## [9] 6737.801 6610.190anova(fits[[1]],fits[[2]],fits[[3]],fits[[4]],fits[[5]], fits[[6]],fits[[7]],fits[[8]],fits[[9]],fits[[10]])## Analysis of Variance Table ## ## Model 1: mpg ~ poly(displacement, d) ## Model 2: mpg ~ poly(displacement, d) ## Model 3: mpg ~ poly(displacement, d) ## Model 4: mpg ~ poly(displacement, d) ## Model 5: mpg ~ poly(displacement, d) ## Model 6: mpg ~ poly(displacement, d) ## Model 7: mpg ~ poly(displacement, d) ## Model 8: mpg ~ poly(displacement, d) ## Model 9: mpg ~ poly(displacement, d) ## Model 10: mpg ~ poly(displacement, d) ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 390 8378.8 ## 2 389 7412.3 1 966.56 55.7108 5.756e-13 *** ## 3 388 7392.3 1 19.94 1.1494 0.284364 ## 4 387 7391.7 1 0.60 0.0346 0.852549 ## 5 386 7380.8 1 10.88 0.6273 0.428823 ## 6 385 7270.7 1 110.09 6.3455 0.012177 * ## 7 384 7089.7 1 181.03 10.4343 0.001344 ** ## 8 383 6917.4 1 172.31 9.9319 0.001753 ** ## 9 382 6737.8 1 179.60 10.3518 0.001404 ** ## 10 381 6610.2 1 127.61 7.3553 0.006990 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Confirming our visual indications, we see that the second degree models work well.\nlibrary(glmnet)## Loading required package: Matrix## ## Attaching package: \u0026#39;Matrix\u0026#39;## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: ## ## expand, pack, unpack## Loaded glmnet 3.0-2library(boot)cv.errs = rep(NA, 15) for (d in 1:15) { fit = glm(mpg ~ poly(displacement, d), data = Auto) cv.errs[d] = cv.glm(Auto, fit, K = 15)$delta[2] } which.min(cv.errs)## [1] 10 Strangely, we seem to have ended up with a ten variable model here.\n# Step functions cv.errs = rep(NA, 10) for (c in 2:10) { Auto$dis.cut = cut(Auto$displacement, c) fit = glm(mpg ~ dis.cut, data = Auto) cv.errs[c] = cv.glm(Auto, fit, K = 10)$delta[2] } which.min(cv.errs) %\u0026gt;% print## [1] 9library(splines) cv.errs = rep(NA, 10) for (df in 3:10) { fit = glm(mpg ~ ns(displacement, df = df), data = Auto) cv.errs[df] = cv.glm(Auto, fit, K = 10)$delta[2] } which.min(cv.errs) %\u0026gt;% print## [1] 10library(gam)## Loading required package: foreach## ## Attaching package: \u0026#39;foreach\u0026#39;## The following objects are masked from \u0026#39;package:purrr\u0026#39;: ## ## accumulate, when## Loaded gam 1.16.1# GAMs fit = gam(mpg ~ s(displacement, 4) + s(horsepower, 4), data = Auto)## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument ## ignoredsummary(fit)## ## Call: gam(formula = mpg ~ s(displacement, 4) + s(horsepower, 4), data = Auto) ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -11.2982 -2.1592 -0.4394 2.1247 17.0946 ## ## (Dispersion Parameter for gaussian family taken to be 15.3543) ## ## Null Deviance: 23818.99 on 391 degrees of freedom ## Residual Deviance: 5880.697 on 382.9999 degrees of freedom ## AIC: 2194.05 ## ## Number of Local Scoring Iterations: 2 ## ## Anova for Parametric Effects ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## s(displacement, 4) 1 15254.9 15254.9 993.524 \u0026lt; 2e-16 *** ## s(horsepower, 4) 1 1038.4 1038.4 67.632 3.1e-15 *** ## Residuals 383 5880.7 15.4 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Anova for Nonparametric Effects ## Npar Df Npar F Pr(F) ## (Intercept) ## s(displacement, 4) 3 13.613 1.863e-08 *** ## s(horsepower, 4) 3 15.606 1.349e-09 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Question 7.9 - Pages 299-300 This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response.\n(a) Use the poly() function to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.\n(b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.\n\u0026copy; Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.\n(d) Use the bs() function to fit a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.\n(e) Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.\n(f) Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results.\nAnswer boston\u0026lt;-MASS::Boston boston %\u0026gt;% str %\u0026gt;% print## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... ## NULLboston %\u0026gt;% summary %\u0026gt;% print## crim zn indus chas ## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 ## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 ## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 ## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 ## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 ## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 ## nox rm age dis ## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 ## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 ## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 ## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 ## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 ## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 ## rad tax ptratio black ## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 ## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 ## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 ## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 ## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 ## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 ## lstat medv ## Min. : 1.73 Min. : 5.00 ## 1st Qu.: 6.95 1st Qu.:17.02 ## Median :11.36 Median :21.20 ## Mean :12.65 Mean :22.53 ## 3rd Qu.:16.95 3rd Qu.:25.00 ## Max. :37.97 Max. :50.00boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print## crim zn indus chas nox rm age dis rad tax ## 504 26 76 2 81 446 356 412 9 66 ## ptratio black lstat medv ## 46 357 455 229 a) Polynomial fit.03 \u0026lt;- lm(nox~poly(dis,3), data=boston) dislims \u0026lt;- range(boston$dis) dis.grid \u0026lt;- seq(dislims[1], dislims[2], 0.1) preds \u0026lt;- predict(fit.03, newdata=list(dis=dis.grid), se=TRUE) se.bands \u0026lt;- preds$fit + cbind(2*preds$se.fit, -2*preds$se.fit) par(mfrow=c(1,1), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0)) plot(boston$dis, boston$nox, xlim=dislims, cex=0.5, col=\u0026#34;darkgrey\u0026#34;) title(\u0026#34;Degree 3 Polynomial Fit\u0026#34;) lines(dis.grid, preds$fit, lwd=2, col=\u0026#34;blue\u0026#34;) matlines(dis.grid, se.bands, lwd=1, col=\u0026#34;blue\u0026#34;, lty=3)  summary(fit.03)## ## Call: ## lm(formula = nox ~ poly(dis, 3), data = boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.121130 -0.040619 -0.009738 0.023385 0.194904 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.554695 0.002759 201.021 \u0026lt; 2e-16 *** ## poly(dis, 3)1 -2.003096 0.062071 -32.271 \u0026lt; 2e-16 *** ## poly(dis, 3)2 0.856330 0.062071 13.796 \u0026lt; 2e-16 *** ## poly(dis, 3)3 -0.318049 0.062071 -5.124 4.27e-07 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.06207 on 502 degrees of freedom ## Multiple R-squared: 0.7148, Adjusted R-squared: 0.7131 ## F-statistic: 419.3 on 3 and 502 DF, p-value: \u0026lt; 2.2e-16 b) Multiple Polynomials rss.error \u0026lt;- rep(0,10) for (i in 1:10) { lm.fit \u0026lt;- lm(nox~poly(dis,i), data=boston) rss.error[i] \u0026lt;- sum(lm.fit$residuals^2) } rss.error## [1] 2.768563 2.035262 1.934107 1.932981 1.915290 1.878257 1.849484 1.835630 ## [9] 1.833331 1.832171plot(rss.error, type=\u0026#34;b\u0026#34;)  c) Cross validation and polynomial selection require(boot) set.seed(1) cv.error \u0026lt;- rep(0,10) for (i in 1:10) { glm.fit \u0026lt;- glm(nox~poly(dis,i), data=boston) cv.error[i] \u0026lt;- cv.glm(boston, glm.fit, K=10)$delta[1] # [1]:std, [2]:bias-corrected } cv.error## [1] 0.005558263 0.004085706 0.003876521 0.003863342 0.004237452 0.005686862 ## [7] 0.010278897 0.006810868 0.033308607 0.004075599plot(cv.error, type=\u0026#34;b\u0026#34;)   I feel like the second degree fit would be the most reasonable, though the fourth degree seems to be doing well.  d) Regression spline fit.sp \u0026lt;- lm(nox~bs(dis, df=4), data=boston) pred \u0026lt;- predict(fit.sp, newdata=list(dis=dis.grid), se=T) plot(boston$dis, boston$nox, col=\u0026#34;gray\u0026#34;) lines(dis.grid, pred$fit, lwd=2) lines(dis.grid, pred$fit+2*pred$se, lty=\u0026#34;dashed\u0026#34;) lines(dis.grid, pred$fit-2*pred$se, lty=\u0026#34;dashed\u0026#34;)  # set df to select knots at uniform quantiles of `dis` attr(bs(boston$dis,df=4),\u0026#34;knots\u0026#34;) # only 1 knot at 50th percentile## 50% ## 3.20745 e) Range of regression splines rss.error \u0026lt;- rep(0,7) for (i in 4:10) { fit.sp \u0026lt;- lm(nox~bs(dis, df=i), data=boston) rss.error[i-3] \u0026lt;- sum(fit.sp$residuals^2) } rss.error## [1] 1.922775 1.840173 1.833966 1.829884 1.816995 1.825653 1.792535plot(4:10, rss.error, type=\u0026#34;b\u0026#34;)   As the model gains more degrees of freedom, it tends to over fit to the training data better  f) Cross validation for best spline cv.error \u0026lt;- rep(0,7) for (i in 4:10) { glm.fit \u0026lt;- glm(nox~bs(dis, df=i), data=boston) cv.error[i-3] \u0026lt;- cv.glm(boston, glm.fit, K=10)$delta[1] }## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.1523), Boundary.knots = ## c(1.1296, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned ## bases ## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.1523), Boundary.knots = ## c(1.1296, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned ## bases## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.2157), Boundary.knots = ## c(1.137, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`50%` = 3.2157), Boundary.knots = ## c(1.137, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.35953333333333, : some ## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.35953333333333, : some ## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.38403333333333, : some ## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`33.33333%` = 2.38403333333333, : some ## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.07945, `50%` = 3.1323, : ## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.07945, `50%` = 3.1323, : ## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.1103, `50%` = 3.2797, : some ## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`25%` = 2.1103, `50%` = 3.2797, : some ## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.9682, `40%` = 2.7147, : some ## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.9682, `40%` = 2.7147, : some ## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.95434, `40%` = 2.59666, : ## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`20%` = 1.95434, `40%` = 2.59666, : ## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.82203333333333, : some ## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.82203333333333, : some ## \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.8226, `33.33333%` = ## 2.3817, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`16.66667%` = 1.8226, `33.33333%` = ## 2.3817, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`14.28571%` = 1.7936, `28.57143%` ## = 2.16972857142857, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill- ## conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`14.28571%` = 1.7936, `28.57143%` ## = 2.16972857142857, : some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill- ## conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.754625, `25%` = 2.10215, : ## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.754625, `25%` = 2.10215, : ## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.751575, `25%` = 2.08755, : ## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(dis, degree = 3L, knots = c(`12.5%` = 1.751575, `25%` = 2.08755, : ## some \u0026#39;x\u0026#39; values beyond boundary knots may cause ill-conditioned basescv.error## [1] 0.003898810 0.003694675 0.003732665 0.003766202 0.003716389 0.003723126 ## [7] 0.003727358plot(4:10, cv.error, type=\u0026#34;b\u0026#34;)   A fifth degree polynomial is clearly indicated  Question 10 - Page 300 This question relates to the College data set.\n(a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.\n(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your ﬁndings.\n\u0026copy; Evaluate the model obtained on the test set, and explain the results obtained.\n(d) For which variables, if any, is there evidence of a non-linear relationship with the response?\nAnswer colDat\u0026lt;-ISLR::College colDat %\u0026gt;% str %\u0026gt;% print## \u0026#39;data.frame\u0026#39;: 777 obs. of 18 variables: ## $ Private : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Apps : num 1660 2186 1428 417 193 ... ## $ Accept : num 1232 1924 1097 349 146 ... ## $ Enroll : num 721 512 336 137 55 158 103 489 227 172 ... ## $ Top10perc : num 23 16 22 60 16 38 17 37 30 21 ... ## $ Top25perc : num 52 29 50 89 44 62 45 68 63 44 ... ## $ F.Undergrad: num 2885 2683 1036 510 249 ... ## $ P.Undergrad: num 537 1227 99 63 869 ... ## $ Outstate : num 7440 12280 11250 12960 7560 ... ## $ Room.Board : num 3300 6450 3750 5450 4120 ... ## $ Books : num 450 750 400 450 800 500 500 450 300 660 ... ## $ Personal : num 2200 1500 1165 875 1500 ... ## $ PhD : num 70 29 53 92 76 67 90 89 79 40 ... ## $ Terminal : num 78 30 66 97 72 73 93 100 84 41 ... ## $ S.F.Ratio : num 18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ... ## $ perc.alumni: num 12 16 30 37 2 11 26 37 23 15 ... ## $ Expend : num 7041 10527 8735 19016 10922 ... ## $ Grad.Rate : num 60 56 54 59 15 55 63 73 80 52 ... ## NULLcolDat %\u0026gt;% summary %\u0026gt;% print## Private Apps Accept Enroll Top10perc ## No :212 Min. : 81 Min. : 72 Min. : 35 Min. : 1.00 ## Yes:565 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 1st Qu.:15.00 ## Median : 1558 Median : 1110 Median : 434 Median :23.00 ## Mean : 3002 Mean : 2019 Mean : 780 Mean :27.56 ## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 3rd Qu.:35.00 ## Max. :48094 Max. :26330 Max. :6392 Max. :96.00 ## Top25perc F.Undergrad P.Undergrad Outstate ## Min. : 9.0 Min. : 139 Min. : 1.0 Min. : 2340 ## 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 1st Qu.: 7320 ## Median : 54.0 Median : 1707 Median : 353.0 Median : 9990 ## Mean : 55.8 Mean : 3700 Mean : 855.3 Mean :10441 ## 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 3rd Qu.:12925 ## Max. :100.0 Max. :31643 Max. :21836.0 Max. :21700 ## Room.Board Books Personal PhD ## Min. :1780 Min. : 96.0 Min. : 250 Min. : 8.00 ## 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 1st Qu.: 62.00 ## Median :4200 Median : 500.0 Median :1200 Median : 75.00 ## Mean :4358 Mean : 549.4 Mean :1341 Mean : 72.66 ## 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 3rd Qu.: 85.00 ## Max. :8124 Max. :2340.0 Max. :6800 Max. :103.00 ## Terminal S.F.Ratio perc.alumni Expend ## Min. : 24.0 Min. : 2.50 Min. : 0.00 Min. : 3186 ## 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 1st Qu.: 6751 ## Median : 82.0 Median :13.60 Median :21.00 Median : 8377 ## Mean : 79.7 Mean :14.09 Mean :22.74 Mean : 9660 ## 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 3rd Qu.:10830 ## Max. :100.0 Max. :39.80 Max. :64.00 Max. :56233 ## Grad.Rate ## Min. : 10.00 ## 1st Qu.: 53.00 ## Median : 65.00 ## Mean : 65.46 ## 3rd Qu.: 78.00 ## Max. :118.00colDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print## Private Apps Accept Enroll Top10perc Top25perc ## 2 711 693 581 82 89 ## F.Undergrad P.Undergrad Outstate Room.Board Books Personal ## 714 566 640 553 122 294 ## PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate ## 78 65 173 61 744 81plotLEAP=function(leapObj){ par(mfrow = c(2,2)) bar2=which.max(leapObj$adjr2) bbic=which.min(leapObj$bic) bcp=which.min(leapObj$cp) plot(leapObj$rss,xlab=\u0026#34;Number of variables\u0026#34;,ylab=\u0026#34;RSS\u0026#34;,type=\u0026#34;b\u0026#34;) plot(leapObj$adjr2,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;Adjusted R^2\u0026#34;),type=\u0026#34;b\u0026#34;) points(bar2,leapObj$adjr2[bar2],col=\u0026#34;green\u0026#34;,cex=2,pch=20) plot(leapObj$bic,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;BIC\u0026#34;),type=\u0026#34;b\u0026#34;) points(bbic,leapObj$bic[bbic],col=\u0026#34;blue\u0026#34;,cex=2,pch=20) plot(leapObj$cp,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;C_p\u0026#34;),type=\u0026#34;b\u0026#34;) points(bcp,leapObj$cp[bcp],col=\u0026#34;red\u0026#34;,cex=2,pch=20) } a) Train test train_ind = sample(colDat %\u0026gt;% nrow,100) test_ind = setdiff(seq_len(colDat %\u0026gt;% nrow), train_ind) Best subset selection train_set\u0026lt;-colDat[train_ind,] test_set\u0026lt;-colDat[-train_ind,]library(leaps)modelFit\u0026lt;-regsubsets(Outstate~.,data=colDat,nvmax=20) modelFit %\u0026gt;% summary %\u0026gt;% print## Subset selection object ## Call: regsubsets.formula(Outstate ~ ., data = colDat, nvmax = 20) ## 17 Variables (and intercept) ## Forced in Forced out ## PrivateYes FALSE FALSE ## Apps FALSE FALSE ## Accept FALSE FALSE ## Enroll FALSE FALSE ## Top10perc FALSE FALSE ## Top25perc FALSE FALSE ## F.Undergrad FALSE FALSE ## P.Undergrad FALSE FALSE ## Room.Board FALSE FALSE ## Books FALSE FALSE ## Personal FALSE FALSE ## PhD FALSE FALSE ## Terminal FALSE FALSE ## S.F.Ratio FALSE FALSE ## perc.alumni FALSE FALSE ## Expend FALSE FALSE ## Grad.Rate FALSE FALSE ## 1 subsets of each size up to 17 ## Selection Algorithm: exhaustive ## PrivateYes Apps Accept Enroll Top10perc Top25perc F.Undergrad ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 11 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 12 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 13 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 14 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 15 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 16 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## P.Undergrad Room.Board Books Personal PhD Terminal S.F.Ratio ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 10 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 11 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 12 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 13 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 14 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 15 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 16 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## perc.alumni Expend Grad.Rate ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 11 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 12 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 13 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 14 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 15 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 16 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\npar(mfrow=c(2,2)) plot(modelFit) plot(modelFit,scale=\u0026#39;Cp\u0026#39;) plot(modelFit,scale=\u0026#39;r2\u0026#39;) plot(modelFit,scale=\u0026#39;adjr2\u0026#39;)  plotLEAP(modelFit %\u0026gt;% summary)   So we like 14 variables, namely   coefficients(modelFit,id=14) %\u0026gt;% print## (Intercept) PrivateYes Apps Accept Enroll ## -1.817040e+03 2.256946e+03 -2.999022e-01 8.023519e-01 -5.372545e-01 ## Top10perc F.Undergrad Room.Board Personal PhD ## 2.365529e+01 -9.569936e-02 8.741819e-01 -2.478418e-01 1.269506e+01 ## Terminal S.F.Ratio perc.alumni Expend Grad.Rate ## 2.297296e+01 -4.700560e+01 4.195006e+01 2.003912e-01 2.383197e+01  But five seems like a better bet.   coefficients(modelFit,id=5)## (Intercept) PrivateYes Room.Board PhD perc.alumni ## -2864.6325619 2936.7416766 1.0677573 40.5334088 61.3147684 ## Expend ## 0.2253945 b) GAM library(gam)fit = gam(Outstate ~ Private+s(Apps,3)+Accept+Enroll+ Top10perc+F.Undergrad+Room.Board+ Personal+PhD+Terminal+S.F.Ratio+ perc.alumni+Expend+Grad.Rate , data = colDat)## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument ## ignoredfit2 = gam(Outstate ~ Private+s(Room.Board,2)+s(PhD,3)+s(perc.alumni)+Expend , data = colDat)## Warning in model.matrix.default(mt, mf, contrasts): non-list contrasts argument ## ignoredpar(mfrow=c(2,2)) plot(fit,se=TRUE) par(mfrow=c(2,2)) plot(fit2,se=TRUE)  c) Evaluate pred \u0026lt;- predict(fit, test_set) mse.error \u0026lt;- mean((test_set$Outstate - pred)^2) mse.error %\u0026gt;% print## [1] 3691891gam.tss = mean((test_set$Outstate - mean(test_set$Outstate))^2) test.rss = 1 - mse.error/gam.tss test.rss %\u0026gt;% print## [1] 0.7731239pred2 \u0026lt;- predict(fit2, test_set) mse.error2 \u0026lt;- mean((test_set$Outstate - pred2)^2) mse.error2 %\u0026gt;% print## [1] 4121902gam.tss2 = mean((test_set$Outstate - mean(test_set$Outstate))^2) test.rss2 = 1 - mse.error2/gam.tss2 test.rss2 %\u0026gt;% print## [1] 0.7466987 This is pretty good model, all told.\nd) Summary summary(fit) %\u0026gt;% print## ## Call: gam(formula = Outstate ~ Private + s(Apps, 3) + Accept + Enroll + ## Top10perc + F.Undergrad + Room.Board + Personal + PhD + Terminal + ## S.F.Ratio + perc.alumni + Expend + Grad.Rate, data = colDat) ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6641.083 -1262.806 -5.698 1270.911 9965.901 ## ## (Dispersion Parameter for gaussian family taken to be 3749048) ## ## Null Deviance: 12559297426 on 776 degrees of freedom ## Residual Deviance: 2849276343 on 760 degrees of freedom ## AIC: 13985.3 ## ## Number of Local Scoring Iterations: 2 ## ## Anova for Parametric Effects ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Private 1 4034912907 4034912907 1076.250 \u0026lt; 2.2e-16 *** ## s(Apps, 3) 1 1344548030 1344548030 358.637 \u0026lt; 2.2e-16 *** ## Accept 1 90544274 90544274 24.151 1.091e-06 *** ## Enroll 1 144471570 144471570 38.535 8.838e-10 *** ## Top10perc 1 1802244831 1802244831 480.721 \u0026lt; 2.2e-16 *** ## F.Undergrad 1 45230645 45230645 12.065 0.0005430 *** ## Room.Board 1 1110285773 1110285773 296.151 \u0026lt; 2.2e-16 *** ## Personal 1 47886988 47886988 12.773 0.0003738 *** ## PhD 1 220249039 220249039 58.748 5.476e-14 *** ## Terminal 1 66366007 66366007 17.702 2.892e-05 *** ## S.F.Ratio 1 190811028 190811028 50.896 2.274e-12 *** ## perc.alumni 1 225293653 225293653 60.094 2.904e-14 *** ## Expend 1 258162295 258162295 68.861 4.805e-16 *** ## Grad.Rate 1 57947219 57947219 15.457 9.214e-05 *** ## Residuals 760 2849276343 3749048 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Anova for Nonparametric Effects ## Npar Df Npar F Pr(F) ## (Intercept) ## Private ## s(Apps, 3) 2 8.571 0.0002085 *** ## Accept ## Enroll ## Top10perc ## F.Undergrad ## Room.Board ## Personal ## PhD ## Terminal ## S.F.Ratio ## perc.alumni ## Expend ## Grad.Rate ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1summary(fit2) %\u0026gt;% print## ## Call: gam(formula = Outstate ~ Private + s(Room.Board, 2) + s(PhD, ## 3) + s(perc.alumni) + Expend, data = colDat) ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -8676.030 -1345.678 -8.409 1265.524 9590.459 ## ## (Dispersion Parameter for gaussian family taken to be 4175193) ## ## Null Deviance: 12559297426 on 776 degrees of freedom ## Residual Deviance: 3194023899 on 765.0002 degrees of freedom ## AIC: 14064.05 ## ## Number of Local Scoring Iterations: 2 ## ## Anova for Parametric Effects ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Private 1 3751107814 3751107814 898.43 \u0026lt; 2.2e-16 *** ## s(Room.Board, 2) 1 2913770756 2913770756 697.88 \u0026lt; 2.2e-16 *** ## s(PhD, 3) 1 1149711330 1149711330 275.37 \u0026lt; 2.2e-16 *** ## s(perc.alumni) 1 556759894 556759894 133.35 \u0026lt; 2.2e-16 *** ## Expend 1 554812125 554812125 132.88 \u0026lt; 2.2e-16 *** ## Residuals 765 3194023899 4175193 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Anova for Nonparametric Effects ## Npar Df Npar F Pr(F) ## (Intercept) ## Private ## s(Room.Board, 2) 1 4.9853 0.0258517 * ## s(PhD, 3) 2 9.1614 0.0001171 *** ## s(perc.alumni) 3 0.8726 0.4548496 ## Expend ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1","permalink":"https://rgoswami.me/posts/islr-ch7/","tags":["solutions","R","ISLR"],"title":"ISLR :: Moving Beyond Linearity"},{"categories":["programming"],"contents":" Chapter VI - Linear Model Selection and Regularization All the questions are as per the ISL seventh printing1.\nCommon Instead of using the standard functions, we will leverage the mlr3 package2.\n#install.packages(\u0026#34;mlr3\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;mlr3viz\u0026#34;,\u0026#34;mlr3learners\u0026#34;) Actually for R version 3.6.2, the steps to get it working were a bit more involved.\nLoad ISLR and other libraries.\nlibsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;, \u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;,\u0026#34;MASS\u0026#34;, \u0026#34;gridExtra\u0026#34;, \u0026#34;pls\u0026#34;,\u0026#34;latex2exp\u0026#34;,\u0026#34;data.table\u0026#34;) invisible(lapply(libsUsed, library, character.only = TRUE))## ## Attaching package: \u0026#39;dplyr\u0026#39;## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──## ✔ tibble 2.1.3 ✔ purrr 0.3.3 ## ✔ tidyr 1.0.0 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag()## Loading required package: lattice## ## Attaching package: \u0026#39;caret\u0026#39;## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## lift## ## Attaching package: \u0026#39;MASS\u0026#39;## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## select## ## Attaching package: \u0026#39;gridExtra\u0026#39;## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## combine## ## Attaching package: \u0026#39;pls\u0026#39;## The following object is masked from \u0026#39;package:caret\u0026#39;: ## ## R2## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## loadings## ## Attaching package: \u0026#39;data.table\u0026#39;## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## transpose## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## between, first, last Question 6.8 - Page 262 In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\n(a) Use the =rnorm()=function to generate a predictor \\(X\\) of length \\(n = 100\\), as well as a noise vector \\(\\eta\\) of length \\(n = 100\\).\n(b) Generate a response vector \\(Y\\) of length \\(n = 100\\) according to the model \\[Y = \\beta_0 + \\beta_1X + \\beta2X^2 + \\beta_3X^3 + \\eta\\], where \\(\\beta_{0}\\) , \\(\\beta_{1}\\), \\(\\beta_{2}\\), and \\(\\beta_{3}\\) are constants of your choice.\n\u0026copy; Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors \\(X\\), \\(X^{2}\\), \u0026hellip;, \\(X^{10}\\). What is the best model obtained according to \\(C_p\\) , BIC, and adjusted \\(R^2\\) ? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\).\n(d) Repeat \u0026copy;, using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in \u0026copy;?\n(e) Now fit a lasso model to the simulated data, again using \\(X\\), \\(X^{2}\\), \u0026hellip;, \\(X^{10}\\) as predictors. Use cross-validation to select the optimal value of \\(\\lambda\\). Create plots of the cross-validation error as a function of \\(\\lambda\\). Report the resulting coefficient estimates, and discuss the results obtained.\n(f) Now generate a response vector Y according to the model \\[Y = \\beta_{0} + \\beta_{7}X^{7} + \\eta,\\] and perform best subset selection and the lasso. Discuss the results obtained.\nAnswer a) Generate model set.seed(1984) x\u0026lt;-rnorm(100) noise\u0026lt;-rnorm(100) b) Response vector beta=c(43,5,3,6) y\u0026lt;-beta[1] + beta[2]*x + beta[3]*x^2 + beta[4]*x^3 + noise qplot(x,y)  c) Subset selection Since the question requires it, we will be using the leaps libraries.\nlibrary(leaps) df\u0026lt;-data.frame(y=y,x=x) sets=regsubsets(y~poly(x,10,raw=T),data=df,nvmax=10) sets %\u0026gt;% summary## Subset selection object ## Call: regsubsets.formula(y ~ poly(x, 10, raw = T), data = df, nvmax = 10) ## 10 Variables (and intercept) ## Forced in Forced out ## poly(x, 10, raw = T)1 FALSE FALSE ## poly(x, 10, raw = T)2 FALSE FALSE ## poly(x, 10, raw = T)3 FALSE FALSE ## poly(x, 10, raw = T)4 FALSE FALSE ## poly(x, 10, raw = T)5 FALSE FALSE ## poly(x, 10, raw = T)6 FALSE FALSE ## poly(x, 10, raw = T)7 FALSE FALSE ## poly(x, 10, raw = T)8 FALSE FALSE ## poly(x, 10, raw = T)9 FALSE FALSE ## poly(x, 10, raw = T)10 FALSE FALSE ## 1 subsets of each size up to 10 ## Selection Algorithm: exhaustive ## poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 poly(x, 10, raw = T)3 ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## poly(x, 10, raw = T)4 poly(x, 10, raw = T)5 poly(x, 10, raw = T)6 ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## poly(x, 10, raw = T)7 poly(x, 10, raw = T)8 poly(x, 10, raw = T)9 ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## poly(x, 10, raw = T)10 ## 1 ( 1 ) \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34;*\u0026#34; ## 6 ( 1 ) \u0026#34;*\u0026#34; ## 7 ( 1 ) \u0026#34;*\u0026#34; ## 8 ( 1 ) \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; We also want the best parameters.\nsummarySet\u0026lt;-summary(sets) which.min(summarySet$cp) %\u0026gt;% print## [1] 3which.min(summarySet$bic) %\u0026gt;% print## [1] 3which.max(summarySet$adjr2) %\u0026gt;% print## [1] 7 We might want to see this as a plot.\nplot(summarySet$cp, xlab = \u0026#34;Subset Size\u0026#34;, ylab = \u0026#34;Cp\u0026#34;, pch = 20, type = \u0026#34;l\u0026#34;) points(3,summarySet$cp[3],pch=4,col=\u0026#39;red\u0026#39;,lwd=7)  plot(summarySet$bic, xlab = \u0026#34;Subset Size\u0026#34;, ylab = \u0026#34;BIC\u0026#34;, pch = 20, type = \u0026#34;l\u0026#34;) points(3,summarySet$bic[3],pch=4,col=\u0026#39;red\u0026#39;,lwd=7)  plot(summarySet$adjr2, xlab = \u0026#34;Subset Size\u0026#34;, ylab = \u0026#34;Adjusted R2\u0026#34;, pch = 20, type = \u0026#34;l\u0026#34;) points(3,summarySet$adjr2[3],pch=4,col=\u0026#39;red\u0026#39;,lwd=7)  Lets check the coefficients.\ncoefficients(sets,id=3) %\u0026gt;% print## (Intercept) poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 ## 42.895657 5.108094 3.034408 ## poly(x, 10, raw = T)3 ## 5.989367beta %\u0026gt;% print## [1] 43 5 3 6 We see that we actually have a pretty good set of coefficients.\nd) Forward and backward stepwise models modelX\u0026lt;-poly(x,10,raw=T) forwardFit\u0026lt;-regsubsets(y~modelX,data=df,nvmax=10,method=\u0026#34;forward\u0026#34;) forwardFit %\u0026gt;% summary %\u0026gt;% print## Subset selection object ## Call: regsubsets.formula(y ~ modelX, data = df, nvmax = 10, method = \u0026#34;forward\u0026#34;) ## 10 Variables (and intercept) ## Forced in Forced out ## modelX1 FALSE FALSE ## modelX2 FALSE FALSE ## modelX3 FALSE FALSE ## modelX4 FALSE FALSE ## modelX5 FALSE FALSE ## modelX6 FALSE FALSE ## modelX7 FALSE FALSE ## modelX8 FALSE FALSE ## modelX9 FALSE FALSE ## modelX10 FALSE FALSE ## 1 subsets of each size up to 10 ## Selection Algorithm: forward ## modelX1 modelX2 modelX3 modelX4 modelX5 modelX6 modelX7 modelX8 ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## modelX9 modelX10 ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\npar(mfrow=c(2,2)) plot(forwardFit) plot(forwardFit,scale=\u0026#39;Cp\u0026#39;) plot(forwardFit,scale=\u0026#39;r2\u0026#39;) plot(forwardFit,scale=\u0026#39;adjr2\u0026#39;)  I find these not as fun to look at, so we will do better.\nplotLEAP=function(leapObj){ par(mfrow = c(2,2)) bar2=which.max(leapObj$adjr2) bbic=which.min(leapObj$bic) bcp=which.min(leapObj$cp) plot(leapObj$rss,xlab=\u0026#34;Number of variables\u0026#34;,ylab=\u0026#34;RSS\u0026#34;,type=\u0026#34;b\u0026#34;) plot(leapObj$adjr2,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;Adjusted R^2\u0026#34;),type=\u0026#34;b\u0026#34;) points(bar2,leapObj$adjr2[bar2],col=\u0026#34;green\u0026#34;,cex=2,pch=20) plot(leapObj$bic,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;BIC\u0026#34;),type=\u0026#34;b\u0026#34;) points(bbic,leapObj$bic[bbic],col=\u0026#34;blue\u0026#34;,cex=2,pch=20) plot(leapObj$cp,xlab=\u0026#34;Number of variables\u0026#34;,ylab=TeX(\u0026#34;C_p\u0026#34;),type=\u0026#34;b\u0026#34;) points(bcp,leapObj$cp[bcp],col=\u0026#34;red\u0026#34;,cex=2,pch=20) }plotLEAP(forwardFit %\u0026gt;% summary)  Lets check the backward selection as well.\nmodelX\u0026lt;-poly(x,10,raw=T) backwardFit\u0026lt;-regsubsets(y~modelX,data=df,nvmax=10,method=\u0026#34;backward\u0026#34;) backwardFit %\u0026gt;% summary %\u0026gt;% print## Subset selection object ## Call: regsubsets.formula(y ~ modelX, data = df, nvmax = 10, method = \u0026#34;backward\u0026#34;) ## 10 Variables (and intercept) ## Forced in Forced out ## modelX1 FALSE FALSE ## modelX2 FALSE FALSE ## modelX3 FALSE FALSE ## modelX4 FALSE FALSE ## modelX5 FALSE FALSE ## modelX6 FALSE FALSE ## modelX7 FALSE FALSE ## modelX8 FALSE FALSE ## modelX9 FALSE FALSE ## modelX10 FALSE FALSE ## 1 subsets of each size up to 10 ## Selection Algorithm: backward ## modelX1 modelX2 modelX3 modelX4 modelX5 modelX6 modelX7 modelX8 ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## modelX9 modelX10 ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\npar(mfrow=c(2,2)) plot(backwardFit) plot(backwardFit,scale=\u0026#39;Cp\u0026#39;) plot(backwardFit,scale=\u0026#39;r2\u0026#39;) plot(backwardFit,scale=\u0026#39;adjr2\u0026#39;)  plotLEAP(backwardFit %\u0026gt;% summary)  In spite of some slight variations, overall all methods converge to the same best set of parameters, that of the third model.\ne) LASSO and Cross Validation For this, instead of using glmnet directly, we will use caret.\ndf\u0026lt;-df %\u0026gt;% mutate(x2=x^2,x3=x^3, x4=x^4,x5=x^5, x6=x^6,x7=x^7, x8=x^8,x9=x^9, x10=x^10)lambda\u0026lt;-10^seq(-3, 3, length = 100) lassoCaret= train(y~.,data=df,method=\u0026#34;glmnet\u0026#34;,tuneGrid=expand.grid(alpha=1,lambda=lambda))## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures.lassoCaret %\u0026gt;% print## glmnet ## ## 100 samples ## 10 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 100, 100, 100, 100, 100, 100, ... ## Resampling results across tuning parameters: ## ## lambda RMSE Rsquared MAE ## 1.000000e-03 1.009696 0.9965632 0.8051425 ## 1.149757e-03 1.009696 0.9965632 0.8051425 ## 1.321941e-03 1.009696 0.9965632 0.8051425 ## 1.519911e-03 1.009696 0.9965632 0.8051425 ## 1.747528e-03 1.009696 0.9965632 0.8051425 ## 2.009233e-03 1.009696 0.9965632 0.8051425 ## 2.310130e-03 1.009696 0.9965632 0.8051425 ## 2.656088e-03 1.009696 0.9965632 0.8051425 ## 3.053856e-03 1.009696 0.9965632 0.8051425 ## 3.511192e-03 1.009696 0.9965632 0.8051425 ## 4.037017e-03 1.009696 0.9965632 0.8051425 ## 4.641589e-03 1.009696 0.9965632 0.8051425 ## 5.336699e-03 1.009696 0.9965632 0.8051425 ## 6.135907e-03 1.009696 0.9965632 0.8051425 ## 7.054802e-03 1.009696 0.9965632 0.8051425 ## 8.111308e-03 1.009696 0.9965632 0.8051425 ## 9.326033e-03 1.009696 0.9965632 0.8051425 ## 1.072267e-02 1.009696 0.9965632 0.8051425 ## 1.232847e-02 1.009696 0.9965632 0.8051425 ## 1.417474e-02 1.009696 0.9965632 0.8051425 ## 1.629751e-02 1.009696 0.9965632 0.8051425 ## 1.873817e-02 1.009696 0.9965632 0.8051425 ## 2.154435e-02 1.009696 0.9965632 0.8051425 ## 2.477076e-02 1.009696 0.9965632 0.8051425 ## 2.848036e-02 1.009696 0.9965632 0.8051425 ## 3.274549e-02 1.009696 0.9965632 0.8051425 ## 3.764936e-02 1.009696 0.9965632 0.8051425 ## 4.328761e-02 1.009696 0.9965632 0.8051425 ## 4.977024e-02 1.009696 0.9965632 0.8051425 ## 5.722368e-02 1.009696 0.9965632 0.8051425 ## 6.579332e-02 1.009696 0.9965632 0.8051425 ## 7.564633e-02 1.009637 0.9965632 0.8050666 ## 8.697490e-02 1.009216 0.9965637 0.8047862 ## 1.000000e-01 1.008901 0.9965636 0.8046468 ## 1.149757e-01 1.009470 0.9965616 0.8054790 ## 1.321941e-01 1.011206 0.9965561 0.8074253 ## 1.519911e-01 1.014475 0.9965476 0.8104930 ## 1.747528e-01 1.019202 0.9965383 0.8147296 ## 2.009233e-01 1.025943 0.9965259 0.8203974 ## 2.310130e-01 1.035374 0.9965094 0.8284187 ## 2.656088e-01 1.048294 0.9964878 0.8393282 ## 3.053856e-01 1.065717 0.9964592 0.8530952 ## 3.511192e-01 1.088903 0.9964215 0.8701072 ## 4.037017e-01 1.119433 0.9963715 0.8918217 ## 4.641589e-01 1.158919 0.9963053 0.9193677 ## 5.336699e-01 1.209841 0.9962136 0.9532842 ## 6.135907e-01 1.275467 0.9960778 0.9957151 ## 7.054802e-01 1.357247 0.9958966 1.0471169 ## 8.111308e-01 1.457886 0.9956561 1.1087362 ## 9.326033e-01 1.580743 0.9953362 1.1818188 ## 1.072267e+00 1.729330 0.9949070 1.2696235 ## 1.232847e+00 1.907599 0.9943306 1.3758463 ## 1.417474e+00 2.120178 0.9935518 1.5059031 ## 1.629751e+00 2.369642 0.9924954 1.6673393 ## 1.873817e+00 2.662906 0.9910539 1.8621728 ## 2.154435e+00 3.007271 0.9890638 2.0978907 ## 2.477076e+00 3.409377 0.9863097 2.3788439 ## 2.848036e+00 3.864727 0.9825900 2.7053428 ## 3.274549e+00 4.350785 0.9778541 3.0659309 ## 3.764936e+00 4.847045 0.9724311 3.4403210 ## 4.328761e+00 5.369017 0.9668240 3.8351441 ## 4.977024e+00 5.919492 0.9626812 4.2512694 ## 5.722368e+00 6.562134 0.9580843 4.7389049 ## 6.579332e+00 7.307112 0.9534537 5.2945905 ## 7.564633e+00 8.132296 0.9500300 5.8774541 ## 8.697490e+00 9.067321 0.9486589 6.4760997 ## 1.000000e+01 10.167822 0.9483195 7.1226569 ## 1.149757e+01 11.473284 0.9482975 7.8556639 ## 1.321941e+01 13.002703 0.9482975 8.6990451 ## 1.519911e+01 14.727852 0.9454119 9.6414650 ## 1.747528e+01 16.325210 0.9426796 10.5303097 ## 2.009233e+01 17.740599 0.9357286 11.3560865 ## 2.310130e+01 18.585795 0.9227167 11.8799668 ## 2.656088e+01 18.939596 0.9080584 12.1336575 ## 3.053856e+01 19.123568 0.9109065 12.2733471 ## 3.511192e+01 19.197966 NaN 12.3308613 ## 4.037017e+01 19.197966 NaN 12.3308613 ## 4.641589e+01 19.197966 NaN 12.3308613 ## 5.336699e+01 19.197966 NaN 12.3308613 ## 6.135907e+01 19.197966 NaN 12.3308613 ## 7.054802e+01 19.197966 NaN 12.3308613 ## 8.111308e+01 19.197966 NaN 12.3308613 ## 9.326033e+01 19.197966 NaN 12.3308613 ## 1.072267e+02 19.197966 NaN 12.3308613 ## 1.232847e+02 19.197966 NaN 12.3308613 ## 1.417474e+02 19.197966 NaN 12.3308613 ## 1.629751e+02 19.197966 NaN 12.3308613 ## 1.873817e+02 19.197966 NaN 12.3308613 ## 2.154435e+02 19.197966 NaN 12.3308613 ## 2.477076e+02 19.197966 NaN 12.3308613 ## 2.848036e+02 19.197966 NaN 12.3308613 ## 3.274549e+02 19.197966 NaN 12.3308613 ## 3.764936e+02 19.197966 NaN 12.3308613 ## 4.328761e+02 19.197966 NaN 12.3308613 ## 4.977024e+02 19.197966 NaN 12.3308613 ## 5.722368e+02 19.197966 NaN 12.3308613 ## 6.579332e+02 19.197966 NaN 12.3308613 ## 7.564633e+02 19.197966 NaN 12.3308613 ## 8.697490e+02 19.197966 NaN 12.3308613 ## 1.000000e+03 19.197966 NaN 12.3308613 ## ## Tuning parameter \u0026#39;alpha\u0026#39; was held constant at a value of 1 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 1 and lambda = 0.1.lassoCaret %\u0026gt;% ggplot  lassoCaret %\u0026gt;% varImp %\u0026gt;% ggplot  library(glmnet)## Loading required package: Matrix## ## Attaching package: \u0026#39;Matrix\u0026#39;## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: ## ## expand, pack, unpack## Loaded glmnet 3.0-2library(boot)## ## Attaching package: \u0026#39;boot\u0026#39;## The following object is masked from \u0026#39;package:lattice\u0026#39;: ## ## melanomalasso.mod \u0026lt;- cv.glmnet(as.matrix(df[-1]), y, alpha=1) lambda \u0026lt;- lasso.mod$lambda.min plot(lasso.mod)  predict(lasso.mod, s=lambda, type=\u0026#34;coefficients\u0026#34;)## 11 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) 42.975240 ## x 5.005023 ## x2 2.947540 ## x3 5.989105 ## x4 . ## x5 . ## x6 . ## x7 . ## x8 . ## x9 . ## x10 . Clearly, the only important variables are \\(x\\), \\(x^2\\) and \\(x^3\\).\nf) New model Our new model requires a newly expanded set of betas as well.\ny2\u0026lt;-beta[1]+23*x^7+noisemodelX\u0026lt;-poly(x,10,raw=T) newDF\u0026lt;-data.frame(x=as.matrix(modelX),y=y2) newSub\u0026lt;-regsubsets(y2~.,data=newDF,nvmax=10) newSub %\u0026gt;% summary## Subset selection object ## Call: regsubsets.formula(y2 ~ ., data = newDF, nvmax = 10) ## 11 Variables (and intercept) ## Forced in Forced out ## x.1 FALSE FALSE ## x.2 FALSE FALSE ## x.3 FALSE FALSE ## x.4 FALSE FALSE ## x.5 FALSE FALSE ## x.6 FALSE FALSE ## x.7 FALSE FALSE ## x.8 FALSE FALSE ## x.9 FALSE FALSE ## x.10 FALSE FALSE ## y FALSE FALSE ## 1 subsets of each size up to 10 ## Selection Algorithm: exhaustive ## x.1 x.2 x.3 x.4 x.5 x.6 x.7 x.8 x.9 x.10 y ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 3 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34;plotLEAP(newSub %\u0026gt;% summary)  Or in its more native look,\npar(mfrow=c(2,2)) plot(newSub) plot(newSub,scale=\u0026#39;Cp\u0026#39;) plot(newSub,scale=\u0026#39;r2\u0026#39;) plot(newSub,scale=\u0026#39;adjr2\u0026#39;)  library(glmnet) library(boot) lasso.mod2 \u0026lt;- cv.glmnet(as.matrix(newDF[-1]), y, alpha=1) lambda2 \u0026lt;- lasso.mod2$lambda.min plot(lasso.mod2)  predict(lasso.mod2, s=lambda, type=\u0026#34;coefficients\u0026#34;)## 11 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) 42.67982691 ## x.2 3.22521396 ## x.3 8.56699146 ## x.4 . ## x.5 -0.10229572 ## x.6 . ## x.7 -0.03184905 ## x.8 . ## x.9 . ## x.10 . ## y .lambda\u0026lt;-10^seq(-3, 3, length = 100) lassocaret2= train(y~.,data=newDF,method=\u0026#34;glmnet\u0026#34;,tuneGrid=expand.grid(alpha=1,lambda=lambda))## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures.lassocaret2 %\u0026gt;% print## glmnet ## ## 100 samples ## 10 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 100, 100, 100, 100, 100, 100, ... ## Resampling results across tuning parameters: ## ## lambda RMSE Rsquared MAE ## 1.000000e-03 40.03231 0.9999955 14.48774 ## 1.149757e-03 40.03231 0.9999955 14.48774 ## 1.321941e-03 40.03231 0.9999955 14.48774 ## 1.519911e-03 40.03231 0.9999955 14.48774 ## 1.747528e-03 40.03231 0.9999955 14.48774 ## 2.009233e-03 40.03231 0.9999955 14.48774 ## 2.310130e-03 40.03231 0.9999955 14.48774 ## 2.656088e-03 40.03231 0.9999955 14.48774 ## 3.053856e-03 40.03231 0.9999955 14.48774 ## 3.511192e-03 40.03231 0.9999955 14.48774 ## 4.037017e-03 40.03231 0.9999955 14.48774 ## 4.641589e-03 40.03231 0.9999955 14.48774 ## 5.336699e-03 40.03231 0.9999955 14.48774 ## 6.135907e-03 40.03231 0.9999955 14.48774 ## 7.054802e-03 40.03231 0.9999955 14.48774 ## 8.111308e-03 40.03231 0.9999955 14.48774 ## 9.326033e-03 40.03231 0.9999955 14.48774 ## 1.072267e-02 40.03231 0.9999955 14.48774 ## 1.232847e-02 40.03231 0.9999955 14.48774 ## 1.417474e-02 40.03231 0.9999955 14.48774 ## 1.629751e-02 40.03231 0.9999955 14.48774 ## 1.873817e-02 40.03231 0.9999955 14.48774 ## 2.154435e-02 40.03231 0.9999955 14.48774 ## 2.477076e-02 40.03231 0.9999955 14.48774 ## 2.848036e-02 40.03231 0.9999955 14.48774 ## 3.274549e-02 40.03231 0.9999955 14.48774 ## 3.764936e-02 40.03231 0.9999955 14.48774 ## 4.328761e-02 40.03231 0.9999955 14.48774 ## 4.977024e-02 40.03231 0.9999955 14.48774 ## 5.722368e-02 40.03231 0.9999955 14.48774 ## 6.579332e-02 40.03231 0.9999955 14.48774 ## 7.564633e-02 40.03231 0.9999955 14.48774 ## 8.697490e-02 40.03231 0.9999955 14.48774 ## 1.000000e-01 40.03231 0.9999955 14.48774 ## 1.149757e-01 40.03231 0.9999955 14.48774 ## 1.321941e-01 40.03231 0.9999955 14.48774 ## 1.519911e-01 40.03231 0.9999955 14.48774 ## 1.747528e-01 40.03231 0.9999955 14.48774 ## 2.009233e-01 40.03231 0.9999955 14.48774 ## 2.310130e-01 40.03231 0.9999955 14.48774 ## 2.656088e-01 40.03231 0.9999955 14.48774 ## 3.053856e-01 40.03231 0.9999955 14.48774 ## 3.511192e-01 40.03231 0.9999955 14.48774 ## 4.037017e-01 40.03231 0.9999955 14.48774 ## 4.641589e-01 40.03231 0.9999955 14.48774 ## 5.336699e-01 40.03231 0.9999955 14.48774 ## 6.135907e-01 40.03231 0.9999955 14.48774 ## 7.054802e-01 40.03231 0.9999955 14.48774 ## 8.111308e-01 40.03231 0.9999955 14.48774 ## 9.326033e-01 40.03231 0.9999955 14.48774 ## 1.072267e+00 40.03231 0.9999955 14.48774 ## 1.232847e+00 40.03231 0.9999955 14.48774 ## 1.417474e+00 40.03231 0.9999955 14.48774 ## 1.629751e+00 40.03231 0.9999955 14.48774 ## 1.873817e+00 40.03231 0.9999955 14.48774 ## 2.154435e+00 40.03231 0.9999955 14.48774 ## 2.477076e+00 40.03231 0.9999955 14.48774 ## 2.848036e+00 40.03231 0.9999955 14.48774 ## 3.274549e+00 40.03231 0.9999955 14.48774 ## 3.764936e+00 40.03231 0.9999955 14.48774 ## 4.328761e+00 40.03231 0.9999955 14.48774 ## 4.977024e+00 40.03231 0.9999955 14.48774 ## 5.722368e+00 40.03231 0.9999955 14.48774 ## 6.579332e+00 40.03231 0.9999955 14.48774 ## 7.564633e+00 40.43005 0.9999955 14.59881 ## 8.697490e+00 41.25214 0.9999955 14.81913 ## 1.000000e+01 42.30446 0.9999955 15.09937 ## 1.149757e+01 43.59429 0.9999955 15.44307 ## 1.321941e+01 45.43633 0.9999955 15.93255 ## 1.519911e+01 47.55425 0.9999955 16.49605 ## 1.747528e+01 49.98935 0.9999955 17.14447 ## 2.009233e+01 52.90533 0.9999955 17.91650 ## 2.310130e+01 57.57589 0.9999955 19.10125 ## 2.656088e+01 63.25484 0.9999955 20.53147 ## 3.053856e+01 70.51580 0.9999955 22.36400 ## 3.511192e+01 78.93391 0.9999955 24.49105 ## 4.037017e+01 88.61274 0.9999955 26.93830 ## 4.641589e+01 99.97831 0.9999955 29.83601 ## 5.336699e+01 113.48225 0.9999955 33.39320 ## 6.135907e+01 129.17536 0.9999955 37.58303 ## 7.054802e+01 147.76452 0.9999957 42.74333 ## 8.111308e+01 169.60027 0.9999961 48.98043 ## 9.326033e+01 194.94266 0.9999965 56.29001 ## 1.072267e+02 224.07631 0.9999969 64.70026 ## 1.232847e+02 257.56092 0.9999971 74.36989 ## 1.417474e+02 296.13382 0.9999971 85.51504 ## 1.629751e+02 340.49129 0.9999971 98.33212 ## 1.873817e+02 391.49185 0.9999971 113.06864 ## 2.154435e+02 450.13031 0.9999971 130.01206 ## 2.477076e+02 509.28329 0.9999970 147.15405 ## 2.848036e+02 564.17558 0.9999969 163.34475 ## 3.274549e+02 618.84080 0.9999969 179.85589 ## 3.764936e+02 681.69265 0.9999969 198.83969 ## 4.328761e+02 741.14452 0.9999967 217.28049 ## 4.977024e+02 807.25385 0.9999967 237.88938 ## 5.722368e+02 883.26360 0.9999967 261.58461 ## 6.579332e+02 970.65640 0.9999967 288.82836 ## 7.564633e+02 1037.84801 0.9999960 312.54099 ## 8.697490e+02 1088.92551 0.9999960 334.04769 ## 1.000000e+03 1131.46176 0.9999955 354.62317 ## ## Tuning parameter \u0026#39;alpha\u0026#39; was held constant at a value of 1 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 1 and lambda = 6.579332.lassocaret2 %\u0026gt;% ggplot  lassocaret2 %\u0026gt;% varImp %\u0026gt;% ggplot  Clearly, the LASSO model has correctly reduced the model down to the correct single variable form, though best subset seems to suggest using more predictors, their coefficients are low enough to recognize that they are noise.\nQuestion 6.9 - Page 263 In this exercise, we will predict the number of applications received using the other variables in the College data set.\n(a) Split the data set into a training set and a test set.\n(b) Fit a linear model using least squares on the training set, and report the test error obtained.\n\u0026copy; Fit a ridge regression model on the training set, with \\(\\lambda\\) chosen by cross-validation. Report the test error obtained.\n(d) Fit a lasso model on the training set, with \\(\\lambda\\) chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\n(e) Fit a PCR model on the training set, with \\(M\\) chosen by crossvalidation. Report the test error obtained, along with the value of \\(M\\) selected by cross-validation.\n(f) Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.\n(g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?\nAnswer We will use the caret package, since at the moment, mlr3 does not have learners for PCR and PLS.\ncolDat\u0026lt;-ISLR::College colDat %\u0026gt;% summary %\u0026gt;% print## Private Apps Accept Enroll Top10perc ## No :212 Min. : 81 Min. : 72 Min. : 35 Min. : 1.00 ## Yes:565 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 1st Qu.:15.00 ## Median : 1558 Median : 1110 Median : 434 Median :23.00 ## Mean : 3002 Mean : 2019 Mean : 780 Mean :27.56 ## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 3rd Qu.:35.00 ## Max. :48094 Max. :26330 Max. :6392 Max. :96.00 ## Top25perc F.Undergrad P.Undergrad Outstate ## Min. : 9.0 Min. : 139 Min. : 1.0 Min. : 2340 ## 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 1st Qu.: 7320 ## Median : 54.0 Median : 1707 Median : 353.0 Median : 9990 ## Mean : 55.8 Mean : 3700 Mean : 855.3 Mean :10441 ## 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 3rd Qu.:12925 ## Max. :100.0 Max. :31643 Max. :21836.0 Max. :21700 ## Room.Board Books Personal PhD ## Min. :1780 Min. : 96.0 Min. : 250 Min. : 8.00 ## 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 1st Qu.: 62.00 ## Median :4200 Median : 500.0 Median :1200 Median : 75.00 ## Mean :4358 Mean : 549.4 Mean :1341 Mean : 72.66 ## 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 3rd Qu.: 85.00 ## Max. :8124 Max. :2340.0 Max. :6800 Max. :103.00 ## Terminal S.F.Ratio perc.alumni Expend ## Min. : 24.0 Min. : 2.50 Min. : 0.00 Min. : 3186 ## 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 1st Qu.: 6751 ## Median : 82.0 Median :13.60 Median :21.00 Median : 8377 ## Mean : 79.7 Mean :14.09 Mean :22.74 Mean : 9660 ## 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 3rd Qu.:10830 ## Max. :100.0 Max. :39.80 Max. :64.00 Max. :56233 ## Grad.Rate ## Min. : 10.00 ## 1st Qu.: 53.00 ## Median : 65.00 ## Mean : 65.46 ## 3rd Qu.: 78.00 ## Max. :118.00colDat %\u0026gt;% str %\u0026gt;% print## \u0026#39;data.frame\u0026#39;: 777 obs. of 18 variables: ## $ Private : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Apps : num 1660 2186 1428 417 193 ... ## $ Accept : num 1232 1924 1097 349 146 ... ## $ Enroll : num 721 512 336 137 55 158 103 489 227 172 ... ## $ Top10perc : num 23 16 22 60 16 38 17 37 30 21 ... ## $ Top25perc : num 52 29 50 89 44 62 45 68 63 44 ... ## $ F.Undergrad: num 2885 2683 1036 510 249 ... ## $ P.Undergrad: num 537 1227 99 63 869 ... ## $ Outstate : num 7440 12280 11250 12960 7560 ... ## $ Room.Board : num 3300 6450 3750 5450 4120 ... ## $ Books : num 450 750 400 450 800 500 500 450 300 660 ... ## $ Personal : num 2200 1500 1165 875 1500 ... ## $ PhD : num 70 29 53 92 76 67 90 89 79 40 ... ## $ Terminal : num 78 30 66 97 72 73 93 100 84 41 ... ## $ S.F.Ratio : num 18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ... ## $ perc.alumni: num 12 16 30 37 2 11 26 37 23 15 ... ## $ Expend : num 7041 10527 8735 19016 10922 ... ## $ Grad.Rate : num 60 56 54 59 15 55 63 73 80 52 ... ## NULLcolDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print## Private Apps Accept Enroll Top10perc Top25perc ## 2 711 693 581 82 89 ## F.Undergrad P.Undergrad Outstate Room.Board Books Personal ## 714 566 640 553 122 294 ## PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate ## 78 65 173 61 744 81 Clearly, there are no psuedo-factors which might have been converted at this stage.\na) Train-Test split train_ind\u0026lt;-createDataPartition(colDat$Apps,p=0.8,times=1,list=FALSE) train_set\u0026lt;-colDat[train_ind,] test_set\u0026lt;-colDat[-train_ind,] b) Linear least squares linCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;lm\u0026#34;) linCol %\u0026gt;% summary## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5145.6 -414.8 -20.3 340.5 7526.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -2.918e+02 4.506e+02 -0.648 0.517486 ## PrivateYes -5.351e+02 1.532e+02 -3.494 0.000511 *** ## Accept 1.617e+00 4.258e-02 37.983 \u0026lt; 2e-16 *** ## Enroll -1.012e+00 1.959e-01 -5.165 3.26e-07 *** ## Top10perc 5.379e+01 6.221e+00 8.647 \u0026lt; 2e-16 *** ## Top25perc -1.632e+01 5.046e+00 -3.235 0.001282 ** ## F.Undergrad 6.836e-02 3.457e-02 1.978 0.048410 * ## P.Undergrad 7.929e-02 3.367e-02 2.355 0.018854 * ## Outstate -7.303e-02 2.098e-02 -3.481 0.000536 *** ## Room.Board 1.695e-01 5.367e-02 3.159 0.001663 ** ## Books 9.998e-02 2.578e-01 0.388 0.698328 ## Personal -3.145e-03 6.880e-02 -0.046 0.963553 ## PhD -8.926e+00 5.041e+00 -1.771 0.077112 . ## Terminal -2.298e+00 5.608e+00 -0.410 0.682152 ## S.F.Ratio 6.038e+00 1.420e+01 0.425 0.670757 ## perc.alumni -5.085e-01 4.560e+00 -0.112 0.911249 ## Expend 4.668e-02 1.332e-02 3.505 0.000490 *** ## Grad.Rate 9.042e+00 3.379e+00 2.676 0.007653 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1042 on 606 degrees of freedom ## Multiple R-squared: 0.9332, Adjusted R-squared: 0.9313 ## F-statistic: 497.7 on 17 and 606 DF, p-value: \u0026lt; 2.2e-16linPred\u0026lt;-predict(linCol,test_set) linPred %\u0026gt;% postResample(obs = test_set$Apps)## RMSE Rsquared MAE ## 1071.6360025 0.9017032 625.7827996 Do note that the metrics are calculated in a manner to ensure no negative values are obtained.\nc) Ridge regression with CV for λ L2Grid \u0026lt;- expand.grid(alpha=0, lambda=10^seq(from=-3,to=30,length=100))ridgCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid)## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures.ridgCol %\u0026gt;% summary %\u0026gt;% print## Length Class Mode ## a0 100 -none- numeric ## beta 1700 dgCMatrix S4 ## df 100 -none- numeric ## dim 2 -none- numeric ## lambda 100 -none- numeric ## dev.ratio 100 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## call 5 -none- call ## nobs 1 -none- numeric ## lambdaOpt 1 -none- numeric ## xNames 17 -none- character ## problemType 1 -none- character ## tuneValue 2 data.frame list ## obsLevels 1 -none- logical ## param 0 -none- listcoef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print## 18 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) -1.407775e+03 ## PrivateYes -5.854245e+02 ## Accept 1.042778e+00 ## Enroll 3.511219e-01 ## Top10perc 2.780211e+01 ## Top25perc 2.883536e-02 ## F.Undergrad 6.825141e-02 ## P.Undergrad 5.281320e-02 ## Outstate -2.011504e-02 ## Room.Board 2.155224e-01 ## Books 1.517585e-01 ## Personal -3.711406e-02 ## PhD -4.453155e+00 ## Terminal -3.783231e+00 ## S.F.Ratio 6.897360e+00 ## perc.alumni -9.301831e+00 ## Expend 5.601144e-02 ## Grad.Rate 1.259989e+01ggplot(ridgCol)  ridgPred\u0026lt;-predict(ridgCol,test_set) ridgPred %\u0026gt;% postResample(obs = test_set$Apps)## RMSE Rsquared MAE ## 1047.7545250 0.9051726 644.4535063 d) LASSO with CV for λ L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso lambda=10^seq(from=-3,to=30,length=100))lassoCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid)## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures.lassoCol %\u0026gt;% summary %\u0026gt;% print## Length Class Mode ## a0 81 -none- numeric ## beta 1377 dgCMatrix S4 ## df 81 -none- numeric ## dim 2 -none- numeric ## lambda 81 -none- numeric ## dev.ratio 81 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## call 5 -none- call ## nobs 1 -none- numeric ## lambdaOpt 1 -none- numeric ## xNames 17 -none- character ## problemType 1 -none- character ## tuneValue 2 data.frame list ## obsLevels 1 -none- logical ## param 0 -none- listcoef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print## 18 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) -325.51554340 ## PrivateYes -532.28956305 ## Accept 1.60370798 ## Enroll -0.90158328 ## Top10perc 51.96610325 ## Top25perc -14.87886847 ## F.Undergrad 0.05352324 ## P.Undergrad 0.07832395 ## Outstate -0.07047302 ## Room.Board 0.16783269 ## Books 0.08836704 ## Personal . ## PhD -8.67634519 ## Terminal -2.18494018 ## S.F.Ratio 5.25050018 ## perc.alumni -0.67848535 ## Expend 0.04597728 ## Grad.Rate 8.67569015ggplot(lassoCol)  lassoPred\u0026lt;-predict(lassoCol,test_set) lassoPred %\u0026gt;% postResample(obs = test_set$Apps)## RMSE Rsquared MAE ## 1068.9834769 0.9021268 622.7029418 e) PCR with CV for M mGrid \u0026lt;- expand.grid(ncomp=seq(from=1,to=20,length=10))pcrCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;pcr\u0026#34;,tuneGrid = mGrid) pcrCol %\u0026gt;% summary %\u0026gt;% print## Data: X dimension: 624 17 ## Y dimension: 624 1 ## Fit method: svdpc ## Number of components considered: 17 ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps ## X 48.2314 87.24 95.02 97.26 98.63 99.43 99.91 ## .outcome 0.2419 76.54 77.88 80.19 91.27 91.34 91.34 ## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps ## X 99.96 100.00 100.00 100.00 100.00 100.00 100.00 ## .outcome 91.65 91.66 92.26 92.65 92.66 92.67 92.76 ## 15 comps 16 comps 17 comps ## X 100.00 100.00 100.00 ## .outcome 93.17 93.18 93.32 ## NULLggplot(pcrCol)  pcrPred\u0026lt;-predict(pcrCol,test_set) pcrPred %\u0026gt;% postResample(obs = test_set$Apps)## RMSE Rsquared MAE ## 1071.6360025 0.9017032 625.7827996 f) PLS with CV for M plsCol\u0026lt;-train(Apps~.,data=train_set,method=\u0026#34;pls\u0026#34;,tuneGrid = mGrid) plsCol %\u0026gt;% summary %\u0026gt;% print## Data: X dimension: 624 17 ## Y dimension: 624 1 ## Fit method: oscorespls ## Number of components considered: 17 ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps ## X 39.02 56.4 91.83 96.61 98.62 99.22 99.49 ## .outcome 78.04 84.1 86.88 91.09 91.38 91.49 91.66 ## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps ## X 99.96 99.99 100.00 100.00 100.00 100.00 100.00 ## .outcome 91.68 91.85 92.64 92.87 93.16 93.18 93.18 ## 15 comps 16 comps 17 comps ## X 100.00 100.00 100.00 ## .outcome 93.18 93.19 93.32 ## NULLggplot(plsCol)  plsPred\u0026lt;-predict(plsCol,test_set) plsPred %\u0026gt;% postResample(obs = test_set$Apps)## RMSE Rsquared MAE ## 1071.6360039 0.9017032 625.7827987 g) Comments and Comparison models \u0026lt;- list(ridge = ridgCol, lasso = lassoCol, pcr = pcrCol, pls=plsCol,linear=linCol) resamples(models) %\u0026gt;% summary## ## Call: ## summary.resamples(object = .) ## ## Models: ridge, lasso, pcr, pls, linear ## Number of resamples: 25 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## ridge 536.9612 600.5398 623.2005 649.6713 707.4014 793.4972 0 ## lasso 573.8563 616.3883 671.9453 655.8858 691.7620 732.2155 0 ## pcr 576.1427 618.8694 650.0360 662.9040 714.8491 767.5535 0 ## pls 553.3999 607.9757 637.1985 638.6619 668.5120 735.4479 0 ## linear 556.5553 619.2395 654.1478 659.4635 686.7747 792.4912 0 ## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## ridge 882.2646 920.5934 1000.519 1168.603 1163.377 1939.541 0 ## lasso 801.9415 990.0724 1168.234 1184.329 1302.221 1584.712 0 ## pcr 828.1370 942.2678 1131.207 1144.071 1284.178 1544.078 0 ## pls 786.7989 1038.3265 1167.764 1157.026 1274.041 1461.434 0 ## linear 798.3771 1063.3690 1134.291 1135.977 1215.115 1403.576 0 ## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## ridge 0.8735756 0.8962010 0.9185736 0.9136429 0.9306819 0.9474913 0 ## lasso 0.8851991 0.9132766 0.9217660 0.9191638 0.9284838 0.9398772 0 ## pcr 0.8658504 0.9080179 0.9235117 0.9146884 0.9281892 0.9471991 0 ## pls 0.8881249 0.9080786 0.9183968 0.9173632 0.9258994 0.9420894 0 ## linear 0.8840049 0.8986452 0.9222319 0.9160913 0.9296275 0.9492198 0resamples(models) %\u0026gt;% bwplot(scales=\u0026#34;free\u0026#34;)   Given the tighter spread of PLS, it seems more reliable than PCR Ridge is just poor in every way OLS does well, but it also has a worrying outlier LASSO appears to be doing alright as well We also have kept track of the performance on the test_set  We might want to see the variable significance values as well.\nlgp\u0026lt;-linCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;OLS Variable Importance\u0026#34;) rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) pcgp\u0026lt;-pcrCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PCR Variable Importance\u0026#34;) plgp\u0026lt;-plsCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PLS Variable Importance\u0026#34;) grid.arrange(lgp,rgp,lsgp,pcgp,plgp,ncol=3)  Question 6.10 - Pages 263-264 We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.\n(a) Generate a data set with \\(p = 20\\) features, \\(n = 1,000\\) observations, and an associated quantitative response vector generated according to the model \\[Y = X\\beta + \\eta,\\] where \\(\\beta\\) has some elements that are exactly equal to zero.\n(b) Split your data set into a training set containing \\(100\\) observations and a test set containing \\(900\\) observations.\n\u0026copy; Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.\n(d) Plot the test set MSE associated with the best model of each size.\n(e) For which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.\n(f) How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.\n(g) Create a plot displaying \\(\\sqrt{\\Sum_{j=1}^{p}(\\beta_{j}-\\hat{\\beta}_{j}^{r})^{2}}\\) for a range of values of \\(r\\), where \\(\\hat{\\beta}_{j}^{r}\\) is the $j$th coefficient estimate for the best model containing \\(r\\) coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?\nAnswer Model creation p=20 n=1000 noise\u0026lt;-rnorm(n) xmat\u0026lt;-matrix(rnorm(n*p),nrow=n,ncol=p) beta\u0026lt;-sample(-10:34,20) beta[sample(1:20,4)]=0 myY\u0026lt;-xmat %*% beta + noise modelDat\u0026lt;-data.frame(x=as.matrix(xmat),y=myY)  As always we will want to take a peak   modelDat %\u0026gt;% str %\u0026gt;% print## \u0026#39;data.frame\u0026#39;: 1000 obs. of 21 variables: ## $ x.1 : num -0.406 -1.375 0.858 -0.231 -0.601 ... ## $ x.2 : num -0.129 -0.218 -0.17 0.573 -0.513 ... ## $ x.3 : num 0.127 -0.224 1.014 0.896 0.159 ... ## $ x.4 : num 0.499 -0.151 -0.488 -0.959 2.187 ... ## $ x.5 : num -0.235 -0.345 -0.773 -0.346 0.773 ... ## $ x.6 : num 0.26 -0.429 -1.183 -1.159 0.959 ... ## $ x.7 : num 0.567 1.647 0.149 -0.593 -0.902 ... ## $ x.8 : num -0.092 0.8391 -1.4835 0.0229 -0.1353 ... ## $ x.9 : num -0.998 -1.043 -0.563 -0.377 0.324 ... ## $ x.10: num -0.4401 -0.195 -0.5139 -0.0156 -0.9543 ... ## $ x.11: num -0.147 0.829 0.165 0.101 -0.105 ... ## $ x.12: num -0.0118 1.02 1.0794 1.3184 -2.2844 ... ## $ x.13: num -1.683 0.487 -1.142 -0.744 -0.175 ... ## $ x.14: num 0.228 -1.031 -2.798 -0.646 0.56 ... ## $ x.15: num -0.718 0.508 0.637 -0.556 0.585 ... ## $ x.16: num -1.6378 0.581 -0.9939 0.0537 -0.5854 ... ## $ x.17: num 1.758 -0.616 1.377 -0.876 -1.174 ... ## $ x.18: num -1.438 0.373 1.364 0.399 0.949 ... ## $ x.19: num -0.715 -0.731 1.142 0.149 0.916 ... ## $ x.20: num 2.774 -2.024 1.316 0.138 0.187 ... ## $ y : num 77.5 -82.8 -38.9 -79.7 64.9 ... ## NULLmodelDat %\u0026gt;% summary %\u0026gt;% print## x.1 x.2 x.3 x.4 ## Min. :-2.79766 Min. :-3.13281 Min. :-2.71232 Min. :-4.29604 ## 1st Qu.:-0.60516 1st Qu.:-0.66759 1st Qu.:-0.60561 1st Qu.:-0.66598 ## Median : 0.04323 Median : 0.03681 Median : 0.06556 Median : 0.06589 ## Mean : 0.06879 Mean : 0.01004 Mean : 0.06443 Mean : 0.02244 ## 3rd Qu.: 0.74049 3rd Qu.: 0.68234 3rd Qu.: 0.70521 3rd Qu.: 0.71174 ## Max. : 3.50354 Max. : 3.47268 Max. : 3.02817 Max. : 3.27326 ## x.5 x.6 x.7 x.8 ## Min. :-3.228376 Min. :-4.24014 Min. :-2.98577 Min. :-3.27770 ## 1st Qu.:-0.698220 1st Qu.:-0.69448 1st Qu.:-0.59092 1st Qu.:-0.52939 ## Median :-0.058778 Median :-0.01141 Median : 0.01732 Median : 0.05703 ## Mean : 0.000126 Mean :-0.05158 Mean : 0.04767 Mean : 0.08231 ## 3rd Qu.: 0.663570 3rd Qu.: 0.64217 3rd Qu.: 0.67438 3rd Qu.: 0.72849 ## Max. : 3.036307 Max. : 3.27572 Max. : 2.72163 Max. : 3.33409 ## x.9 x.10 x.11 x.12 ## Min. :-3.08957 Min. :-3.21268 Min. :-3.00572 Min. :-3.72016 ## 1st Qu.:-0.65456 1st Qu.:-0.69401 1st Qu.:-0.68226 1st Qu.:-0.63043 ## Median :-0.04242 Median :-0.03069 Median :-0.04777 Median : 0.07079 ## Mean : 0.02049 Mean :-0.02400 Mean :-0.03729 Mean : 0.03769 ## 3rd Qu.: 0.71209 3rd Qu.: 0.61540 3rd Qu.: 0.64873 3rd Qu.: 0.67155 ## Max. : 3.23110 Max. : 2.76059 Max. : 2.87306 Max. : 3.48569 ## x.13 x.14 x.15 x.16 ## Min. :-3.20126 Min. :-3.55432 Min. :-2.857575 Min. :-3.5383 ## 1st Qu.:-0.68535 1st Qu.:-0.66752 1st Qu.:-0.658708 1st Qu.:-0.7813 ## Median :-0.01329 Median :-0.03302 Median : 0.020581 Median :-0.0740 ## Mean : 0.01094 Mean : 0.02113 Mean : 0.007976 Mean :-0.0883 ## 3rd Qu.: 0.64877 3rd Qu.: 0.74919 3rd Qu.: 0.670464 3rd Qu.: 0.5568 ## Max. : 2.78973 Max. : 3.47923 Max. : 2.891527 Max. : 3.0938 ## x.17 x.18 x.19 x.20 ## Min. :-3.28570 Min. :-4.06416 Min. :-3.0443 Min. :-4.06307 ## 1st Qu.:-0.72302 1st Qu.:-0.72507 1st Qu.:-0.6684 1st Qu.:-0.70518 ## Median :-0.02439 Median :-0.04941 Median :-0.0610 Median :-0.07697 ## Mean :-0.01459 Mean :-0.03164 Mean :-0.0414 Mean :-0.05302 ## 3rd Qu.: 0.62692 3rd Qu.: 0.68115 3rd Qu.: 0.6381 3rd Qu.: 0.58597 ## Max. : 2.86446 Max. : 3.32958 Max. : 3.1722 Max. : 3.01358 ## y ## Min. :-199.268 ## 1st Qu.: -54.758 ## Median : -1.607 ## Mean : -1.710 ## 3rd Qu.: 49.367 ## Max. : 278.244 b) Train Test Split train_ind = sample(modelDat %\u0026gt;% nrow,100) test_ind = setdiff(seq_len(modelDat %\u0026gt;% nrow), train_set) Best subset selection train_set\u0026lt;-modelDat[train_ind,] test_set\u0026lt;-modelDat[-train_ind,]linCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;lm\u0026#34;) linCol %\u0026gt;% summary## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.12474 -0.53970 -0.00944 0.42398 2.21086 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.06052 0.09604 -0.630 0.530 ## x.1 -0.02265 0.09198 -0.246 0.806 ## x.2 28.91650 0.09879 292.719 \u0026lt;2e-16 *** ## x.3 14.16532 0.09343 151.610 \u0026lt;2e-16 *** ## x.4 28.16256 0.09828 286.564 \u0026lt;2e-16 *** ## x.5 0.13742 0.09658 1.423 0.159 ## x.6 27.01497 0.08540 316.341 \u0026lt;2e-16 *** ## x.7 31.15917 0.09003 346.092 \u0026lt;2e-16 *** ## x.8 -9.66308 0.11095 -87.094 \u0026lt;2e-16 *** ## x.9 0.11641 0.10768 1.081 0.283 ## x.10 19.06687 0.09662 197.344 \u0026lt;2e-16 *** ## x.11 -9.09956 0.08627 -105.472 \u0026lt;2e-16 *** ## x.12 -8.01933 0.10198 -78.633 \u0026lt;2e-16 *** ## x.13 4.26852 0.09888 43.170 \u0026lt;2e-16 *** ## x.14 20.22366 0.09853 205.247 \u0026lt;2e-16 *** ## x.15 -0.16607 0.10466 -1.587 0.117 ## x.16 7.95594 0.11250 70.721 \u0026lt;2e-16 *** ## x.17 10.89851 0.11157 97.684 \u0026lt;2e-16 *** ## x.18 -1.09760 0.09391 -11.688 \u0026lt;2e-16 *** ## x.19 22.05197 0.08697 253.553 \u0026lt;2e-16 *** ## x.20 20.88796 0.09274 225.221 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.8583 on 79 degrees of freedom ## Multiple R-squared: 0.9999, Adjusted R-squared: 0.9999 ## F-statistic: 4.71e+04 on 20 and 79 DF, p-value: \u0026lt; 2.2e-16linPred\u0026lt;-predict(linCol,test_set) linPred %\u0026gt;% postResample(obs = test_set$y)## RMSE Rsquared MAE ## 1.2151815 0.9997265 0.9638378L2Grid \u0026lt;- expand.grid(alpha=0, lambda=10^seq(from=-3,to=30,length=100))ridgCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid)## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures.ridgCol %\u0026gt;% summary %\u0026gt;% print## Length Class Mode ## a0 100 -none- numeric ## beta 2000 dgCMatrix S4 ## df 100 -none- numeric ## dim 2 -none- numeric ## lambda 100 -none- numeric ## dev.ratio 100 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## call 5 -none- call ## nobs 1 -none- numeric ## lambdaOpt 1 -none- numeric ## xNames 20 -none- character ## problemType 1 -none- character ## tuneValue 2 data.frame list ## obsLevels 1 -none- logical ## param 0 -none- listcoef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print## 21 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) 0.03898376 ## x.1 -0.12140945 ## x.2 27.63771674 ## x.3 13.46853844 ## x.4 26.54402352 ## x.5 -0.13838118 ## x.6 25.87706885 ## x.7 29.90687677 ## x.8 -9.42088971 ## x.9 -0.08983349 ## x.10 17.45444598 ## x.11 -8.33991071 ## x.12 -7.23653865 ## x.13 3.35145521 ## x.14 19.42178898 ## x.15 -0.02794731 ## x.16 7.63951382 ## x.17 11.08083907 ## x.18 -1.36872894 ## x.19 20.90257005 ## x.20 20.07494414ggplot(ridgCol)  ridgPred\u0026lt;-predict(ridgCol,test_set) ridgPred %\u0026gt;% postResample(obs = test_set$y)## RMSE Rsquared MAE ## 3.7554417 0.9994231 3.0184859L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso lambda=10^seq(from=-3,to=30,length=100))lassoCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid)## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures.lassoCol %\u0026gt;% summary %\u0026gt;% print## Length Class Mode ## a0 47 -none- numeric ## beta 940 dgCMatrix S4 ## df 47 -none- numeric ## dim 2 -none- numeric ## lambda 47 -none- numeric ## dev.ratio 47 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## call 5 -none- call ## nobs 1 -none- numeric ## lambdaOpt 1 -none- numeric ## xNames 20 -none- character ## problemType 1 -none- character ## tuneValue 2 data.frame list ## obsLevels 1 -none- logical ## param 0 -none- listcoef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print## 21 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) 0.1158884 ## x.1 . ## x.2 28.5897869 ## x.3 13.3637110 ## x.4 27.2558797 ## x.5 . ## x.6 26.6625588 ## x.7 30.6841774 ## x.8 -9.1388677 ## x.9 . ## x.10 17.9220939 ## x.11 -8.2461257 ## x.12 -7.0603651 ## x.13 3.2052101 ## x.14 19.7219890 ## x.15 . ## x.16 7.2082509 ## x.17 10.4137411 ## x.18 -0.6693664 ## x.19 21.5357460 ## x.20 20.5226071ggplot(lassoCol)  lassoPred\u0026lt;-predict(lassoCol,test_set) lassoPred %\u0026gt;% postResample(obs = test_set$y)## RMSE Rsquared MAE ## 2.7289452 0.9992454 2.2029482mGrid \u0026lt;- expand.grid(ncomp=seq(from=1,to=20,length=10))pcrCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;pcr\u0026#34;,tuneGrid = mGrid) pcrCol %\u0026gt;% summary %\u0026gt;% print## Data: X dimension: 100 20 ## Y dimension: 100 1 ## Fit method: svdpc ## Number of components considered: 20 ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps ## X 10.040 18.46 26.62 34.56 41.87 48.54 54.56 ## .outcome 8.425 34.90 41.09 43.12 45.06 48.09 66.44 ## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps ## X 60.33 65.37 70.01 74.46 78.72 82.32 85.67 ## .outcome 85.76 88.81 89.93 91.66 91.91 92.04 92.08 ## 15 comps 16 comps 17 comps 18 comps 19 comps 20 comps ## X 88.85 91.94 94.59 96.73 98.51 100.00 ## .outcome 92.15 94.96 99.51 99.57 99.76 99.99 ## NULLggplot(pcrCol)  pcrPred\u0026lt;-predict(pcrCol,test_set) pcrPred %\u0026gt;% postResample(obs = test_set$y)## RMSE Rsquared MAE ## 1.2151815 0.9997265 0.9638378plsCol\u0026lt;-train(y~.,data=train_set,method=\u0026#34;pls\u0026#34;,tuneGrid = mGrid) plsCol %\u0026gt;% summary %\u0026gt;% print## Data: X dimension: 100 20 ## Y dimension: 100 1 ## Fit method: oscorespls ## Number of components considered: 20 ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps ## X 7.762 14.79 21.01 26.89 31.55 36.13 41.12 ## .outcome 92.765 98.81 99.75 99.96 99.98 99.99 99.99 ## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps ## X 46.35 51.21 56.12 60.34 65.63 71.57 76.16 ## .outcome 99.99 99.99 99.99 99.99 99.99 99.99 99.99 ## 15 comps 16 comps 17 comps 18 comps 19 comps 20 comps ## X 80.72 84.69 88.98 92.69 96.71 100.00 ## .outcome 99.99 99.99 99.99 99.99 99.99 99.99 ## NULLggplot(plsCol)  plsPred\u0026lt;-predict(plsCol,test_set) plsPred %\u0026gt;% postResample(obs = test_set$y)## RMSE Rsquared MAE ## 1.2151815 0.9997265 0.9638378 d) Test MSE for best models  All the models have the same R² but Ridge does the worst followed by LASSO  For the rest of the question, we will consider the OLS model.\nmodelFit\u0026lt;-regsubsets(y~.,data=modelDat,nvmax=20) modelFit %\u0026gt;% summary %\u0026gt;% print## Subset selection object ## Call: regsubsets.formula(y ~ ., data = modelDat, nvmax = 20) ## 20 Variables (and intercept) ## Forced in Forced out ## x.1 FALSE FALSE ## x.2 FALSE FALSE ## x.3 FALSE FALSE ## x.4 FALSE FALSE ## x.5 FALSE FALSE ## x.6 FALSE FALSE ## x.7 FALSE FALSE ## x.8 FALSE FALSE ## x.9 FALSE FALSE ## x.10 FALSE FALSE ## x.11 FALSE FALSE ## x.12 FALSE FALSE ## x.13 FALSE FALSE ## x.14 FALSE FALSE ## x.15 FALSE FALSE ## x.16 FALSE FALSE ## x.17 FALSE FALSE ## x.18 FALSE FALSE ## x.19 FALSE FALSE ## x.20 FALSE FALSE ## 1 subsets of each size up to 20 ## Selection Algorithm: exhaustive ## x.1 x.2 x.3 x.4 x.5 x.6 x.7 x.8 x.9 x.10 x.11 x.12 x.13 x.14 x.15 ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 10 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 11 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 12 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 13 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 14 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 15 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 16 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 18 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 19 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 20 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## x.16 x.17 x.18 x.19 x.20 ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 5 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 6 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 7 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 8 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 11 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 12 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 13 ( 1 ) \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 14 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 15 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 16 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 17 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 18 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 19 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 20 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\npar(mfrow=c(2,2)) plot(modelFit) plot(modelFit,scale=\u0026#39;Cp\u0026#39;) plot(modelFit,scale=\u0026#39;r2\u0026#39;) plot(modelFit,scale=\u0026#39;adjr2\u0026#39;)  plotLEAP(modelFit %\u0026gt;% summary)  It would appear that 16 variables would be a good bet. We note that the lasso model did void out 4 parameters, namely x₁,x₃,x₁₃ and x₁₇.\nLets take a quick look at the various model variable significance values.\nlgp\u0026lt;-linCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;OLS Variable Importance\u0026#34;) rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) pcgp\u0026lt;-pcrCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PCR Variable Importance\u0026#34;) plgp\u0026lt;-plsCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;PLS Variable Importance\u0026#34;) grid.arrange(lgp,rgp,lsgp,pcgp,plgp,ncol=3,bottom=\u0026#34;Effective Importance, scaled\u0026#34;)  e) Model size The test set numeric minimum RMSE is a tie between OLS and PCR, and this was achieved for the (effective) 16 variable OLS model, as well as the 18 variable PCR model.\nf) Best model We will consider the OLS and PCR models and its parameters.\nlinCol$finalModel %\u0026gt;% print## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Coefficients: ## (Intercept) x.1 x.2 x.3 x.4 x.5 ## -0.06052 -0.02265 28.91650 14.16532 28.16256 0.13742 ## x.6 x.7 x.8 x.9 x.10 x.11 ## 27.01497 31.15917 -9.66308 0.11641 19.06687 -9.09956 ## x.12 x.13 x.14 x.15 x.16 x.17 ## -8.01933 4.26852 20.22366 -0.16607 7.95594 10.89851 ## x.18 x.19 x.20 ## -1.09760 22.05197 20.88796pcrCol$bestTune %\u0026gt;% print## ncomp ## 10 20 Now to compare this to the original.\nbeta %\u0026gt;% print## [1] 0 29 14 28 0 27 31 -10 0 19 -9 -8 4 20 0 8 11 -1 22 ## [20] 21t=data.frame(linCol$finalModel$coefficients[-1]) %\u0026gt;% rename(\u0026#34;Model_Coeffs\u0026#34;=1) %\u0026gt;% add_column(beta) %\u0026gt;% rename(\u0026#34;Original_Coeffs\u0026#34;=2) print(t)## Model_Coeffs Original_Coeffs ## x.1 -0.02265289 0 ## x.2 28.91649699 29 ## x.3 14.16532050 14 ## x.4 28.16255937 28 ## x.5 0.13741621 0 ## x.6 27.01497459 27 ## x.7 31.15917172 31 ## x.8 -9.66308362 -10 ## x.9 0.11641282 0 ## x.10 19.06687041 19 ## x.11 -9.09955826 -9 ## x.12 -8.01932598 -8 ## x.13 4.26852334 4 ## x.14 20.22366153 20 ## x.15 -0.16606531 0 ## x.16 7.95593559 8 ## x.17 10.89851353 11 ## x.18 -1.09759687 -1 ## x.19 22.05196537 22 ## x.20 20.88795623 21 We see that the coefficients are pretty similar.\ng) Plotting differences val.errors = rep(NaN, p) a = rep(NaN, p) b = rep(NaN, p) x_cols = colnames(xmat, do.NULL = FALSE, prefix = \u0026#34;x.\u0026#34;) for (i in 1:p) { coefi = coef(modelFit, id = i) a[i] = length(coefi) - 1 ## Not counting the intercept b[i] = sqrt(sum((beta[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) + sum(beta[!(x_cols %in% names(coefi))])^2) ## Handling the intercept } plot(x = a, y = b, xlab = \u0026#34;Number of Coefficients\u0026#34;, ylab = \u0026#34;Relative Error\u0026#34;)  Question 6.11 - Page 264 We will now try to predict per capita crime rate in the Boston data set.\n(a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.\n(b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, crossvalidation, or some other reasonable alternative, as opposed to using training error.\n\u0026copy; Does your chosen model involve all of the features in the data set? Why or why not?\nAnswer boston\u0026lt;-MASS::Boston  Summarize   boston %\u0026gt;% str %\u0026gt;% print## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... ## NULLboston %\u0026gt;% summary %\u0026gt;% print## crim zn indus chas ## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 ## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 ## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 ## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 ## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 ## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 ## nox rm age dis ## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 ## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 ## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 ## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 ## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 ## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 ## rad tax ptratio black ## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 ## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 ## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 ## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 ## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 ## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 ## lstat medv ## Min. : 1.73 Min. : 5.00 ## 1st Qu.: 6.95 1st Qu.:17.02 ## Median :11.36 Median :21.20 ## Mean :12.65 Mean :22.53 ## 3rd Qu.:16.95 3rd Qu.:25.00 ## Max. :37.97 Max. :50.00boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print## crim zn indus chas nox rm age dis rad tax ## 504 26 76 2 81 446 356 412 9 66 ## ptratio black lstat medv ## 46 357 455 229 a) Test regression models train_ind = sample(boston %\u0026gt;% nrow,100) test_ind = setdiff(seq_len(boston %\u0026gt;% nrow), train_set)train_set\u0026lt;-boston[train_ind,] test_set\u0026lt;-boston[-train_ind,]linCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;lm\u0026#34;) linCol %\u0026gt;% summary## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.2431 -1.0344 -0.0563 0.8187 8.1318 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.9339246 7.6508393 0.122 0.9031 ## zn 0.0046819 0.0157375 0.297 0.7668 ## indus 0.0276209 0.0875254 0.316 0.7531 ## chas -1.1602278 1.2386869 -0.937 0.3516 ## nox -7.5024503 5.0207818 -1.494 0.1388 ## rm 1.1240874 0.7462340 1.506 0.1356 ## age 0.0020182 0.0137404 0.147 0.8836 ## dis -0.3934753 0.2454365 -1.603 0.1126 ## rad 0.4540613 0.0791580 5.736 1.41e-07 *** ## tax 0.0008469 0.0052593 0.161 0.8724 ## ptratio -0.2978204 0.1637629 -1.819 0.0725 . ## black 0.0030642 0.0045281 0.677 0.5004 ## lstat 0.1322779 0.0626485 2.111 0.0376 * ## medv -0.0841382 0.0590700 -1.424 0.1580 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.362 on 86 degrees of freedom ## Multiple R-squared: 0.775, Adjusted R-squared: 0.741 ## F-statistic: 22.78 on 13 and 86 DF, p-value: \u0026lt; 2.2e-16linPred\u0026lt;-predict(linCol,test_set) linPred %\u0026gt;% postResample(obs = test_set$crim)## RMSE Rsquared MAE ## 7.3794735 0.4056002 2.6774969L2Grid \u0026lt;- expand.grid(alpha=0, lambda=10^seq(from=-3,to=30,length=100))ridgCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid)## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures.ridgCol %\u0026gt;% summary %\u0026gt;% print## Length Class Mode ## a0 100 -none- numeric ## beta 1300 dgCMatrix S4 ## df 100 -none- numeric ## dim 2 -none- numeric ## lambda 100 -none- numeric ## dev.ratio 100 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## call 5 -none- call ## nobs 1 -none- numeric ## lambdaOpt 1 -none- numeric ## xNames 13 -none- character ## problemType 1 -none- character ## tuneValue 2 data.frame list ## obsLevels 1 -none- logical ## param 0 -none- listcoef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print## 14 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) -3.881166065 ## zn 0.002597790 ## indus -0.005103517 ## chas -0.674764337 ## nox -0.053645732 ## rm 0.600064844 ## age 0.001153570 ## dis -0.179295384 ## rad 0.267082956 ## tax 0.006447932 ## ptratio -0.075885753 ## black -0.001650403 ## lstat 0.086462700 ## medv -0.027519270ggplot(ridgCol)  ridgPred\u0026lt;-predict(ridgCol,test_set) ridgPred %\u0026gt;% postResample(obs = test_set$crim)## RMSE Rsquared MAE ## 7.5065916 0.4017056 2.4777547L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso lambda=10^seq(from=-3,to=30,length=100))lassoCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid)## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures.lassoCol %\u0026gt;% summary %\u0026gt;% print## Length Class Mode ## a0 78 -none- numeric ## beta 1014 dgCMatrix S4 ## df 78 -none- numeric ## dim 2 -none- numeric ## lambda 78 -none- numeric ## dev.ratio 78 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## call 5 -none- call ## nobs 1 -none- numeric ## lambdaOpt 1 -none- numeric ## xNames 13 -none- character ## problemType 1 -none- character ## tuneValue 2 data.frame list ## obsLevels 1 -none- logical ## param 0 -none- listcoef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print## 14 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) -2.024006430 ## zn . ## indus . ## chas . ## nox . ## rm . ## age . ## dis -0.008506188 ## rad 0.386379255 ## tax 0.001779579 ## ptratio . ## black . ## lstat 0.040788606 ## medv .ggplot(lassoCol)  lassoPred\u0026lt;-predict(lassoCol,test_set) lassoPred %\u0026gt;% postResample(obs = test_set$crim)## RMSE Rsquared MAE ## 7.5868293 0.3859121 2.4892258mGrid \u0026lt;- expand.grid(ncomp=seq(from=1,to=20,length=10))  All the models have the same R² but Ridge does the worst followed by LASSO  For the rest of the question, we will consider the OLS model.\nmodelFit\u0026lt;-regsubsets(crim~.,data=boston,nvmax=20) modelFit %\u0026gt;% summary %\u0026gt;% print## Subset selection object ## Call: regsubsets.formula(crim ~ ., data = boston, nvmax = 20) ## 13 Variables (and intercept) ## Forced in Forced out ## zn FALSE FALSE ## indus FALSE FALSE ## chas FALSE FALSE ## nox FALSE FALSE ## rm FALSE FALSE ## age FALSE FALSE ## dis FALSE FALSE ## rad FALSE FALSE ## tax FALSE FALSE ## ptratio FALSE FALSE ## black FALSE FALSE ## lstat FALSE FALSE ## medv FALSE FALSE ## 1 subsets of each size up to 13 ## Selection Algorithm: exhaustive ## zn indus chas nox rm age dis rad tax ptratio black lstat medv ## 1 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; ## 2 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 3 ( 1 ) \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; ## 4 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 5 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 6 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 7 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; ## 8 ( 1 ) \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 9 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 10 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 11 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 12 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34; \u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; ## 13 ( 1 ) \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; We might want to take a look at these.\npar(mfrow=c(2,2)) plot(modelFit) plot(modelFit,scale=\u0026#39;Cp\u0026#39;) plot(modelFit,scale=\u0026#39;r2\u0026#39;) plot(modelFit,scale=\u0026#39;adjr2\u0026#39;)  plotLEAP(modelFit %\u0026gt;% summary)  It would appear that 16 variables would be a good bet. We note that the lasso model did void out 4 parameters, namely x₁,x₃,x₁₃ and x₁₇.\nLets take a quick look at the various model variable significance values.\nlgp\u0026lt;-linCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;OLS Variable Importance\u0026#34;) rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) grid.arrange(lgp,rgp,lsgp,ncol=2,bottom=\u0026#34;Effective Importance, scaled\u0026#34;)  b) Propose a model  Given the data and plots, I would probably end up using the Ridge regression model\n Clearly, LASSO is not working very well since it seems to have taken mainly 3 variables, one of which is largely categorical (9 levels)\n  c) Model properties boston %\u0026gt;% str %\u0026gt;% print## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... ## NULLboston %\u0026gt;% summary %\u0026gt;% print## crim zn indus chas ## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 ## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 ## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 ## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 ## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 ## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 ## nox rm age dis ## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 ## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 ## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 ## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 ## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 ## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 ## rad tax ptratio black ## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 ## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 ## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 ## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 ## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 ## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 ## lstat medv ## Min. : 1.73 Min. : 5.00 ## 1st Qu.: 6.95 1st Qu.:17.02 ## Median :11.36 Median :21.20 ## Mean :12.65 Mean :22.53 ## 3rd Qu.:16.95 3rd Qu.:25.00 ## Max. :37.97 Max. :50.00boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length) %\u0026gt;% print## crim zn indus chas nox rm age dis rad tax ## 504 26 76 2 81 446 356 412 9 66 ## ptratio black lstat medv ## 46 357 455 229  A good idea would be removing rad and chas from the regression   boston\u0026lt;-boston %\u0026gt;% subset(select=-c(rad,chas))train_ind = sample(boston %\u0026gt;% nrow,100) test_ind = setdiff(seq_len(boston %\u0026gt;% nrow), train_set)train_set\u0026lt;-boston[train_ind,] test_set\u0026lt;-boston[-train_ind,]L2Grid \u0026lt;- expand.grid(alpha=0, lambda=10^seq(from=-3,to=30,length=100))ridgCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L2Grid)## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures.ridgCol %\u0026gt;% summary %\u0026gt;% print## Length Class Mode ## a0 100 -none- numeric ## beta 1100 dgCMatrix S4 ## df 100 -none- numeric ## dim 2 -none- numeric ## lambda 100 -none- numeric ## dev.ratio 100 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## call 5 -none- call ## nobs 1 -none- numeric ## lambdaOpt 1 -none- numeric ## xNames 11 -none- character ## problemType 1 -none- character ## tuneValue 2 data.frame list ## obsLevels 1 -none- logical ## param 0 -none- listcoef(ridgCol$finalModel, ridgCol$bestTune$lambda) %\u0026gt;% print## 12 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) 0.23015398 ## zn 0.01595378 ## indus -0.01006683 ## nox 3.42104450 ## rm -0.05904911 ## age 0.01419585 ## dis -0.16724715 ## tax 0.01021790 ## ptratio 0.05741508 ## black -0.01414707 ## lstat 0.15519923 ## medv -0.04483670ggplot(ridgCol)  ridgPred\u0026lt;-predict(ridgCol,test_set) ridgPred %\u0026gt;% postResample(obs = test_set$crim)## RMSE Rsquared MAE ## 6.9365371 0.3619974 2.8570012L1Grid \u0026lt;- expand.grid(alpha=1, # for lasso lambda=10^seq(from=-3,to=30,length=100))lassoCol\u0026lt;-train(crim~.,data=train_set,method=\u0026#34;glmnet\u0026#34;,tuneGrid = L1Grid)## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures.lassoCol %\u0026gt;% summary %\u0026gt;% print## Length Class Mode ## a0 74 -none- numeric ## beta 814 dgCMatrix S4 ## df 74 -none- numeric ## dim 2 -none- numeric ## lambda 74 -none- numeric ## dev.ratio 74 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## call 5 -none- call ## nobs 1 -none- numeric ## lambdaOpt 1 -none- numeric ## xNames 11 -none- character ## problemType 1 -none- character ## tuneValue 2 data.frame list ## obsLevels 1 -none- logical ## param 0 -none- listcoef(lassoCol$finalModel, lassoCol$bestTune$lambda) %\u0026gt;% print## 12 x 1 sparse Matrix of class \u0026#34;dgCMatrix\u0026#34; ## 1 ## (Intercept) 0.86635621 ## zn . ## indus . ## nox . ## rm . ## age . ## dis . ## tax 0.01402174 ## ptratio . ## black -0.01454496 ## lstat 0.15290845 ## medv .ggplot(lassoCol)  lassoPred\u0026lt;-predict(lassoCol,test_set) lassoPred %\u0026gt;% postResample(obs = test_set$crim)## RMSE Rsquared MAE ## 6.9630616 0.3693364 2.6767742rgp\u0026lt;-ridgCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Ridge Variable Importance\u0026#34;) lsgp\u0026lt;-lassoCol %\u0026gt;% varImp %\u0026gt;% ggplot + ggtitle(\u0026#34;Lasso Variable Importance\u0026#34;) grid.arrange(rgp,lsgp,ncol=2,bottom=\u0026#34;Effective Importance, scaled\u0026#34;)  None of these models are actually any good apparently, given that we have an R² of 0.3619974 for the L2 regularization and 0.3693364 for the L1.\n James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media. [return] Lang et al., (2019). mlr3: A modern object-oriented machine learning framework in R. Journal of Open Source Software, 4(44), 1903, https://doi.org/10.21105/joss.01903 [return]   ","permalink":"https://rgoswami.me/posts/islr-ch6/","tags":["solutions","R","ISLR"],"title":"ISLR :: Linear Model Selection and Regularization"},{"categories":["programming"],"contents":" Chapter V - Resampling Methods All the questions are as per the ISL seventh printing1.\nCommon Instead of using the standard functions, we will leverage the mlr3 package2.\n#install.packages(\u0026#34;mlr3\u0026#34;,\u0026#34;data.table\u0026#34;,\u0026#34;mlr3viz\u0026#34;,\u0026#34;mlr3learners\u0026#34;) Actually for R version 3.6.2, the steps to get it working were a bit more involved.\ninstall.packages(\u0026#34;remotes\u0026#34;,\u0026#34;data.table\u0026#34;, \u0026#34;GGally\u0026#34;,\u0026#34;precerec\u0026#34;) # For plotslibrary(remotes) remotes::install_github(\u0026#34;mlr-org/mlr3\u0026#34;)## Skipping install of \u0026#39;mlr3\u0026#39; from a github remote, the SHA1 (fca21c10) has not changed since last install. ## Use `force = TRUE` to force installationremotes::install_github(\u0026#34;mlr-org/mlr3viz\u0026#34;)## Skipping install of \u0026#39;mlr3viz\u0026#39; from a github remote, the SHA1 (0b4ea273) has not changed since last install. ## Use `force = TRUE` to force installationremotes::install_github(\u0026#34;mlr-org/mlr3learners\u0026#34;)## Skipping install of \u0026#39;mlr3learners\u0026#39; from a github remote, the SHA1 (64b275a0) has not changed since last install. ## Use `force = TRUE` to force installation Load ISLR and other libraries.\nlibsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;, \u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;,\u0026#34;MASS\u0026#34;, \u0026#34;pROC\u0026#34;,\u0026#34;mlr3\u0026#34;,\u0026#34;data.table\u0026#34;, \u0026#34;mlr3viz\u0026#34;,\u0026#34;mlr3learners\u0026#34;) invisible(lapply(libsUsed, library, character.only = TRUE))## ## Attaching package: \u0026#39;dplyr\u0026#39;## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──## ✔ tibble 2.1.3 ✔ purrr 0.3.3 ## ✔ tidyr 1.0.0 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag()## Loading required package: lattice## ## Attaching package: \u0026#39;caret\u0026#39;## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## lift## ## Attaching package: \u0026#39;MASS\u0026#39;## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## select## Type \u0026#39;citation(\u0026#34;pROC\u0026#34;)\u0026#39; for a citation.## ## Attaching package: \u0026#39;pROC\u0026#39;## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## cov, smooth, var## ## Attaching package: \u0026#39;data.table\u0026#39;## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## transpose## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## between, first, last Question 5.5 - Page 198 In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n(a) Fit a logistic regression model that uses income and balance to predict default.\n(b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n Split the sample set into a training set and a validation set.\n Fit a multiple logistic regression model using only the training observations.\n Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than \\(0.5\\).\n Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.\n  \u0026copy; Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.\n(d) Now consider a logistic regression model that predicts the prob- ability of default using income , balance , and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.\nAnswer We will need our data.\ndefDat\u0026lt;-ISLR::Default  Very quick peek   defDat %\u0026gt;% summary## default student balance income ## No :9667 No :7056 Min. : 0.0 Min. : 772 ## Yes: 333 Yes:2944 1st Qu.: 481.7 1st Qu.:21340 ## Median : 823.6 Median :34553 ## Mean : 835.4 Mean :33517 ## 3rd Qu.:1166.3 3rd Qu.:43808 ## Max. :2654.3 Max. :73554defDat %\u0026gt;% str## \u0026#39;data.frame\u0026#39;: 10000 obs. of 4 variables: ## $ default: Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 1 1 1 1 1 1 1 1 1 1 ... ## $ student: Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 1 2 1 1 1 2 1 2 1 1 ... ## $ balance: num 730 817 1074 529 786 ... ## $ income : num 44362 12106 31767 35704 38463 ... a) Logistic Model with mlr3 Following the new approach which leverages R6 features leads us to define a classification task first. As far as I can tell, the data needs to be filtered to contain only the things we need to predict with, in this case we are required to use only income and balance so we will do so.\nset.seed(1984) redDat\u0026lt;-defDat %\u0026gt;% subset(select=c(income,balance,default)) tskLogiFull=TaskClassif$new(id=\u0026#34;credit\u0026#34;,backend=redDat,target=\u0026#34;default\u0026#34;) print(tskLogiFull)## \u0026lt;TaskClassif:credit\u0026gt; (10000 x 3) ## * Target: default ## * Properties: twoclass ## * Features (2): ## - dbl (2): balance, income This can be visualized neatly as well.\nautoplot(tskLogiFull) Figure 1: MLR3 Visualizations  We have a pretty imbalanced data-set.\nautoplot(tskLogiFull,type=\u0026#34;pairs\u0026#34;)## Registered S3 method overwritten by \u0026#39;GGally\u0026#39;: ## method from ## +.gg ggplot2## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 2: Paired mlr3 data  We can use any of the learners implemented, so it is a good idea to take a quick peek at them all.\nas.data.table(mlr_learners)## key feature_types ## 1: classif.debug logical,integer,numeric,character,factor,ordered ## 2: classif.featureless logical,integer,numeric,character,factor,ordered ## 3: classif.glmnet logical,integer,numeric ## 4: classif.kknn logical,integer,numeric,factor,ordered ## 5: classif.lda logical,integer,numeric,factor,ordered ## 6: classif.log_reg logical,integer,numeric,character,factor,ordered ## 7: classif.naive_bayes logical,integer,numeric,factor ## 8: classif.qda logical,integer,numeric,factor,ordered ## 9: classif.ranger logical,integer,numeric,character,factor,ordered ## 10: classif.rpart logical,integer,numeric,factor,ordered ## 11: classif.svm logical,integer,numeric ## 12: classif.xgboost logical,integer,numeric ## 13: regr.featureless logical,integer,numeric,character,factor,ordered ## 14: regr.glmnet logical,integer,numeric ## 15: regr.kknn logical,integer,numeric,factor,ordered ## 16: regr.km logical,integer,numeric ## 17: regr.lm logical,integer,numeric,factor ## 18: regr.ranger logical,integer,numeric,character,factor,ordered ## 19: regr.rpart logical,integer,numeric,factor,ordered ## 20: regr.svm logical,integer,numeric ## 21: regr.xgboost logical,integer,numeric ## key feature_types ## packages ## 1: ## 2: ## 3: glmnet ## 4: kknn ## 5: MASS ## 6: stats ## 7: e1071 ## 8: MASS ## 9: ranger ## 10: rpart ## 11: e1071 ## 12: xgboost ## 13: stats ## 14: glmnet ## 15: kknn ## 16: DiceKriging ## 17: stats ## 18: ranger ## 19: rpart ## 20: e1071 ## 21: xgboost ## packages ## properties ## 1: missings,multiclass,twoclass ## 2: importance,missings,multiclass,selected_features,twoclass ## 3: multiclass,twoclass,weights ## 4: multiclass,twoclass ## 5: multiclass,twoclass,weights ## 6: twoclass,weights ## 7: multiclass,twoclass ## 8: multiclass,twoclass,weights ## 9: importance,multiclass,oob_error,twoclass,weights ## 10: importance,missings,multiclass,selected_features,twoclass,weights ## 11: multiclass,twoclass ## 12: importance,missings,multiclass,twoclass,weights ## 13: importance,missings,selected_features ## 14: weights ## 15: ## 16: ## 17: weights ## 18: importance,oob_error,weights ## 19: importance,missings,selected_features,weights ## 20: ## 21: importance,missings,weights ## properties ## predict_types ## 1: response,prob ## 2: response,prob ## 3: response,prob ## 4: response,prob ## 5: response,prob ## 6: response,prob ## 7: response,prob ## 8: response,prob ## 9: response,prob ## 10: response,prob ## 11: response,prob ## 12: response,prob ## 13: response,se ## 14: response ## 15: response ## 16: response,se ## 17: response,se ## 18: response,se ## 19: response ## 20: response ## 21: response ## predict_types We can now pick the logistic one. Note that this essentially proxies our requests down to the stats package.\nlearner = mlr_learners$get(\u0026#34;classif.log_reg\u0026#34;) Now we can final solve the question, which is to simply use the model on all our data and return the accuracy metrics.\ntrainFullCred=learner$train(tskLogiFull) print(learner$predict(tskLogiFull)$confusion)## truth ## response No Yes ## No 9629 225 ## Yes 38 108measure = msr(\u0026#34;classif.acc\u0026#34;) print(learner$predict(tskLogiFull)$score(measure))## classif.acc ## 0.9737 Note that this style of working with objects does not really utilize the familiar %\u0026gt;% interface.\nThe caret package still has neater default metrics so we will use that as well.\nconfusionMatrix(learner$predict(tskLogiFull)$response,defDat$default)## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 9629 225 ## Yes 38 108 ## ## Accuracy : 0.9737 ## 95% CI : (0.9704, 0.9767) ## No Information Rate : 0.9667 ## P-Value [Acc \u0026gt; NIR] : 3.067e-05 ## ## Kappa : 0.4396 ## ## Mcnemar\u0026#39;s Test P-Value : \u0026lt; 2.2e-16 ## ## Sensitivity : 0.9961 ## Specificity : 0.3243 ## Pos Pred Value : 0.9772 ## Neg Pred Value : 0.7397 ## Prevalence : 0.9667 ## Detection Rate : 0.9629 ## Detection Prevalence : 0.9854 ## Balanced Accuracy : 0.6602 ## ## \u0026#39;Positive\u0026#39; Class : No ##autoplot(learner$predict(tskLogiFull)) Figure 3: Autoplot results  We can get some other plots as well, but we need our probabilities to be returned.\n# For ROC curves lrnprob = lrn(\u0026#34;classif.log_reg\u0026#34;,predict_type=\u0026#34;prob\u0026#34;) lrnprob$train(tskLogiFull) autoplot(lrnprob$predict(tskLogiFull),type=\u0026#34;roc\u0026#34;) Figure 4: ROC curve  b) Validation Sets with mlr3 Though the question seems to require a manual validation set generation and thresholding, we can simply use the defaults.\ntrain_set = sample(tskLogiFull$nrow, 0.8 * tskLogiFull$nrow) test_set = setdiff(seq_len(tskLogiFull$nrow), train_set) learner$train(tskLogiFull,row_ids=train_set) confusionMatrix(learner$predict(tskLogiFull, row_ids=test_set)$response,defDat[-train_set,]$default)## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 1921 47 ## Yes 9 23 ## ## Accuracy : 0.972 ## 95% CI : (0.9638, 0.9788) ## No Information Rate : 0.965 ## P-Value [Acc \u0026gt; NIR] : 0.04663 ## ## Kappa : 0.4387 ## ## Mcnemar\u0026#39;s Test P-Value : 7.641e-07 ## ## Sensitivity : 0.9953 ## Specificity : 0.3286 ## Pos Pred Value : 0.9761 ## Neg Pred Value : 0.7188 ## Prevalence : 0.9650 ## Detection Rate : 0.9605 ## Detection Prevalence : 0.9840 ## Balanced Accuracy : 0.6620 ## ## \u0026#39;Positive\u0026#39; Class : No ## For a reasonable comparison, we will demonstrate a standard approach as well. In this instance we will not use caret to ensure that our class distribution in the train and test sets are not sampled to remain the same.\ntrainNoCaret\u0026lt;-sample(nrow(defDat), size = floor(.8*nrow(defDat)), replace = F) glm.fit=glm(default~income+balance,data=defDat,family=binomial,subset=trainNoCaret) glm.probs\u0026lt;-predict(glm.fit,defDat[-trainNoCaret,],type=\u0026#34;response\u0026#34;) glm.preds\u0026lt;-ifelse(glm.probs \u0026lt; 0.5, \u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;) confusionMatrix(glm.preds %\u0026gt;% factor,defDat[-trainNoCaret,]$default)## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 1930 46 ## Yes 6 18 ## ## Accuracy : 0.974 ## 95% CI : (0.966, 0.9805) ## No Information Rate : 0.968 ## P-Value [Acc \u0026gt; NIR] : 0.06859 ## ## Kappa : 0.3986 ## ## Mcnemar\u0026#39;s Test P-Value : 6.362e-08 ## ## Sensitivity : 0.9969 ## Specificity : 0.2812 ## Pos Pred Value : 0.9767 ## Neg Pred Value : 0.7500 ## Prevalence : 0.9680 ## Detection Rate : 0.9650 ## Detection Prevalence : 0.9880 ## Balanced Accuracy : 0.6391 ## ## \u0026#39;Positive\u0026#39; Class : No ## Since the two approaches use different samples there is a little variation, but we can see that the accuracy is essentially the same.\nc) 3-fold cross validation As per the question, we can repeat the block above three times, or extract it into a function which takes a seed value and run that three times. Either way, here we will present the mlr3 approach to cross validation and resampling.\nrr = resample(tskLogiFull, lrnprob, rsmp(\u0026#34;cv\u0026#34;, folds = 3))## INFO [22:12:30.025] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 1/3) ## INFO [22:12:30.212] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 2/3) ## INFO [22:12:30.360] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 3/3)autoplot(rr,type=\u0026#34;roc\u0026#34;) Figure 5: Resampled ROC curve  We might want the average as well.\nrr$aggregate(msr(\u0026#34;classif.ce\u0026#34;)) %\u0026gt;% print## classif.ce ## 0.02630035 Adding Student as a dummy variable We will stick to the mlr3 approach because it is faster.\nredDat2\u0026lt;-defDat %\u0026gt;% mutate(student=as.numeric(defDat$student)) tskLogi2=TaskClassif$new(id=\u0026#34;credit\u0026#34;,backend=redDat2,target=\u0026#34;default\u0026#34;) print(tskLogi2)## \u0026lt;TaskClassif:credit\u0026gt; (10000 x 4) ## * Target: default ## * Properties: twoclass ## * Features (3): ## - dbl (3): balance, income, studentautoplot(tskLogi2,type=\u0026#34;pairs\u0026#34;)## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 6: Logistic regression pairs data  This gives us a visual indicator and premonition that we might not be getting incredible results with our new variable in the mix, but we should still work it through.\nconfusionMatrix(lrnprob$predict(tskLogi2)$response,defDat$default)## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 9629 225 ## Yes 38 108 ## ## Accuracy : 0.9737 ## 95% CI : (0.9704, 0.9767) ## No Information Rate : 0.9667 ## P-Value [Acc \u0026gt; NIR] : 3.067e-05 ## ## Kappa : 0.4396 ## ## Mcnemar\u0026#39;s Test P-Value : \u0026lt; 2.2e-16 ## ## Sensitivity : 0.9961 ## Specificity : 0.3243 ## Pos Pred Value : 0.9772 ## Neg Pred Value : 0.7397 ## Prevalence : 0.9667 ## Detection Rate : 0.9629 ## Detection Prevalence : 0.9854 ## Balanced Accuracy : 0.6602 ## ## \u0026#39;Positive\u0026#39; Class : No ##autoplot(lrnprob$predict(tskLogi2)) Figure 7: Autoplot figure  lrnprob$train(tskLogi2) autoplot(lrnprob$predict(tskLogi2),type=\u0026#34;roc\u0026#34;) Figure 8: ROC plot  Although we have slightly better accuracy with the new variable, it needs to be compared to determine if it is worth further investigation.\nWith a three-fold validation approach,\nlibrary(\u0026#34;gridExtra\u0026#34;)## ## Attaching package: \u0026#39;gridExtra\u0026#39;## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## combinerr2 = resample(tskLogi2, lrnprob, rsmp(\u0026#34;cv\u0026#34;, folds = 3))## INFO [22:12:39.670] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 1/3) ## INFO [22:12:39.731] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 2/3) ## INFO [22:12:39.780] Applying learner \u0026#39;classif.log_reg\u0026#39; on task \u0026#39;credit\u0026#39; (iter 3/3)wS\u0026lt;-autoplot(rr2) nS\u0026lt;-autoplot(rr) grid.arrange(wS,nS,ncol=2,bottom=\u0026#34;With student (left) and without (right)\u0026#34;) Figure 9: Plot of accuracy  Given the results, it is fair to say that adding the student data is useful in general.\nQuestion 5.6 - Page 199 We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.\n(a) Using the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\n(b) Write a function, boot.fn() , that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\n\u0026copy; Use the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance.\n(d) Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function.\nAnswer This question is slightly more specific to the packages in the book so we will use them.\na) Fit summary glm.fit %\u0026gt;% summary## ## Call: ## glm(formula = default ~ income + balance, family = binomial, ## data = defDat, subset = trainNoCaret) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1943 -0.1488 -0.0588 -0.0217 3.7058 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.150e+01 4.814e-01 -23.885 \u0026lt; 2e-16 *** ## income 2.288e-05 5.553e-06 4.121 3.78e-05 *** ## balance 5.593e-03 2.509e-04 22.295 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2354.0 on 7999 degrees of freedom ## Residual deviance: 1283.6 on 7997 degrees of freedom ## AIC: 1289.6 ## ## Number of Fisher Scoring iterations: 8 b) Function boot.fn=function(data,subs){return(coef(glm(default~income+balance,data=data, family=binomial,subset=subs)))}boot.fn(defDat,train_set) %\u0026gt;% print## (Intercept) income balance ## -1.136824e+01 1.846153e-05 5.576468e-03glm(default~income+balance,data=defDat,family=binomial,subset=train_set) %\u0026gt;% summary## ## Call: ## glm(formula = default ~ income + balance, family = binomial, ## data = defDat, subset = train_set) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4280 -0.1465 -0.0582 -0.0218 3.7115 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.137e+01 4.813e-01 -23.618 \u0026lt; 2e-16 *** ## income 1.846e-05 5.553e-06 3.324 0.000886 *** ## balance 5.576e-03 2.529e-04 22.046 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2313.6 on 7999 degrees of freedom ## Residual deviance: 1266.4 on 7997 degrees of freedom ## AIC: 1272.4 ## ## Number of Fisher Scoring iterations: 8 We see that the statistics obtained from both are the same.\nc) Bootstrap The old fashioned way. R is the resample rate, boot.fn is the statistic used.\nlibrary(boot)## ## Attaching package: \u0026#39;boot\u0026#39;## The following object is masked from \u0026#39;package:lattice\u0026#39;: ## ## melanomaboot(defDat,boot.fn,R=184) %\u0026gt;% print## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = defDat, statistic = boot.fn, R = 184) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* -1.154047e+01 -1.407368e-02 4.073453e-01 ## t2* 2.080898e-05 -6.386634e-08 4.720109e-06 ## t3* 5.647103e-03 1.350950e-05 2.111547e-04 d) Comparison  Clearly, there is not much difference in the standard error estimates  Var | Bootstrap | Summary |\n| :---------: | --------- |\nIntercept | 4.428026e-01 | 4.883e-01 |\nincome | 2.797011e-06 | 5.548e-06 |\nbalance | 2.423002e-04 | 2.591e-04 |\nQuestion 5.8 - Page 200 We will now perform cross-validation on a simulated data set. (a) Generate a simulated data set as follows:\n\u0026gt; set . seed (1) \u0026gt; y = rnorm (100) \u0026gt; x = rnorm (100) \u0026gt; y =x -2\\* x ^2+ rnorm (100) In this data set, what is n and what is p? Write out the model used to generate the data in equation form.\n(b) Create a scatterplot of \\(X\\) against \\(Y\\). Comment on what you find.\n\u0026copy; Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:\n \\(Y=\\beta_0+\\beta_1X+\\eta\\)\n \\(Y=\\beta_0+\\beta_1X+\\beta_2X^2+\\eta\\)\n \\(Y=\\beta_0+\\beta_1X+\\beta_2X^2+\\beta_{3}X^{3}+\\eta\\)\n \\(Y=\\beta_0+\\beta_1X+\\beta_2X^2+\\beta_{3}X^{3}+\\beta_{4}X^{4}+\\eta\\)\n  Note you may find it helpful to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\).\n(d) Repeat \u0026copy; using another random seed, and report your results. Are your results the same as what you got in \u0026copy;? Why?\n(e) Which of the models in \u0026copy; had the smallest LOOCV error? Is this what you expected? Explain your answer.\n(f) Comment on the statistical significance of the coefficient esti- mates that results from fitting each of the models in \u0026copy; using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?\nAnswer a) Modeling data set.seed(1) y \u0026lt;- rnorm(100) x \u0026lt;- rnorm(100) y \u0026lt;- x - 2*x^2 + rnorm(100) Clearly:\n Our equation is \\(y=x-2x^{2}+\\epsilon\\) where \\(epsilon\\) is normally distributed from 100 samples We have \\(n=100\\) observations \\(p=2\\) where \\(p\\) is the number of features  b) Visual inspection qplot(x,y) Figure 10: Model data plot  We observe that the data is quadratic, as we also know from the generating function, which was a quadratic equation plus normally distributed noise.\nc) Least squares fits Not very important, but here we use the caret form.\npow=function(x,y){return(x^y)} dfDat \u0026lt;- data.frame(y,x,x2=pow(x,2),x3=pow(x,3),x4=pow(x,4)) We might have also just used poly(x,n) to skip making the data frame.\nWe will set our resampling method as follows:\nfitControl\u0026lt;-trainControl(method=\u0026#34;LOOCV\u0026#34;)train(y~x,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 1 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 2.427134 0.05389864 1.878566 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUEtrain(y~x+x2,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 2 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.042399 0.8032414 0.8029942 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUEtrain(y~x+x2+x3,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 3 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.050041 0.8003517 0.8073024 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUEtrain(y~x+x2+x3+x4,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 4 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.055828 0.7982111 0.8150296 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE d) Seeding effects set.seed(1995)train(y~x,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 1 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 2.427134 0.05389864 1.878566 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUEtrain(y~x+x2,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 2 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.042399 0.8032414 0.8029942 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUEtrain(y~x+x2+x3,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 3 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.050041 0.8003517 0.8073024 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUEtrain(y~x+x2+x3+x4,data=dfDat,trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 4 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.055828 0.7982111 0.8150296 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE We note that there is no change on varying the seed because LOOCV is exhaustive and uses n folds for each observation.\ne) Analysis train(y~x,data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 1 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 2.427134 0.05389864 1.878566 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUEtrain(y~poly(x,2),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 1 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.042399 0.8032414 0.8029942 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUEtrain(y~poly(x,3),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 1 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.050041 0.8003517 0.8073024 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUEtrain(y~poly(x,4),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% print## Linear Regression ## ## 100 samples ## 1 predictor ## ## No pre-processing ## Resampling: Leave-One-Out Cross-Validation ## Summary of sample sizes: 99, 99, 99, 99, 99, 99, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 1.055828 0.7982111 0.8150296 ## ## Tuning parameter \u0026#39;intercept\u0026#39; was held constant at a value of TRUE Clearly the quadratic polynomial has the lowest error, which makes sense given how the data was generated.\nf) Statistical significance train(y~x,data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.3469 -0.9275 0.8028 1.5608 4.3974 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.8185 0.2364 -7.692 1.14e-11 *** ## x 0.2430 0.2479 0.981 0.329 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.362 on 98 degrees of freedom ## Multiple R-squared: 0.009717, Adjusted R-squared: -0.0003881 ## F-statistic: 0.9616 on 1 and 98 DF, p-value: 0.3292train(y~poly(x,2),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.89884 -0.53765 0.04135 0.61490 2.73607 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.8277 0.1032 -17.704 \u0026lt;2e-16 *** ## `poly(x, 2)1` 2.3164 1.0324 2.244 0.0271 * ## `poly(x, 2)2` -21.0586 1.0324 -20.399 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.032 on 97 degrees of freedom ## Multiple R-squared: 0.8128, Adjusted R-squared: 0.8089 ## F-statistic: 210.6 on 2 and 97 DF, p-value: \u0026lt; 2.2e-16train(y~poly(x,3),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.87250 -0.53881 0.02862 0.59383 2.74350 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.8277 0.1037 -17.621 \u0026lt;2e-16 *** ## `poly(x, 3)1` 2.3164 1.0372 2.233 0.0279 * ## `poly(x, 3)2` -21.0586 1.0372 -20.302 \u0026lt;2e-16 *** ## `poly(x, 3)3` -0.3048 1.0372 -0.294 0.7695 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.037 on 96 degrees of freedom ## Multiple R-squared: 0.813, Adjusted R-squared: 0.8071 ## F-statistic: 139.1 on 3 and 96 DF, p-value: \u0026lt; 2.2e-16train(y~poly(x,4),data=dfDat %\u0026gt;% subset(select=c(y,x)),trControl=fitControl,method=\u0026#34;lm\u0026#34;) %\u0026gt;% summary %\u0026gt;% print## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8914 -0.5244 0.0749 0.5932 2.7796 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.8277 0.1041 -17.549 \u0026lt;2e-16 *** ## `poly(x, 4)1` 2.3164 1.0415 2.224 0.0285 * ## `poly(x, 4)2` -21.0586 1.0415 -20.220 \u0026lt;2e-16 *** ## `poly(x, 4)3` -0.3048 1.0415 -0.293 0.7704 ## `poly(x, 4)4` -0.4926 1.0415 -0.473 0.6373 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.041 on 95 degrees of freedom ## Multiple R-squared: 0.8134, Adjusted R-squared: 0.8055 ## F-statistic: 103.5 on 4 and 95 DF, p-value: \u0026lt; 2.2e-16  Clearly, the second order terms are the most significant, as expected  Question 5.9 - Page 201 We will now consider the Boston housing data set, from the MASS library.\n(a) Based on this data set, provide an estimate for the population mean of medv. Call this estimate \\(\\hat{\\mu}\\).\n(b) Provide an estimate of the standard error of \\(\\hat{\\mu}\\). Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\n\u0026copy; Now estimate the standard error of \\(\\hat{\\mu}\\) using the bootstrap. How does this compare to your answer from (b)?\n(d) Based on your bootstrap estimate from \u0026copy;, provide a 95 % confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston\\$medv). Hint: You can approximate a 95 % confidence interval using the formula \\([\\hat{\\mu} − 2SE(\\hat{\\mu}), \\hat{\\mu} + 2SE(\\hat{\\mu})]\\).\n(e) Based on this data set, provide an estimate, \\(\\hat{\\mu_{med}}\\), for the median value of medv in the population.\n(f) We now would like to estimate the standard error of \\(\\hat{\\mu}\\) med. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.\n(g) Based on this data set, provide an estimate for the tenth percentile of medv in Boston suburbs. Call this quantity \\(\\hat{\\mu_{0.1}}\\). (You can use the quantile() function.)\n(h) Use the bootstrap to estimate the standard error of \\(\\hat{\\mu_{0.1}}\\). Comment on your findings.\nAnswer boston\u0026lt;-MASS::Boston  Reminder   boston %\u0026gt;% summary %\u0026gt;% print## crim zn indus chas ## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 ## 1st Qu.: 0.08204 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 ## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 ## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 ## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 ## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 ## nox rm age dis ## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 ## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 ## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 ## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 ## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 ## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 ## rad tax ptratio black ## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 ## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 ## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 ## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 ## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 ## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 ## lstat medv ## Min. : 1.73 Min. : 5.00 ## 1st Qu.: 6.95 1st Qu.:17.02 ## Median :11.36 Median :21.20 ## Mean :12.65 Mean :22.53 ## 3rd Qu.:16.95 3rd Qu.:25.00 ## Max. :37.97 Max. :50.00boston %\u0026gt;% str %\u0026gt;% print## \u0026#39;data.frame\u0026#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... ## NULL a) Mean muhat=boston$medv %\u0026gt;% mean() print(muhat)## [1] 22.53281 b) Standard error Recall that \\(SE=\\frac{SD}{\\sqrt{N_{obs}}}\\)\nboston$medv %\u0026gt;% sd/(nrow(boston)^0.5) %\u0026gt;% print## [1] 22.49444## [1] 0.4088611 c) Bootstrap estimate library(boot) myMean\u0026lt;-function(frame,ind){return(mean(frame[ind]))}boot(boston$medv,myMean,R=184) %\u0026gt;% print## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = boston$medv, statistic = myMean, R = 184) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 22.53281 0.03451839 0.409621 We see that the bootstrapped error over 184 samples is 0.4341499 while without it we had 0.4088611 which is similar enough.\nd) Confidence intervals with bootstrap and t.test boston$medv %\u0026gt;% t.test %\u0026gt;% print## ## One Sample t-test ## ## data: . ## t = 55.111, df = 505, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 21.72953 23.33608 ## sample estimates: ## mean of x ## 22.53281 We can approximate this with what we already have\nbRes=boot(boston$medv,myMean,R=184) seBoot\u0026lt;-bRes$t %\u0026gt;% var %\u0026gt;% sqrt xlow=muhat-2*(seBoot) xhigh=muhat+2*(seBoot) c(xlow,xhigh) %\u0026gt;% print## [1] 21.72675 23.33887 Our intervals are also pretty close to each other.\ne) Median boston$medv %\u0026gt;% sort %\u0026gt;% median %\u0026gt;% print## [1] 21.2 f) Median standard error We can reuse the logic of the myMean function defined previously.\nmyMedian=function(data,ind){return(median(data[ind]))}boston$medv %\u0026gt;% boot(myMedian,R=1500) %\u0026gt;% print## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = ., statistic = myMedian, R = 1500) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 21.2 -0.03773333 0.387315 We see that the standard error is 0.3767072.\ng) Tenth percentile mu0one\u0026lt;-boston$medv %\u0026gt;% quantile(c(0.1)) print(mu0one)## 10% ## 12.75 h) Bootstrap Once again.\nmyQuant=function(data,ind){return(quantile(data[ind],0.1))}boston$medv %\u0026gt;% boot(myQuant,R=500) %\u0026gt;% print## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = ., statistic = myQuant, R = 500) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 12.75 -0.0095 0.4951415 The standard error is 0.5024526\n James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media. [return] Lang et al., (2019). mlr3: A modern object-oriented machine learning framework in R. Journal of Open Source Software, 4(44), 1903, https://doi.org/10.21105/joss.01903 [return]   ","permalink":"https://rgoswami.me/posts/islr-ch5/","tags":["solutions","R","ISLR"],"title":"ISLR :: Resampling Methods"},{"categories":["programming"],"contents":" Chapter IV - Classification All the questions are as per the ISL seventh printing\u0026nbsp;1.\nCommon Stuff Here I\u0026rsquo;ll load things I will be using throughout, mostly libraries.\nlibsUsed\u0026lt;-c(\u0026#34;dplyr\u0026#34;,\u0026#34;ggplot2\u0026#34;,\u0026#34;tidyverse\u0026#34;,\u0026#34;ISLR\u0026#34;,\u0026#34;caret\u0026#34;) invisible(lapply(libsUsed, library, character.only = TRUE))# # Attaching package: \u0026#39;dplyr\u0026#39;# The following objects are masked from \u0026#39;package:stats\u0026#39;: # # filter, lag# The following objects are masked from \u0026#39;package:base\u0026#39;: # # intersect, setdiff, setequal, union# ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──# ✔ tibble 2.1.3 ✔ purrr 0.3.3 # ✔ tidyr 1.0.0 ✔ stringr 1.4.0 # ✔ readr 1.3.1 ✔ forcats 0.4.0# ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── # ✖ dplyr::filter() masks stats::filter() # ✖ dplyr::lag() masks stats::lag()# Loading required package: lattice# # Attaching package: \u0026#39;caret\u0026#39;# The following object is masked from \u0026#39;package:purrr\u0026#39;: # # lift Question 4.10 - Page 171 This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapter\u0026rsquo;s lab, except that it contains 1, 089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\n(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\n(b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\n\u0026copy; Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\n(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\n(e) Repeat (d) using LDA.\n(f) Repeat (d) using QDA.\n(g) Repeat (d) using KNN with \\(K = 1\\).\n(h) Which of these methods appears to provide the best results on this data?\n(i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.\nAnswer We will need the data in a variable for ease of use.\nweeklyDat\u0026lt;-ISLR::Weekly a) Summary Statistics Text Most of this segment relies heavily on usage of dplyr and especially the %\u0026gt;% or pipe operator for readability. The use of the skimr package2 might added more descriptive statistics, but is not covered here.\nBasic Summaries weeklyDat %\u0026gt;% str# \u0026#39;data.frame\u0026#39;: 1089 obs. of 9 variables: # $ Year : num 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ... # $ Lag1 : num 0.816 -0.27 -2.576 3.514 0.712 ... # $ Lag2 : num 1.572 0.816 -0.27 -2.576 3.514 ... # $ Lag3 : num -3.936 1.572 0.816 -0.27 -2.576 ... # $ Lag4 : num -0.229 -3.936 1.572 0.816 -0.27 ... # $ Lag5 : num -3.484 -0.229 -3.936 1.572 0.816 ... # $ Volume : num 0.155 0.149 0.16 0.162 0.154 ... # $ Today : num -0.27 -2.576 3.514 0.712 1.178 ... # $ Direction: Factor w/ 2 levels \u0026#34;Down\u0026#34;,\u0026#34;Up\u0026#34;: 1 1 2 2 2 1 2 2 2 1 ... We see that there is only one Factor, which makes sense.\nweeklyDat %\u0026gt;% summary# Year Lag1 Lag2 Lag3 # Min. :1990 Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 # 1st Qu.:1995 1st Qu.: -1.1540 1st Qu.: -1.1540 1st Qu.: -1.1580 # Median :2000 Median : 0.2410 Median : 0.2410 Median : 0.2410 # Mean :2000 Mean : 0.1506 Mean : 0.1511 Mean : 0.1472 # 3rd Qu.:2005 3rd Qu.: 1.4050 3rd Qu.: 1.4090 3rd Qu.: 1.4090 # Max. :2010 Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 # Lag4 Lag5 Volume Today # Min. :-18.1950 Min. :-18.1950 Min. :0.08747 Min. :-18.1950 # 1st Qu.: -1.1580 1st Qu.: -1.1660 1st Qu.:0.33202 1st Qu.: -1.1540 # Median : 0.2380 Median : 0.2340 Median :1.00268 Median : 0.2410 # Mean : 0.1458 Mean : 0.1399 Mean :1.57462 Mean : 0.1499 # 3rd Qu.: 1.4090 3rd Qu.: 1.4050 3rd Qu.:2.05373 3rd Qu.: 1.4050 # Max. : 12.0260 Max. : 12.0260 Max. :9.32821 Max. : 12.0260 # Direction # Down:484 # Up :605 # # # # Unique Values We might also want to know how many unique values are there in each column.\nweeklyDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length)# Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today # 21 1004 1005 1005 1005 1005 1089 1003 # Direction # 2 We note that year has disproportionately lower values, something to keep in mind while constructing models later.\nRange The range of each variable might be useful as well, but we have to ignore the factor.\nweeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% sapply(range)# Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today # [1,] 1990 -18.195 -18.195 -18.195 -18.195 -18.195 0.087465 -18.195 # [2,] 2010 12.026 12.026 12.026 12.026 12.026 9.328214 12.026 The most interesting thing about this is probably that the Lag variables all have the same range, also something to be kept in mind while applying transformations to the variable (if at all).\nMean and Std. Dev By now we might have a pretty good idea of how this will look, but it is still worth seeing.\nweeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% sapply(mean)# Year Lag1 Lag2 Lag3 Lag4 Lag5 # 2000.0486685 0.1505849 0.1510790 0.1472048 0.1458182 0.1398926 # Volume Today # 1.5746176 0.1498990 As expected, the Lag values have almost the same mean, what is a bit interesting though, is that the Today variable has roughly the same mean as the Lag variables.\nweeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% sapply(sd)# Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today # 6.033182 2.357013 2.357254 2.360502 2.360279 2.361285 1.686636 2.356927 This is largely redundant in terms of new information.\nCorrelations weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% cor# Year Lag1 Lag2 Lag3 Lag4 # Year 1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923 # Lag1 -0.03228927 1.000000000 -0.07485305 0.05863568 -0.071273876 # Lag2 -0.03339001 -0.074853051 1.00000000 -0.07572091 0.058381535 # Lag3 -0.03000649 0.058635682 -0.07572091 1.00000000 -0.075395865 # Lag4 -0.03112792 -0.071273876 0.05838153 -0.07539587 1.000000000 # Lag5 -0.03051910 -0.008183096 -0.07249948 0.06065717 -0.075675027 # Volume 0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617 # Today -0.03245989 -0.075031842 0.05916672 -0.07124364 -0.007825873 # Lag5 Volume Today # Year -0.030519101 0.84194162 -0.032459894 # Lag1 -0.008183096 -0.06495131 -0.075031842 # Lag2 -0.072499482 -0.08551314 0.059166717 # Lag3 0.060657175 -0.06928771 -0.071243639 # Lag4 -0.075675027 -0.06107462 -0.007825873 # Lag5 1.000000000 -0.05851741 0.011012698 # Volume -0.058517414 1.00000000 -0.033077783 # Today 0.011012698 -0.03307778 1.000000000 Useful though this is, it is kind of difficult to work with, in this form, so we might as well programmatic-ally remove strongly correlated data instead.\n# Uses caret corrCols=weeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% cor %\u0026gt;% findCorrelation(cutoff=0.8) reducedDat\u0026lt;-weeklyDat[-c(corrCols)] reducedDat %\u0026gt;% summary# Year Lag1 Lag2 Lag3 # Min. :1990 Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 # 1st Qu.:1995 1st Qu.: -1.1540 1st Qu.: -1.1540 1st Qu.: -1.1580 # Median :2000 Median : 0.2410 Median : 0.2410 Median : 0.2410 # Mean :2000 Mean : 0.1506 Mean : 0.1511 Mean : 0.1472 # 3rd Qu.:2005 3rd Qu.: 1.4050 3rd Qu.: 1.4090 3rd Qu.: 1.4090 # Max. :2010 Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 # Lag4 Lag5 Today Direction # Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 Down:484 # 1st Qu.: -1.1580 1st Qu.: -1.1660 1st Qu.: -1.1540 Up :605 # Median : 0.2380 Median : 0.2340 Median : 0.2410 # Mean : 0.1458 Mean : 0.1399 Mean : 0.1499 # 3rd Qu.: 1.4090 3rd Qu.: 1.4050 3rd Qu.: 1.4050 # Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 We can see that the Volume variable has been dropped, since it evidently is strongly correlated with Year. This may or may not be a useful insight, but it is good to keep in mind.\nVisualization We will be using the ggplot2 library throughout for this segment.\nLets start with some scatter plots in a one v/s all scheme, similar to the methodology described here.\nweeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% gather(-Year,key=\u0026#34;Variable\u0026#34;, value=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Value,y=Year)) + geom_point() + facet_wrap(~Variable) + coord_flip() Figure 1: One v/s all for Direction  That didn\u0026rsquo;t really tell us much which we didn\u0026rsquo;t already get from the cor() function, but we can go the whole hog and do this for every variable since we don\u0026rsquo;t have that many in the first place..\nweeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% pairs Figure 2: Pairs  This is not especially useful, and it is doubtful if more scatter-plots will help at all, so lets move on to box plots.\nweeklyDat %\u0026gt;% pivot_longer(-c(Direction,Volume,Today,Year),names_to=\u0026#34;Lag\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Direction,y=Value,fill=Lag)) + geom_boxplot() Figure 3: Box plots for Direction  weeklyDat %\u0026gt;% pivot_longer(-c(Direction,Volume,Today,Year),names_to=\u0026#34;Lag\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Today,y=Value,fill=Lag)) + geom_boxplot() Figure 4: More box plots  weeklyDat %\u0026gt;% pivot_longer(-c(Direction,Volume,Today,Year),names_to=\u0026#34;Lag\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=Lag,y=Value,fill=Direction)) + geom_boxplot() Figure 5: Lag v/s all  This does summarize our text analysis quite well. Importantly, it tells us that the Today value is largely unrelated to the \\(4\\) Lag variables.\nA really good-looking box-plot is easy to get with the caret library:\nweeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% featurePlot( y = weeklyDat$Direction, plot = \u0026#34;box\u0026#34;, # Pass in options to bwplot() scales = list(y = list(relation=\u0026#34;free\u0026#34;), x = list(rot = 90)), auto.key = list(columns = 2)) Figure 6: Plots with `caret`  We might want to visualize our correlation matrix as well.\nlibrary(reshape2)# # Attaching package: \u0026#39;reshape2\u0026#39;# The following object is masked from \u0026#39;package:tidyr\u0026#39;: # # smithsweeklyDat %\u0026gt;% subset(select=-c(Direction)) %\u0026gt;% cor %\u0026gt;% melt %\u0026gt;% ggplot(aes(x=Var1,y=Var2,fill=value)) + geom_tile() Figure 7: Heatmap of the correlation matrix  b) Logistic Regression - Predictor Significance Lets start with the native glm function.\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=weeklyDat, family=binomial) summary(glm.fit)# # Call: # glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + # Volume, family = binomial, data = weeklyDat) # # Deviance Residuals: # Min 1Q Median 3Q Max # -1.6949 -1.2565 0.9913 1.0849 1.4579 # # Coefficients: # Estimate Std. Error z value Pr(\u0026gt;|z|) # (Intercept) 0.26686 0.08593 3.106 0.0019 ** # Lag1 -0.04127 0.02641 -1.563 0.1181 # Lag2 0.05844 0.02686 2.175 0.0296 * # Lag3 -0.01606 0.02666 -0.602 0.5469 # Lag4 -0.02779 0.02646 -1.050 0.2937 # Lag5 -0.01447 0.02638 -0.549 0.5833 # Volume -0.02274 0.03690 -0.616 0.5377 # --- # Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 # # (Dispersion parameter for binomial family taken to be 1) # # Null deviance: 1496.2 on 1088 degrees of freedom # Residual deviance: 1486.4 on 1082 degrees of freedom # AIC: 1500.4 # # Number of Fisher Scoring iterations: 4 Evidently, only the Lag2 value is of statistical significance.\nIt is always of importance to figure out what numeric values R will assign to our factors, and it is best not to guess.\ncontrasts(weeklyDat$Direction)# Up # Down 0 # Up 1 c) Confusion Matrix and Metrics Essentially:\n Predict the response Create an output length vector Apply thresholding to obtain labels   glm.probs = predict(glm.fit, type = \u0026#34;response\u0026#34;) glm.pred = rep(\u0026#34;Up\u0026#34;,length(glm.probs)) glm.pred[glm.probs\u0026lt;0.5]=\u0026#34;Down\u0026#34; glm.pred=factor(glm.pred) confusionMatrix(glm.pred,weeklyDat$Direction)# Confusion Matrix and Statistics # # Reference # Prediction Down Up # Down 54 48 # Up 430 557 # # Accuracy : 0.5611 # 95% CI : (0.531, 0.5908) # No Information Rate : 0.5556 # P-Value [Acc \u0026gt; NIR] : 0.369 # # Kappa : 0.035 # # Mcnemar\u0026#39;s Test P-Value : \u0026lt;2e-16 # # Sensitivity : 0.11157 # Specificity : 0.92066 # Pos Pred Value : 0.52941 # Neg Pred Value : 0.56434 # Prevalence : 0.44444 # Detection Rate : 0.04959 # Detection Prevalence : 0.09366 # Balanced Accuracy : 0.51612 # # \u0026#39;Positive\u0026#39; Class : Down #  We have used the confusionMatrix function from caret (documented here) instead of displaying the results with table and then calculating precision, recall and the rest by hand.  d) Train Test Splits Although we could have used the indices and passed it to glm as the subset attribute, it is cleaner to just make subsets instead.\nweeklyVal\u0026lt;-weeklyDat %\u0026gt;% filter(Year\u0026gt;=2009) weeklyTrain\u0026lt;-weeklyDat %\u0026gt;% filter(Year\u0026lt;2009) Now we can train a model on our training data.\nglm.fit=glm(Direction~Lag2,data=weeklyTrain,family=binomial) summary(glm.fit)# # Call: # glm(formula = Direction ~ Lag2, family = binomial, data = weeklyTrain) # # Deviance Residuals: # Min 1Q Median 3Q Max # -1.536 -1.264 1.021 1.091 1.368 # # Coefficients: # Estimate Std. Error z value Pr(\u0026gt;|z|) # (Intercept) 0.20326 0.06428 3.162 0.00157 ** # Lag2 0.05810 0.02870 2.024 0.04298 * # --- # Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 # # (Dispersion parameter for binomial family taken to be 1) # # Null deviance: 1354.7 on 984 degrees of freedom # Residual deviance: 1350.5 on 983 degrees of freedom # AIC: 1354.5 # # Number of Fisher Scoring iterations: 4 Having fit our model, we will test the predictions on our held out data.\nglm.probs = predict(glm.fit,weeklyVal, type = \u0026#34;response\u0026#34;) glm.pred = rep(\u0026#34;Up\u0026#34;,length(glm.probs)) glm.pred[glm.probs\u0026lt;0.5]=\u0026#34;Down\u0026#34; glm.pred=factor(glm.pred) confusionMatrix(glm.pred,weeklyVal$Direction)# Confusion Matrix and Statistics # # Reference # Prediction Down Up # Down 9 5 # Up 34 56 # # Accuracy : 0.625 # 95% CI : (0.5247, 0.718) # No Information Rate : 0.5865 # P-Value [Acc \u0026gt; NIR] : 0.2439 # # Kappa : 0.1414 # # Mcnemar\u0026#39;s Test P-Value : 7.34e-06 # # Sensitivity : 0.20930 # Specificity : 0.91803 # Pos Pred Value : 0.64286 # Neg Pred Value : 0.62222 # Prevalence : 0.41346 # Detection Rate : 0.08654 # Detection Prevalence : 0.13462 # Balanced Accuracy : 0.56367 # # \u0026#39;Positive\u0026#39; Class : Down # We really aren\u0026rsquo;t doing very well with this single variable model as is evident.\ne) LDA models At this stage we could use MASS to get the lda function, but it would be better to just switch to using caret. Note that the caret prediction is a label by default, so thresholding needs to be specified differently if required.\nlda.fit=train(Direction~Lag2,data=weeklyTrain,method=\u0026#34;lda\u0026#34;) summary(lda.fit)# Length Class Mode # prior 2 -none- numeric # counts 2 -none- numeric # means 2 -none- numeric # scaling 1 -none- numeric # lev 2 -none- character # svd 1 -none- numeric # N 1 -none- numeric # call 3 -none- call # xNames 1 -none- character # problemType 1 -none- character # tuneValue 1 data.frame list # obsLevels 2 -none- character # param 0 -none- listpredict(lda.fit,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction)# Confusion Matrix and Statistics # # Reference # Prediction Down Up # Down 9 5 # Up 34 56 # # Accuracy : 0.625 # 95% CI : (0.5247, 0.718) # No Information Rate : 0.5865 # P-Value [Acc \u0026gt; NIR] : 0.2439 # # Kappa : 0.1414 # # Mcnemar\u0026#39;s Test P-Value : 7.34e-06 # # Sensitivity : 0.20930 # Specificity : 0.91803 # Pos Pred Value : 0.64286 # Neg Pred Value : 0.62222 # Prevalence : 0.41346 # Detection Rate : 0.08654 # Detection Prevalence : 0.13462 # Balanced Accuracy : 0.56367 # # \u0026#39;Positive\u0026#39; Class : Down # f) QDA models qda.fit=train(Direction~Lag2,data=weeklyTrain,method=\u0026#34;qda\u0026#34;) summary(qda.fit)# Length Class Mode # prior 2 -none- numeric # counts 2 -none- numeric # means 2 -none- numeric # scaling 2 -none- numeric # ldet 2 -none- numeric # lev 2 -none- character # N 1 -none- numeric # call 3 -none- call # xNames 1 -none- character # problemType 1 -none- character # tuneValue 1 data.frame list # obsLevels 2 -none- character # param 0 -none- listpredict(qda.fit,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction)# Confusion Matrix and Statistics # # Reference # Prediction Down Up # Down 0 0 # Up 43 61 # # Accuracy : 0.5865 # 95% CI : (0.4858, 0.6823) # No Information Rate : 0.5865 # P-Value [Acc \u0026gt; NIR] : 0.5419 # # Kappa : 0 # # Mcnemar\u0026#39;s Test P-Value : 1.504e-10 # # Sensitivity : 0.0000 # Specificity : 1.0000 # Pos Pred Value : NaN # Neg Pred Value : 0.5865 # Prevalence : 0.4135 # Detection Rate : 0.0000 # Detection Prevalence : 0.0000 # Balanced Accuracy : 0.5000 # # \u0026#39;Positive\u0026#39; Class : Down # This is quite possibly the worst of the lot. As is evident, the model just predicts Up no matter what.\ng) KNN caret tends to over-zealously retrain models and find the best possible parameters. In this case that is annoying and redundant so we will use the class library. We should really scale our data before using KNN though.\nlibrary(class) set.seed(1) knn.pred=knn(as.matrix(weeklyTrain$Lag2),as.matrix(weeklyVal$Lag2),weeklyTrain$Direction,k=1) confusionMatrix(knn.pred,weeklyVal$Direction)# Confusion Matrix and Statistics # # Reference # Prediction Down Up # Down 21 30 # Up 22 31 # # Accuracy : 0.5 # 95% CI : (0.4003, 0.5997) # No Information Rate : 0.5865 # P-Value [Acc \u0026gt; NIR] : 0.9700 # # Kappa : -0.0033 # # Mcnemar\u0026#39;s Test P-Value : 0.3317 # # Sensitivity : 0.4884 # Specificity : 0.5082 # Pos Pred Value : 0.4118 # Neg Pred Value : 0.5849 # Prevalence : 0.4135 # Detection Rate : 0.2019 # Detection Prevalence : 0.4904 # Balanced Accuracy : 0.4983 # # \u0026#39;Positive\u0026#39; Class : Down # Clearly this model is not doing very well.\nh) Model Selection We will first get the ROC curves.\nlibrary(pROC)# Type \u0026#39;citation(\u0026#34;pROC\u0026#34;)\u0026#39; for a citation.# # Attaching package: \u0026#39;pROC\u0026#39;# The following objects are masked from \u0026#39;package:stats\u0026#39;: # # cov, smooth, varknnROC\u0026lt;-roc(predictor=as.numeric(knn.pred),response=weeklyVal$Direction,levels=rev(levels(weeklyVal$Direction)))# Setting direction: controls \u0026lt; caseslogiROC\u0026lt;-roc(predictor=as.numeric(predict(glm.fit,weeklyVal)),response=weeklyVal$Direction)# Setting levels: control = Down, case = Up# Setting direction: controls \u0026gt; casesldaROC\u0026lt;-roc(predictor=as.numeric(predict(lda.fit,weeklyVal)),response=weeklyVal$Direction)# Setting levels: control = Down, case = Up# Setting direction: controls \u0026lt; casesqdaROC\u0026lt;-roc(predictor=as.numeric(predict(qda.fit,weeklyVal)),response=weeklyVal$Direction)# Setting levels: control = Down, case = Up # Setting direction: controls \u0026lt; cases Now to plot them.\nggroc(list(KNN=knnROC,Logistic=logiROC,LDA=ldaROC,QDA=qdaROC)) Figure 8: ROC curves for Weekly data  To compare models with caret it is easy to refit the logistic and knn models in the caret formulation.\nknnCaret=train(Direction~Lag2,data=weeklyTrain,method=\u0026#34;knn\u0026#34;) However, the KNN model is the best parameter model.\nresmod \u0026lt;- resamples(list(lda=lda.fit, qda=qda.fit, KNN=knnCaret)) summary(resmod)# # Call: # summary.resamples(object = resmod) # # Models: lda, qda, KNN # Number of resamples: 25 # # Accuracy # Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s # lda 0.5043228 0.5344353 0.5529101 0.5500861 0.5683060 0.5846995 0 # qda 0.5044248 0.5204360 0.5307263 0.5326785 0.5462428 0.5777778 0 # KNN 0.4472222 0.5082873 0.5240642 0.5168327 0.5302198 0.5485714 0 # # Kappa # Min. 1st Qu. Median Mean 3rd Qu. Max. # lda -0.02618939 -0.003638168 0.005796908 0.007801904 0.01635328 0.05431238 # qda -0.06383592 -0.005606123 0.000000000 -0.003229697 0.00000000 0.03606344 # KNN -0.11297539 0.004168597 0.024774647 0.016171229 0.04456142 0.07724439 # NA\u0026#39;s # lda 0 # qda 0 # KNN 0bwplot(resmod) Figure 9: Caret plots for comparison  dotplot(resmod)  Kappa or Cohen\u0026rsquo;s Kappa is essentially classification accuracy, normalized at the baseline of random chance. It is a more useful measure to use on problems that have imbalanced classes. There\u0026rsquo;s more on model selection here.\ni) Further Tuning Do note the caret defaults.\nfitControl \u0026lt;- trainControl(# 10-fold CV method = \u0026#34;repeatedcv\u0026#34;, number = 10, # repeated ten times repeats = 10) Logistic glm2.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=weeklyDat, family=binomial) glm2.probs = predict(glm2.fit,weeklyVal, type = \u0026#34;response\u0026#34;) glm2.pred = rep(\u0026#34;Up\u0026#34;,length(glm2.probs)) glm2.pred[glm2.probs\u0026lt;0.5]=\u0026#34;Down\u0026#34; glm2.pred=factor(glm2.pred) confusionMatrix(glm2.pred,weeklyVal$Direction)# Confusion Matrix and Statistics # # Reference # Prediction Down Up # Down 17 13 # Up 26 48 # # Accuracy : 0.625 # 95% CI : (0.5247, 0.718) # No Information Rate : 0.5865 # P-Value [Acc \u0026gt; NIR] : 0.24395 # # Kappa : 0.1907 # # Mcnemar\u0026#39;s Test P-Value : 0.05466 # # Sensitivity : 0.3953 # Specificity : 0.7869 # Pos Pred Value : 0.5667 # Neg Pred Value : 0.6486 # Prevalence : 0.4135 # Detection Rate : 0.1635 # Detection Prevalence : 0.2885 # Balanced Accuracy : 0.5911 # # \u0026#39;Positive\u0026#39; Class : Down # QDA qdaCaret=train(Direction~Lag2+Lag4,data=weeklyTrain,method=\u0026#34;qda\u0026#34;,trainControl=fitControl)summary(qdaCaret)# Length Class Mode # prior 2 -none- numeric # counts 2 -none- numeric # means 4 -none- numeric # scaling 8 -none- numeric # ldet 2 -none- numeric # lev 2 -none- character # N 1 -none- numeric # call 4 -none- call # xNames 2 -none- character # problemType 1 -none- character # tuneValue 1 data.frame list # obsLevels 2 -none- character # param 1 -none- listpredict(qdaCaret,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction)# Confusion Matrix and Statistics # # Reference # Prediction Down Up # Down 9 14 # Up 34 47 # # Accuracy : 0.5385 # 95% CI : (0.438, 0.6367) # No Information Rate : 0.5865 # P-Value [Acc \u0026gt; NIR] : 0.863079 # # Kappa : -0.0217 # # Mcnemar\u0026#39;s Test P-Value : 0.006099 # # Sensitivity : 0.20930 # Specificity : 0.77049 # Pos Pred Value : 0.39130 # Neg Pred Value : 0.58025 # Prevalence : 0.41346 # Detection Rate : 0.08654 # Detection Prevalence : 0.22115 # Balanced Accuracy : 0.48990 # # \u0026#39;Positive\u0026#39; Class : Down # LDA ldaCaret=train(Direction~Lag2+Lag1+Year,data=weeklyTrain,method=\u0026#34;lda\u0026#34;,trainControl=fitControl)summary(ldaCaret)# Length Class Mode # prior 2 -none- numeric # counts 2 -none- numeric # means 6 -none- numeric # scaling 3 -none- numeric # lev 2 -none- character # svd 1 -none- numeric # N 1 -none- numeric # call 4 -none- call # xNames 3 -none- character # problemType 1 -none- character # tuneValue 1 data.frame list # obsLevels 2 -none- character # param 1 -none- listpredict(ldaCaret,weeklyVal) %\u0026gt;% confusionMatrix(weeklyVal$Direction)# Confusion Matrix and Statistics # # Reference # Prediction Down Up # Down 20 19 # Up 23 42 # # Accuracy : 0.5962 # 95% CI : (0.4954, 0.6913) # No Information Rate : 0.5865 # P-Value [Acc \u0026gt; NIR] : 0.4626 # # Kappa : 0.1558 # # Mcnemar\u0026#39;s Test P-Value : 0.6434 # # Sensitivity : 0.4651 # Specificity : 0.6885 # Pos Pred Value : 0.5128 # Neg Pred Value : 0.6462 # Prevalence : 0.4135 # Detection Rate : 0.1923 # Detection Prevalence : 0.3750 # Balanced Accuracy : 0.5768 # # \u0026#39;Positive\u0026#39; Class : Down # KNN Honestly, again, this should be scaled. Plot KNN with the best parameters.\nplot(knnCaret) Figure 10: KNN statistics  Evidently, the accuracy increases with an increase in the number of neighbors considered.\nplot(knnCaret, print.thres = 0.5, type=\u0026#34;S\u0026#34;) Figure 11: Visualizing thresholds for KNN  However this shows that we don\u0026rsquo;t actually get much of an increase in accuracy anyway.\nQuestion 4.11 - Pages 171-172 In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.\n(a) Create a binary variable, mpg01 , that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\n(b) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01 ? Scatter-plots and boxplots may be useful tools to answer this question. Describe your findings.\n\u0026copy; Split the data into a training set and a test set.\n(d) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n(e) Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n(f) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?\n(g) Perform KNN on the training data, with several values of \\(K\\), in order to predict mpg01 . Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of \\(K\\) seems to perform the best on this data set?\nAnswer autoDat\u0026lt;-ISLR::Auto a) Binary Variable autoDat$mpg %\u0026gt;% sort() %\u0026gt;% median()# [1] 22.75 Now we can get a new variable from that.\nnewDat=autoDat newDat$mpg01 \u0026lt;- ifelse(autoDat$mpg\u0026lt;autoDat$mpg %\u0026gt;% sort() %\u0026gt;% median(),0,1) %\u0026gt;% factor() Note that the ifelse command takes a truthy function, value when false, value when true, but does not return a factor automatically so we piped it to factor to ensure it is factorial.\nb) Visual Exploration Some box-plots:\nnewDat %\u0026gt;% pivot_longer(-c(mpg01,name),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=mpg01,y=Value)) + geom_boxplot() + facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;) Figure 12: Box plots  With some scatter plots as well:\nnewDat %\u0026gt;% pivot_longer(-c(mpg01,name,weight),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=weight,y=Value,color=mpg01)) + geom_point() + facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;) Figure 13: Scatter plots  Clearly, origin, year and cylinder are essentially not very relevant numerically for the regression lines and confidence intervals.\nnewDat %\u0026gt;% select(-year,-origin,-cylinders) %\u0026gt;% pivot_longer(-c(mpg01,name,mpg),names_to=\u0026#34;Params\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=mpg,y=Value,color=mpg01)) + geom_point() + geom_smooth(method=lm) + facet_wrap(~ Params, scales = \u0026#34;free_y\u0026#34;)  c) Train-Test Split We can split our data very easily with caret. It is important to remember that for factors, random sampling occurs within each class to preserve the overall class distribution of the data.\nset.seed(1984) trainInd \u0026lt;- createDataPartition(newDat$mpg01, # Factor, so class sampling p=0.7, # 70-30 train-test list=FALSE, # No lists times=1) # No bootstrap autoTrain\u0026lt;-newDat[trainInd,] autoTest\u0026lt;-newDat[-trainInd,] d) LDA with Significant Variables Whenever I see significant I think correlation, so let\u0026rsquo;s take a look at that.\nnewDat %\u0026gt;% select(-mpg01,-name) %\u0026gt;% cor# mpg cylinders displacement horsepower weight # mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 # cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 # displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 # horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 # weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 # acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 # year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 # origin 0.5652088 -0.5689316 -0.6145351 -0.4551715 -0.5850054 # acceleration year origin # mpg 0.4233285 0.5805410 0.5652088 # cylinders -0.5046834 -0.3456474 -0.5689316 # displacement -0.5438005 -0.3698552 -0.6145351 # horsepower -0.6891955 -0.4163615 -0.4551715 # weight -0.4168392 -0.3091199 -0.5850054 # acceleration 1.0000000 0.2903161 0.2127458 # year 0.2903161 1.0000000 0.1815277 # origin 0.2127458 0.1815277 1.0000000newDat %\u0026gt;% length# [1] 10 Now lets quickly see what it looks like with correlated values removed.\ncorrCols2=newDat %\u0026gt;% select(-mpg01,-name) %\u0026gt;% cor %\u0026gt;% findCorrelation(cutoff=0.85) newRed\u0026lt;-newDat[-c(corrCols2)] newRed %\u0026gt;% summary# mpg weight acceleration year origin # Min. : 9.00 Min. :1613 Min. : 8.00 Min. :70.00 Min. :1.000 # 1st Qu.:17.00 1st Qu.:2225 1st Qu.:13.78 1st Qu.:73.00 1st Qu.:1.000 # Median :22.75 Median :2804 Median :15.50 Median :76.00 Median :1.000 # Mean :23.45 Mean :2978 Mean :15.54 Mean :75.98 Mean :1.577 # 3rd Qu.:29.00 3rd Qu.:3615 3rd Qu.:17.02 3rd Qu.:79.00 3rd Qu.:2.000 # Max. :46.60 Max. :5140 Max. :24.80 Max. :82.00 Max. :3.000 # # name mpg01 # amc matador : 5 0:196 # ford pinto : 5 1:196 # toyota corolla : 5 # amc gremlin : 4 # amc hornet : 4 # chevrolet chevette: 4 # (Other) :365 Inherent in this discussion is the fact that I consider what is correlated to mpg to be a good indicator of what will help mpg01 for obvious reasons.\nNow we can just use the columns we found with findCorrelation.\ncorrCols2 %\u0026gt;% print# [1] 3 4 2names(newDat)# [1] \u0026#34;mpg\u0026#34; \u0026#34;cylinders\u0026#34; \u0026#34;displacement\u0026#34; \u0026#34;horsepower\u0026#34; \u0026#34;weight\u0026#34; # [6] \u0026#34;acceleration\u0026#34; \u0026#34;year\u0026#34; \u0026#34;origin\u0026#34; \u0026#34;name\u0026#34; \u0026#34;mpg01\u0026#34;autoLDA=train(mpg01~cylinders+displacement+horsepower,data=autoTrain,method=\u0026#34;lda\u0026#34;) valScoreLDA=predict(autoLDA,autoTest) Now we can check the statistics.\nconfusionMatrix(valScoreLDA,autoTest$mpg01)# Confusion Matrix and Statistics # # Reference # Prediction 0 1 # 0 56 2 # 1 2 56 # # Accuracy : 0.9655 # 95% CI : (0.9141, 0.9905) # No Information Rate : 0.5 # P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 # # Kappa : 0.931 # # Mcnemar\u0026#39;s Test P-Value : 1 # # Sensitivity : 0.9655 # Specificity : 0.9655 # Pos Pred Value : 0.9655 # Neg Pred Value : 0.9655 # Prevalence : 0.5000 # Detection Rate : 0.4828 # Detection Prevalence : 0.5000 # Balanced Accuracy : 0.9655 # # \u0026#39;Positive\u0026#39; Class : 0 # That is an amazingly accurate model.\nauto_ldaROC\u0026lt;-roc(predictor=as.numeric(valScoreLDA),response=autoTest$mpg01,levels=levels(autoTest$mpg01))# Setting direction: controls \u0026lt; casesggroc(auto_ldaROC)  e) QDA with Significant Variables Same deal as before.\nautoQDA=train(mpg01~cylinders+displacement+horsepower,data=autoTrain,method=\u0026#34;qda\u0026#34;) valScoreQDA=predict(autoQDA,autoTest) Now we can check the statistics.\nconfusionMatrix(valScoreQDA,autoTest$mpg01)# Confusion Matrix and Statistics # # Reference # Prediction 0 1 # 0 56 2 # 1 2 56 # # Accuracy : 0.9655 # 95% CI : (0.9141, 0.9905) # No Information Rate : 0.5 # P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 # # Kappa : 0.931 # # Mcnemar\u0026#39;s Test P-Value : 1 # # Sensitivity : 0.9655 # Specificity : 0.9655 # Pos Pred Value : 0.9655 # Neg Pred Value : 0.9655 # Prevalence : 0.5000 # Detection Rate : 0.4828 # Detection Prevalence : 0.5000 # Balanced Accuracy : 0.9655 # # \u0026#39;Positive\u0026#39; Class : 0 #auto_qdaROC\u0026lt;-roc(predictor=as.numeric(valScoreQDA),response=autoTest$mpg01,levels=levels(autoTest$mpg01))# Setting direction: controls \u0026lt; casesggroc(auto_qdaROC)  OK, this is weird enough to check if it isn\u0026rsquo;t some sort of artifact.\nautoQDA2=train(mpg01~horsepower, data=autoTrain,method=\u0026#39;qda\u0026#39;) valScoreQDA2=predict(autoQDA2, autoTest) confusionMatrix(valScoreQDA2,autoTest$mpg01)# Confusion Matrix and Statistics # # Reference # Prediction 0 1 # 0 42 3 # 1 16 55 # # Accuracy : 0.8362 # 95% CI : (0.7561, 0.8984) # No Information Rate : 0.5 # P-Value [Acc \u0026gt; NIR] : 4.315e-14 # # Kappa : 0.6724 # # Mcnemar\u0026#39;s Test P-Value : 0.005905 # # Sensitivity : 0.7241 # Specificity : 0.9483 # Pos Pred Value : 0.9333 # Neg Pred Value : 0.7746 # Prevalence : 0.5000 # Detection Rate : 0.3621 # Detection Prevalence : 0.3879 # Balanced Accuracy : 0.8362 # # \u0026#39;Positive\u0026#39; Class : 0 # OK, so the model isn\u0026rsquo;t completely creepily correct all the time. In this case we should probably think about what is going on. I would think it is because of the nature of the train-test split we performed. We have ensured during the sampling of our data that the train and test sets contain the SAME distribution (assumed). So that\u0026rsquo;s why our training result and test results are both incredibly good. They\u0026rsquo;re essentially the same thing.\nIn fact, this is the perfect time to consider a validation set, just to see what the models are really doing. Won\u0026rsquo;t get into it right now though.\nf) Logistic with Significant Variables glmAuto.fit=glm(mpg01~cylinders+displacement+horsepower, data=autoTrain, family=binomial)glmAuto.probs = predict(glmAuto.fit,autoTest, type = \u0026#34;response\u0026#34;) glmAuto.pred = rep(1,length(glmAuto.probs)) glmAuto.pred[glmAuto.probs\u0026lt;0.5]=0 glmAuto.pred=factor(glmAuto.pred) confusionMatrix(glmAuto.pred,autoTest$mpg01)# Confusion Matrix and Statistics # # Reference # Prediction 0 1 # 0 56 4 # 1 2 54 # # Accuracy : 0.9483 # 95% CI : (0.8908, 0.9808) # No Information Rate : 0.5 # P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 # # Kappa : 0.8966 # # Mcnemar\u0026#39;s Test P-Value : 0.6831 # # Sensitivity : 0.9655 # Specificity : 0.9310 # Pos Pred Value : 0.9333 # Neg Pred Value : 0.9643 # Prevalence : 0.5000 # Detection Rate : 0.4828 # Detection Prevalence : 0.5172 # Balanced Accuracy : 0.9483 # # \u0026#39;Positive\u0026#39; Class : 0 # g) KNN Modeling Scale the parameters later.\nknnAuto=train(mpg01~cylinders+displacement+horsepower,data=autoTrain,method=\u0026#34;knn\u0026#34;) Plot KNN with the best parameters.\nplot(knnCaret)  Evidently, the accuracy increases with an increase in the number of neighbors considered.\nplot(knnAuto, print.thres = 0.5, type=\u0026#34;S\u0026#34;)  So we can see that \\(5\\) neighbors is a good compromise.\nQuestion 4.12 - Pages 172-173 This problem involves writing functions.\n(a) Write a function, Power() , that prints out the result of raising 2 to the 3rd power. In other words, your function should compute 2^3 and print out the results.\nHint: Recall that x^a raises x to the power a. Use the print() function to output the result.\n(b) Create a new function, Power2() , that allows you to pass any two numbers, x and a , and prints out the value of x^a . You can do this by beginning your function with the line\nPower2=function(x,a){} You should be able to call your function by entering, for instance,\nPower2(3,8) on the command line. This should output the value of \\(3^8\\), namely, \\(6,651\\).\n\u0026copy; Using the Power2() function that you just wrote, compute \\(10^3\\), \\(8^{17}\\), and \\(131^3\\).\n(d) Now create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this result, using the following line:\nreturn(result) The line above should be the last line in your function, before the } symbol.\n(e) Now using the Power3() function, create a plot of \\(f(x)=x^2\\). The x-axis should display a range of integers from \\(1\\) to \\(10\\), and the y-axis should display \\(x^2\\) . Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using log=‘‘x’’, log=‘‘y’’, or log=‘‘xy’’ as arguments to the plot() function.\n(f) Create a function, PlotPower() , that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call\nPlotPower (1:10 ,3) then a plot should be created with an x-axis taking on values \\(1,2,\u0026hellip;,10\\) and a y-axis taking on values \\(1^3,2^3,\u0026hellip;,10^3\\)\nAnswer a) Create a Squaring Function Power=function(x){print(2^x)} Power(3)# [1] 8 b) Generalizing Power to arbitrary numbers Power2=function(x,a){print(x^a)}Power2(3,8)# [1] 6561 c) Random Testing of Power2 Power2(10,3)# [1] 1000Power2(8,17)# [1] 2.2518e+15Power2(131,2)# [1] 17161 d) Return a value Power3=function(x,a){return(x^a)} e) Plot something with Power3 Actually now would be a good place to introduce LaTeX labeling.\n#install.packages(\u0026#34;latex2exp\u0026#34;) library(latex2exp) No log scale.\nqplot(x=seq(1,10),y=Power3(seq(1,10),2)) + ggtitle(\u0026#34;Function without a log scale\u0026#34;) + geom_point() + xlab(\u0026#34;X\u0026#34;) + ylab(TeX(\u0026#34;$X^2$\u0026#34;))  With a log scale.\nqplot(x=seq(1,10),y=Power3(seq(1,10),2)) + ggtitle(\u0026#34;Function with a log scale\u0026#34;) + geom_point() + xlab(\u0026#34;X\u0026#34;) + ylab(TeX(\u0026#34;$X^2$\u0026#34;)) + scale_y_log10()  f) PlotPower Function PlotPower=function(xrange,pow){return(qplot(x=xrange,y=Power3(xrange,pow)))}plotter\u0026lt;-PlotPower(1:10,3) plotter  The R Cookbook is quite neat for some simple tasks like this.\nQuestion 4.13 - Pages 173 Using the Boston data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN models using various subsets of the predictors. Describe your findings.\nAnswer OK, to speed this up, I will simply run through all the work done on the Auto set. Recall that details about this data-set are also here.\nboston\u0026lt;-MASS::Boston  Check unique values   boston %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length)# crim zn indus chas nox rm age dis rad tax # 504 26 76 2 81 446 356 412 9 66 # ptratio black lstat medv # 46 357 455 229 CHAS is of course something which should be a factor, and with RAD having only \\(9\\) levels, I\u0026rsquo;m inclined to make it a factor as well.\nboston\u0026lt;-boston %\u0026gt;% mutate(rad=factor(rad),chas=factor(chas))  Make a median variable   boston$highCrime\u0026lt;- ifelse(boston$crim\u0026lt;boston$crim %\u0026gt;% median(),0,1) %\u0026gt;% factor()  Take a look at the data  Some box-plots:\nboston %\u0026gt;% pivot_longer(-c(rad,chas,highCrime),names_to=\u0026#34;Param\u0026#34;,values_to=\u0026#34;Value\u0026#34;) %\u0026gt;% ggplot(aes(x=highCrime,y=Value,fill=chas)) + geom_boxplot()+ facet_wrap(~Param,scales=\u0026#34;free_y\u0026#34;)  It is surprising, but evidently the CHAS variable is strangely relevant. 1 implies the tract bounds the river, otherwise 0.\n Correlations   boston %\u0026gt;% select(-c(rad,chas,highCrime)) %\u0026gt;% cor# crim zn indus nox rm age # crim 1.0000000 -0.2004692 0.4065834 0.4209717 -0.2192467 0.3527343 # zn -0.2004692 1.0000000 -0.5338282 -0.5166037 0.3119906 -0.5695373 # indus 0.4065834 -0.5338282 1.0000000 0.7636514 -0.3916759 0.6447785 # nox 0.4209717 -0.5166037 0.7636514 1.0000000 -0.3021882 0.7314701 # rm -0.2192467 0.3119906 -0.3916759 -0.3021882 1.0000000 -0.2402649 # age 0.3527343 -0.5695373 0.6447785 0.7314701 -0.2402649 1.0000000 # dis -0.3796701 0.6644082 -0.7080270 -0.7692301 0.2052462 -0.7478805 # tax 0.5827643 -0.3145633 0.7207602 0.6680232 -0.2920478 0.5064556 # ptratio 0.2899456 -0.3916785 0.3832476 0.1889327 -0.3555015 0.2615150 # black -0.3850639 0.1755203 -0.3569765 -0.3800506 0.1280686 -0.2735340 # lstat 0.4556215 -0.4129946 0.6037997 0.5908789 -0.6138083 0.6023385 # medv -0.3883046 0.3604453 -0.4837252 -0.4273208 0.6953599 -0.3769546 # dis tax ptratio black lstat medv # crim -0.3796701 0.5827643 0.2899456 -0.3850639 0.4556215 -0.3883046 # zn 0.6644082 -0.3145633 -0.3916785 0.1755203 -0.4129946 0.3604453 # indus -0.7080270 0.7207602 0.3832476 -0.3569765 0.6037997 -0.4837252 # nox -0.7692301 0.6680232 0.1889327 -0.3800506 0.5908789 -0.4273208 # rm 0.2052462 -0.2920478 -0.3555015 0.1280686 -0.6138083 0.6953599 # age -0.7478805 0.5064556 0.2615150 -0.2735340 0.6023385 -0.3769546 # dis 1.0000000 -0.5344316 -0.2324705 0.2915117 -0.4969958 0.2499287 # tax -0.5344316 1.0000000 0.4608530 -0.4418080 0.5439934 -0.4685359 # ptratio -0.2324705 0.4608530 1.0000000 -0.1773833 0.3740443 -0.5077867 # black 0.2915117 -0.4418080 -0.1773833 1.0000000 -0.3660869 0.3334608 # lstat -0.4969958 0.5439934 0.3740443 -0.3660869 1.0000000 -0.7376627 # medv 0.2499287 -0.4685359 -0.5077867 0.3334608 -0.7376627 1.0000000 Now, unsurprisingly, there\u0026rsquo;s nothing which is really strongly correlated here for some reason.\n Train test splits   set.seed(1984) trainIndCri \u0026lt;- createDataPartition(boston$highCrime, # Factor, so class sampling p=0.7, # 70-30 train-test list=FALSE, # No lists times=1) # No bootstrap bostonTrain\u0026lt;-boston[trainIndCri,] bostonTest\u0026lt;-boston[-trainIndCri,]  Make a bunch of models   glmBos.fit=glm(highCrime~., data=bostonTrain, family=binomial)# Warning: glm.fit: algorithm did not converge# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurredglmBos.probs = predict(glmBos.fit,bostonTest, type = \u0026#34;response\u0026#34;) glmBos.pred = rep(1,length(glmBos.probs)) glmBos.pred[glmBos.probs\u0026lt;0.5]=0 glmBos.pred=factor(glmBos.pred) confusionMatrix(glmBos.pred,bostonTest$highCrime)# Confusion Matrix and Statistics # # Reference # Prediction 0 1 # 0 68 6 # 1 7 69 # # Accuracy : 0.9133 # 95% CI : (0.8564, 0.953) # No Information Rate : 0.5 # P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 # # Kappa : 0.8267 # # Mcnemar\u0026#39;s Test P-Value : 1 # # Sensitivity : 0.9067 # Specificity : 0.9200 # Pos Pred Value : 0.9189 # Neg Pred Value : 0.9079 # Prevalence : 0.5000 # Detection Rate : 0.4533 # Detection Prevalence : 0.4933 # Balanced Accuracy : 0.9133 # # \u0026#39;Positive\u0026#39; Class : 0 #bostonLDA=train(highCrime~.,data=bostonTrain,method=\u0026#39;lda\u0026#39;) bostonQDA=train(highCrime~tax+crim,data=bostonTrain,method=\u0026#39;qda\u0026#39;) bostonKNN=train(highCrime~.,data=bostonTrain,preProcess = c(\u0026#34;center\u0026#34;,\u0026#34;scale\u0026#34;),method=\u0026#39;knn\u0026#39;)bLDAp=predict(bostonLDA,bostonTest) bQDAp=predict(bostonQDA,bostonTest) bKNNp=predict(bostonKNN,bostonTest)confusionMatrix(bLDAp,bostonTest$highCrime)# Confusion Matrix and Statistics # # Reference # Prediction 0 1 # 0 72 6 # 1 3 69 # # Accuracy : 0.94 # 95% CI : (0.8892, 0.9722) # No Information Rate : 0.5 # P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 # # Kappa : 0.88 # # Mcnemar\u0026#39;s Test P-Value : 0.505 # # Sensitivity : 0.9600 # Specificity : 0.9200 # Pos Pred Value : 0.9231 # Neg Pred Value : 0.9583 # Prevalence : 0.5000 # Detection Rate : 0.4800 # Detection Prevalence : 0.5200 # Balanced Accuracy : 0.9400 # # \u0026#39;Positive\u0026#39; Class : 0 #confusionMatrix(bQDAp,bostonTest$highCrime)# Confusion Matrix and Statistics # # Reference # Prediction 0 1 # 0 73 5 # 1 2 70 # # Accuracy : 0.9533 # 95% CI : (0.9062, 0.981) # No Information Rate : 0.5 # P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 # # Kappa : 0.9067 # # Mcnemar\u0026#39;s Test P-Value : 0.4497 # # Sensitivity : 0.9733 # Specificity : 0.9333 # Pos Pred Value : 0.9359 # Neg Pred Value : 0.9722 # Prevalence : 0.5000 # Detection Rate : 0.4867 # Detection Prevalence : 0.5200 # Balanced Accuracy : 0.9533 # # \u0026#39;Positive\u0026#39; Class : 0 #confusionMatrix(bKNNp,bostonTest$highCrime)# Confusion Matrix and Statistics # # Reference # Prediction 0 1 # 0 74 6 # 1 1 69 # # Accuracy : 0.9533 # 95% CI : (0.9062, 0.981) # No Information Rate : 0.5 # P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 # # Kappa : 0.9067 # # Mcnemar\u0026#39;s Test P-Value : 0.1306 # # Sensitivity : 0.9867 # Specificity : 0.9200 # Pos Pred Value : 0.9250 # Neg Pred Value : 0.9857 # Prevalence : 0.5000 # Detection Rate : 0.4933 # Detection Prevalence : 0.5333 # Balanced Accuracy : 0.9533 # # \u0026#39;Positive\u0026#39; Class : 0 # Clearly in this particular case, an LDA model seems to be working out the best for this data when trained on all the parameters, though Logistic Regression is doing quite well too.\n Notes on KNN   plot(bostonKNN)  plot(bostonKNN, print.thres = 0.5, type=\u0026#34;S\u0026#34;)   Comparison  Finally, we will quickly plot some indicative measures.\nknnBosROC\u0026lt;-roc(predictor=as.numeric(bKNNp),response=bostonTest$highCrime)# Setting levels: control = 0, case = 1# Setting direction: controls \u0026lt; caseslogiBosROC\u0026lt;-roc(predictor=as.numeric(glmBos.probs),response=bostonTest$highCrime)# Setting levels: control = 0, case = 1 # Setting direction: controls \u0026lt; casesldaBosROC\u0026lt;-roc(predictor=as.numeric(bLDAp),response=bostonTest$highCrime)# Setting levels: control = 0, case = 1 # Setting direction: controls \u0026lt; casesqdaBosROC\u0026lt;-roc(predictor=as.numeric(bQDAp),response=bostonTest$highCrime)# Setting levels: control = 0, case = 1 # Setting direction: controls \u0026lt; casesggroc(list(KNN=knnBosROC,Logistic=logiBosROC,LDA=ldaBosROC,QDA=qdaBosROC)) Figure 14: plot of chunk unnamed-chunk-87  OK, one of the reasons why these models do so well is because they are all assuming an equal distribution of train and test classes, and they use crim itself as a predictor. This is no doubt a strong reason why these models uniformly perform so well. I\u0026rsquo;d say 5 is the best option.\n James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media. [return] A good introduction to the caret and skimr packages is here [return]   ","permalink":"https://rgoswami.me/posts/islr-ch4/","tags":["solutions","R","ISLR"],"title":"ISLR :: Classification"},{"categories":["personal"],"contents":" Background  Pandora doesn\u0026rsquo;t work outside the states I keep forgetting how to set-up proxychains  Proxychains Technically this article expects proxychains-ng, which seems to be the more up-to-date fork of the original proxychains.\n Install proxychains-ng\n# I am on archlinux.. sudo pacman -S proxychains-ng Copy the configuration to the $HOME directory\ncp /etc/proxychains.conf . Edit said configuration to add some US-based proxy\n  In my particular case, I don\u0026rsquo;t keep the tor section enabled.\ntail $HOME/proxychains.conf# # proxy types: http, socks4, socks5 # ( auth types supported: \u0026#34;basic\u0026#34;-http \u0026#34;user/pass\u0026#34;-socks ) # [ProxyList] # add proxy here ... # meanwile # defaults set to \u0026#34;tor\u0026#34; # socks4 127.0.0.1 9050 I actually use Windscribe for my VPN needs, and they have a neat SOCKS5 proxy setup. This works out to a line like socks5 $IP $PORT $USERNAME $PASS being added. The default generator gives you a pretty server name, but to get the IP I use ping $SERVER and put that in the conf file.\nPandora I use the excellent pianobar frontend.\n Get pianobar\nsudo pacman -S pianobar Use it with proxychains\nproxychains pianobar Profit\n  I also like setting up some defaults to make life easier:\nmkdir -p ~/.config/pianobar vim ~/.config/pianobar/config I normally set the following (inspired by the ArchWiki):\naudio_quality = {high, medium, low} autostart_station = $ID password = \u0026#34;$PASS\u0026#34; user = \u0026#34;$emailID\u0026#34; The autostart_station ID can be obtained by inspecting the terminal output during an initial run. I usually set it to the QuickMix station.\n","permalink":"https://rgoswami.me/posts/pandora-proxychains/","tags":["tools","workflow"],"title":"Pandora and Proxychains"},{"categories":["programming"],"contents":" Background  I dislike Jupyter notebooks (and JupyterHub) a lot EIN is really not much of a solution either  In the past I have written some posts on TeX with JupyterHub and discussed ways to use virtual Python with JupyterHub in a more reasonable manner.\nHowever, I personally found that EIN was a huge pain to work with, and I mostly ended up working with the web-interface anyway.\nIt is a bit redundant to do so, given that at-least for my purposes, the end result was a LaTeX document. Breaking down the rest of my requirements went a bit like this:\n What exports well to TeX? Org, Markdown, anything which goes into pandoc What displays code really well? LaTeX, Markdown, Org What allows easy visualization of code snippets? Rmarkdown, RStudio, JupyterHub, Org with babel  Clearly, orgmode is the common denominator, and ergo, a perfect JupyterHub alternative.\nSetup Throughout this post I will assume the following structure:\ntree tmp mkdir -p tmp/images touch tmp/myFakeJupyter.org    tmp        ├── images     └── myFakeJupyter.org     1 directory, 1 file    As is evident, we have a folder tmp which will have all the things we need for dealing with our setup.\nVirtual Python Without waxing too eloquent on the whole reason behind doing this, since I will rant about virtual python management systems elsewhere, here I will simply describe my preferred method, which is using poetry.\n# In a folder above tmp poetry init poetry add numpy matplotlib scipy pandas The next part is optional, but a good idea if you figure out using direnv and have configured layout_poetry as described here:\n# Same place as the poetry files echo \u0026#34;layout_poetry()\u0026#34; \u0026gt;\u0026gt; .envrc Note:\n We can nest an arbitrary number of the tmp structures under a single place we define the poetry setup I prefer using direnv to ensure that I never forget to hook into the right environment  Orgmode This is not an introduction to org, however in particular, there are some basic settings to keep in mind to make sure the set-up works as expected.\nIndentation Python is notoriously weird about whitespace, so we will ensure that our export process does not mangle whitespace and offend the python interpreter. We will have the following line at the top of our orgmode file:\n# -*- org-src-preserve-indentation: t; org-edit-src-content: 0; -*- Note:\n this post is actually generating the file being discussed here by  tangling the file\n You can get the whole file here  TeX Settings These are also basically optional, but at the very least you will need the following:\n#+author: Rohit Goswami #+title: Whatever #+subtitle: Wittier line about whatever #+date: \\today #+OPTIONS: toc:nil I actually use a lot of math using the TeX input mode in Emacs, so I like the following settings for math:\n# For math display #+LATEX_HEADER: \\usepackage{amsfonts} #+LATEX_HEADER: \\usepackage{unicode-math} There are a bunch of other settings which may be used, but these are the bare minimum, more on that would be in a snippet anyway.\nNote:\n rendering math in the orgmode file in this manner requires that we use XeTeX to compile the final file  Org-Python We essentially need to ensure that:\n Babel uses our virtual python The same session is used for each block  We will get our poetry python pretty easily:\nwhich python Now we will use this as a common header-arg passed into the property drawer to make sure we don\u0026rsquo;t need to set them in every code block.\nWe can use the following structure in our file:\n\\* Python Stuff :PROPERTIES: :header-args: :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both :END: Now we can simply work with code as we normally would \\#+BEGIN_SRC python print(\u0026#34;Hello World\u0026#34;) \\#+END_SRC Note:\n For some reason, this property needs to be set on every heading (as of Feb 13 2020) In the actual file you will want to remove extraneous \\ symbols:  \\* → * \\#+BEGIN_SRC → #+BEGIN_SRC \\#+END_SRC → #+END_SRC   Python Images and Orgmode To view images in orgmode as we would in a JupyterLab notebook, we will use a slight trick.\n We will ensure that the code block returns a file object with the arguments The code block should end with a print statement to actually generate the file name\nSo we want a code block like this:\n   #+BEGIN_SRC python :results output file :exports both import matplotlib.pyplot as plt from sklearn.datasets.samples_generator import make_circles X, y = make_circles(100, factor=.1, noise=.1) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\u0026#39;autumn\u0026#39;) plt.xlabel(\u0026#39;x1\u0026#39;) plt.ylabel(\u0026#39;x2\u0026#39;) plt.savefig(\u0026#39;images/plotCircles.png\u0026#39;, dpi = 300) print(\u0026#39;images/plotCircles.png\u0026#39;) # return filename to org-mode #+end_src Which would give the following when executed:\n#+RESULTS: [[file:images/plotCircles.png]] Since that looks pretty ugly, this will actually look like this:\nimport matplotlib.pyplot as plt from sklearn.datasets.samples_generator import make_circles X, y = make_circles(100, factor=.1, noise=.1) plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=\u0026#39;autumn\u0026#39;) plt.xlabel(\u0026#39;x1\u0026#39;) plt.ylabel(\u0026#39;x2\u0026#39;) plt.savefig(\u0026#39;images/plotCircles.png\u0026#39;, dpi = 300) print(\u0026#39;images/plotCircles.png\u0026#39;) # return filename to org-mode  Bonus A better way to simulate standard jupyter workflows is to just specify the properties once at the beginning.\n#+PROPERTY: header-args:python :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both This setup circumvents having to set the properties per sub-tree, though for very large projects, it is useful to use different processes.\nConclusions  The last step is of course to export the file as to a TeX file and then compile that with something like latexmk -pdfxe -shell-escape file.tex  There are a million and one variations of this of course, but this is enough to get started.\nThe whole file is also reproduced here.\n","permalink":"https://rgoswami.me/posts/jupyter-orgmode/","tags":["tools","emacs","workflow","orgmode"],"title":"Replacing Jupyter with Orgmode"},{"categories":["programming"],"contents":" Background  I end up writing about using poetry a lot I almost always use direnv in real life too I don\u0026rsquo;t keep writing mini scripts in my .envrc  Honestly there\u0026rsquo;s nothing here anyone using the direnv wiki will find surprising, but then it is still neat to link back to.\nSetting Up Poetry This essentially works by simply modifying the global .direnvrc which essentially gets sourced by every local .envrc anyway.\nvim $HOME/.direnvrc So what we put in there is the following snippet derived from other snippets on the wiki, and is actually now there too.\n# PUT this here layout_poetry() { if [[ ! -f pyproject.toml ]]; then log_error \u0026#39;No pyproject.toml found. Use `poetry new` or `poetry init` to create one first.\u0026#39; exit 2 fi local VENV=$(dirname $(poetry run which python)) export VIRTUAL_ENV=$(echo \u0026#34;$VENV\u0026#34; | rev | cut -d\u0026#39;/\u0026#39; -f2- | rev) export POETRY_ACTIVE=1 PATH_add \u0026#34;$VENV\u0026#34; } Now we can just make .envrc files with layout_poetry and everything will just work™.\n","permalink":"https://rgoswami.me/posts/poetry-direnv/","tags":["tools","direnv","workflow","python"],"title":"Poetry and Direnv"},{"categories":["notes"],"contents":" Background As a member of several large organizations, I get a lot of github notifications. Not all of these are of relevance to me. This is especially true of psuedo-monorepo style repositories like the JOSS review system and especially the exercism community.\n I recently (re-)joined the exercism community as a maintainer for the C++ lessons after having been a (sporadic) teacher This was largely in response to a community call to action as the group needed new blood to usher in v3 of the exercism project  Anyway, I have since found that at the small cost of possibly much of my public repo data, I can manage my notifications better with Octobox\nOctobox  It appears to be free for now It syncs on demand (useful) I can search things quite easily They have a neat logo There appear to be many features I probably won\u0026rsquo;t use  It looks like this:\nFigure 1: Octobox Stock Photo  ","permalink":"https://rgoswami.me/posts/ghnotif/","tags":["tools","github","workflow"],"title":"Taming Github Notifications"},{"categories":["projects"],"contents":" Why this site exists I have a lot of online presences. I have been around (or at-least, lurking) for over ten years. Almost as long as I have been programming. Anyway, I have a penchant lately for using emacs and honestly there isn\u0026rsquo;t very good support for org-mode files. There are options recently with gatsby as well, but this seemed kinda neat.\nWhat \u0026lsquo;this\u0026rsquo; is  This site is built by Hugo The posts are generated with ox-hugo The theme is based of this excellent one and my modifications are here  What is here  Mostly random thoughts I don\u0026rsquo;t mind people knowing Some tech stuff which isn\u0026rsquo;t coherent enough to be put in any form with references Emacs specific workflows which I might want to write about more than short notes on the config  What isn\u0026rsquo;t here  Some collections should and will go to my grimoire My doom-emacs configuration Academic stuff is better tracked on Publons or Google Scholar or my pages hosted by my favorite IITK group or UI group  ","permalink":"https://rgoswami.me/posts/rationale/","tags":["ramblings","explanations"],"title":"Site Rationale"},{"categories":["programming"],"contents":" Chapter II - Statistical Learning All the questions are as per the ISL seventh printing\u0026nbsp;1.\nQuestion 2.8 - Pages 54-55 This exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for \\(777\\) different universities and colleges in the US. The variables are\n Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10 % of high school class Top25perc : New students from top 25 % of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.\u0026rsquo;s Terminal : Percent of faculty with terminal degree S.F.Ratio : Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate  Before reading the data into R, it can be viewed in Excel or a text editor.\n(a) Use the read.csv() function to read the data into R . Call the loaded data college. Make sure that you have the directory set to the correct location for the data.\n(b) Look at the data using the fix() function. You should notice that the ﬁrst column is just the name of each university. We don\u0026rsquo;t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:\nrownames(college)=college[,1] fix(college) You should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the ﬁrst column in the data where the names are stored. Try:\ncollege=college[,-1] fix(college) \u0026copy;\n Use the summary() function to produce a numerical summary of the variables in the data set. Use the pairs() function to produce a scatterplot matrix of the ﬁrst ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10] . Use the plot() function to produce side-by-side boxplots of Outstate versus Private . Create a new qualitative variable, called Elite , by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top \\(10%\\) of their high school classes exceeds \\(50%\\).   Elite = rep(\u0026#34;No\u0026#34;, nrow(college)) Elite [college$Top10perc \u0026gt;50]=\u0026#34;Yes\u0026#34; Elite = as.factor (Elite) college = data.frame (college, Elite) Use the summary() function to see how many elite univer- sities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite .\n Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative vari- ables. You may fnd the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways. Continue exploring the data, and provide a brief summary of what you discover.  Answer Instead of reading in data, for ISLR in particular we can load the ISLR library which is on CRAN and contains the data-sets required for the book.\ninstall.packages(\u0026#34;ISLR\u0026#34;) Thus, we can now read it in as library(\u0026quot;ISLR\u0026quot;)\nThe remaining sections are meant to be executed, and are marked as such, with r in {}.\n\u0026copy;\nWe will load the dataset once for the whole document.\nlibrary(\u0026#34;ISLR\u0026#34;)  Usage of the summary() function   summary(ISLR::College)## Private Apps Accept Enroll Top10perc ## No :212 Min. : 81 Min. : 72 Min. : 35 Min. : 1.00 ## Yes:565 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 1st Qu.:15.00 ## Median : 1558 Median : 1110 Median : 434 Median :23.00 ## Mean : 3002 Mean : 2019 Mean : 780 Mean :27.56 ## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 3rd Qu.:35.00 ## Max. :48094 Max. :26330 Max. :6392 Max. :96.00 ## Top25perc F.Undergrad P.Undergrad Outstate ## Min. : 9.0 Min. : 139 Min. : 1.0 Min. : 2340 ## 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 1st Qu.: 7320 ## Median : 54.0 Median : 1707 Median : 353.0 Median : 9990 ## Mean : 55.8 Mean : 3700 Mean : 855.3 Mean :10441 ## 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 3rd Qu.:12925 ## Max. :100.0 Max. :31643 Max. :21836.0 Max. :21700 ## Room.Board Books Personal PhD ## Min. :1780 Min. : 96.0 Min. : 250 Min. : 8.00 ## 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 1st Qu.: 62.00 ## Median :4200 Median : 500.0 Median :1200 Median : 75.00 ## Mean :4358 Mean : 549.4 Mean :1341 Mean : 72.66 ## 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 3rd Qu.: 85.00 ## Max. :8124 Max. :2340.0 Max. :6800 Max. :103.00 ## Terminal S.F.Ratio perc.alumni Expend ## Min. : 24.0 Min. : 2.50 Min. : 0.00 Min. : 3186 ## 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 1st Qu.: 6751 ## Median : 82.0 Median :13.60 Median :21.00 Median : 8377 ## Mean : 79.7 Mean :14.09 Mean :22.74 Mean : 9660 ## 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 3rd Qu.:10830 ## Max. :100.0 Max. :39.80 Max. :64.00 Max. :56233 ## Grad.Rate ## Min. : 10.00 ## 1st Qu.: 53.00 ## Median : 65.00 ## Mean : 65.46 ## 3rd Qu.: 78.00 ## Max. :118.00  Usage of pairs()   tenColl \u0026lt;- ISLR::College[,1:10] # For getting the first ten columns pairs(tenColl) # Scatterplot Figure 1: Pairs   Boxplot creation with plot()   plot(ISLR::College$Private,ISLR::College$Outstate,xlab=\u0026#34;Private\u0026#34;,ylab=\u0026#34;Outstate\u0026#34;) Figure 2: Boxplots   Binning and plotting   college=ISLR::College Elite=rep(\u0026#34;No\u0026#34;,nrow(college)) Elite[college$Top10perc\u0026gt;50]=\u0026#34;Yes\u0026#34; Elite=as.factor(Elite) college\u0026lt;-data.frame(college,Elite) summary(college$Elite)## No Yes ## 699 78plot(college$Outstate,college$Elite,xlab=\u0026#34;Outstate\u0026#34;,ylab=\u0026#34;Elite\u0026#34;) Figure 3: Plotting Outstate and Elite   Histograms with hist()   par(mfrow=c(2,2)) hist(college$Enroll) hist(college$perc.alumni, col=2) hist(college$Personal, col=3, breaks=10) hist(college$PhD, breaks=10) Figure 4: Histogram  hist(college$Top10perc, col=\u0026#34;blue\u0026#34;) hist(college$Outstate, col=23) Figure 5: Colored Histogram   Explorations (graphical)  \\(0\\) implies the faculty have PhDs. It is clear that people donate more when faculty do not have terminal degrees.\nplot(college$Terminal-college$PhD, college$perc.alumni) Figure 6: Terminal degrees and alumni  High tuition correlates to high graduation rate.\nplot(college$Expend, college$Grad.Rate) Figure 7: Tuiton and graduation  Low acceptance implies a low student to faculty ratio.\nplot(college$Accept / college$Apps, college$S.F.Ratio) Figure 8: Acceptance and Student/Faculty ratio  Question 2.9 - Page 56 This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.\n(a) Which of the predictors are quantitative, and which are qualitative?\n(b) What is the range of each quantitative predictor? You can answer this using the range() function.\n\u0026copy; What is the mean and standard deviation of each quantitative predictor?\n(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\n(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.\nAnswer Once again, since the dataset is loaded from the library, we will simply start manipulating it.\n# Clean data autoDat\u0026lt;-na.omit(ISLR::Auto) # renamed for convenience (a) To determine weather the variables a qualitative or quantitative we can either inspect the variables by eye, or query the dataset.\nsummary(autoDat) # Observe the output for variance## mpg cylinders displacement horsepower weight ## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 ## 1st Qu.:17.00 1st Qu.:4.000 1st Qu.:105.0 1st Qu.: 75.0 1st Qu.:2225 ## Median :22.75 Median :4.000 Median :151.0 Median : 93.5 Median :2804 ## Mean :23.45 Mean :5.472 Mean :194.4 Mean :104.5 Mean :2978 ## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:275.8 3rd Qu.:126.0 3rd Qu.:3615 ## Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 ## ## acceleration year origin name ## Min. : 8.00 Min. :70.00 Min. :1.000 amc matador : 5 ## 1st Qu.:13.78 1st Qu.:73.00 1st Qu.:1.000 ford pinto : 5 ## Median :15.50 Median :76.00 Median :1.000 toyota corolla : 5 ## Mean :15.54 Mean :75.98 Mean :1.577 amc gremlin : 4 ## 3rd Qu.:17.02 3rd Qu.:79.00 3rd Qu.:2.000 amc hornet : 4 ## Max. :24.80 Max. :82.00 Max. :3.000 chevrolet chevette: 4 ## (Other) :365str(autoDat) # Directly find find out## \u0026#39;data.frame\u0026#39;: 392 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cylinders : num 8 8 8 8 8 8 8 8 8 8 ... ## $ displacement: num 307 350 318 304 302 429 454 440 455 390 ... ## $ horsepower : num 130 165 150 150 140 198 220 215 225 190 ... ## $ weight : num 3504 3693 3436 3433 3449 ... ## $ acceleration: num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : num 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : num 1 1 1 1 1 1 1 1 1 1 ... ## $ name : Factor w/ 304 levels \u0026#34;amc ambassador brougham\u0026#34;,..: 49 36 231 14 161 141 54 223 241 2 ... From the above view, we can see that there is only one listed as a qualitative variable or factor, and that is name. However, we can also do this in a cleaner manner or at-least in a different manner with a function.\nfindFactors \u0026lt;- sapply(autoDat,is.factor) findFactors## mpg cylinders displacement horsepower weight acceleration ## FALSE FALSE FALSE FALSE FALSE FALSE ## year origin name ## FALSE FALSE TRUE Though only name is listed as a qualitative variable, we note that origin seems to be almost qualitative as well.\nlength(unique(autoDat$origin))## [1] 3unique(autoDat$origin)## [1] 1 3 2 Infact we can check that nothing else has this property by repeated application of sapply, though a pipe would be more satisfying\ngetUniq\u0026lt;-sapply(autoDat, unique) getLengths\u0026lt;-sapply(getUniq,length) getLengths## mpg cylinders displacement horsepower weight acceleration ## 127 5 81 93 346 95 ## year origin name ## 13 3 301 This is really nicer with pipes\nlibrary(dplyr)## ## Attaching package: \u0026#39;dplyr\u0026#39;## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, unionautoDat %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length)## mpg cylinders displacement horsepower weight acceleration ## 127 5 81 93 346 95 ## year origin name ## 13 3 301 At any rate, we know now that origin and name are probably qualitative, and the rest are quantitative.\n(b) Using range()\nA nice feature of the dataset we have is that the suspected qualitative variables are at the end of the dataset. So we can simply select the first \\(7\\) rows and go nuts on them.\nautoDat[,1:7] %\u0026gt;% sapply(range) # or sapply(autoDat[,1:7],range)## mpg cylinders displacement horsepower weight acceleration year ## [1,] 9.0 3 68 46 1613 8.0 70 ## [2,] 46.6 8 455 230 5140 24.8 82 Once again, more elegant with pipes and subset()\nautoDat %\u0026gt;% subset(select=-c(name,origin)) %\u0026gt;% sapply(range)## mpg cylinders displacement horsepower weight acceleration year ## [1,] 9.0 3 68 46 1613 8.0 70 ## [2,] 46.6 8 455 230 5140 24.8 82# Even simpler with dplyr autoDat %\u0026gt;% select(-name,-origin) %\u0026gt;% sapply(range)## mpg cylinders displacement horsepower weight acceleration year ## [1,] 9.0 3 68 46 1613 8.0 70 ## [2,] 46.6 8 455 230 5140 24.8 82 \u0026copy; Mean and standard deviation\nnoFactors \u0026lt;- autoDat %\u0026gt;% select(-name,-origin) noFactors %\u0026gt;% sapply(mean)## mpg cylinders displacement horsepower weight acceleration ## 23.445918 5.471939 194.411990 104.469388 2977.584184 15.541327 ## year ## 75.979592noFactors %\u0026gt;% sapply(sd)## mpg cylinders displacement horsepower weight acceleration ## 7.805007 1.705783 104.644004 38.491160 849.402560 2.758864 ## year ## 3.683737 (d) Removing observations 10-85 and testing.\nnoFactors[-(10:85),] %\u0026gt;% sapply(mean)## mpg cylinders displacement horsepower weight acceleration ## 24.404430 5.373418 187.240506 100.721519 2935.971519 15.726899 ## year ## 77.145570noFactors[-(10:85),] %\u0026gt;% sapply(sd)## mpg cylinders displacement horsepower weight acceleration ## 7.867283 1.654179 99.678367 35.708853 811.300208 2.693721 ## year ## 3.106217 (e) Plots for determining relationships\npar(mfrow=c(2,2)) plot(autoDat$weight, autoDat$horsepower) plot(autoDat$weight, autoDat$acceleration) plot(autoDat$displacement, autoDat$acceleration) plot(autoDat$cylinders, autoDat$acceleration) Figure 9: Relationship determination   Evidently horsepower is directly proportional to weight but acceleration is inversely proportional to weight Acceleration is also inversely proportional to displacement Cylinders are a poor measure, not surprising since there are only \\(5\\) values  (f) Choosing predictors for gas mileage mpg\nLet us recall certain key elements of the quantitative aspects of the dataset.\nsummary(noFactors) # To understand the spread## mpg cylinders displacement horsepower weight ## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 ## 1st Qu.:17.00 1st Qu.:4.000 1st Qu.:105.0 1st Qu.: 75.0 1st Qu.:2225 ## Median :22.75 Median :4.000 Median :151.0 Median : 93.5 Median :2804 ## Mean :23.45 Mean :5.472 Mean :194.4 Mean :104.5 Mean :2978 ## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:275.8 3rd Qu.:126.0 3rd Qu.:3615 ## Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 ## acceleration year ## Min. : 8.00 Min. :70.00 ## 1st Qu.:13.78 1st Qu.:73.00 ## Median :15.50 Median :76.00 ## Mean :15.54 Mean :75.98 ## 3rd Qu.:17.02 3rd Qu.:79.00 ## Max. :24.80 Max. :82.00getLengths # To get the number of unique values## mpg cylinders displacement horsepower weight acceleration ## 127 5 81 93 346 95 ## year origin name ## 13 3 301 From this we can assert easily that the number of cylinders is not of much interest for predictions of the mileage.\npar(mfrow=c(3,2)) plot(noFactors$mpg,noFactors$horsepower) plot(noFactors$mpg,noFactors$weight) plot(noFactors$mpg,noFactors$displacement) plot(noFactors$mpg,noFactors$acceleration) plot(noFactors$mpg,noFactors$year) Figure 10: Predictions   So now we know that the mileage increases when horsepower is low, weight is low, displacement is low and acceleration is high  Where low represents an inverse response and high represents a direct response.\n It is also clear that the mileage increases every year  Chapter III - Linear Regression Question 3.9 - Page 122 This question involves the use of multiple linear regression on the Auto data set.\n(a) Produce a scatterplot matrix which includes all of the variables in the data set.\n(b) Compute the matrix of correlations between the variables using the function cor() . You will need to exclude the name variable, cor() which is qualitative.\n\u0026copy; Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:\n Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest?  (d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\n(e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\n(f) Try a few different transformations of the variables, such as \\(\\log{X}\\), \\(\\sqrt{X}\\), \\(X^2\\).Comment on your ﬁndings.\nAnswer Once again, we will use the dataset from the library.\ncleanAuto \u0026lt;- na.omit(autoDat) summary(cleanAuto) # Already created above, so no need to do na.omit again## mpg cylinders displacement horsepower weight ## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 ## 1st Qu.:17.00 1st Qu.:4.000 1st Qu.:105.0 1st Qu.: 75.0 1st Qu.:2225 ## Median :22.75 Median :4.000 Median :151.0 Median : 93.5 Median :2804 ## Mean :23.45 Mean :5.472 Mean :194.4 Mean :104.5 Mean :2978 ## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:275.8 3rd Qu.:126.0 3rd Qu.:3615 ## Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 ## ## acceleration year origin name ## Min. : 8.00 Min. :70.00 Min. :1.000 amc matador : 5 ## 1st Qu.:13.78 1st Qu.:73.00 1st Qu.:1.000 ford pinto : 5 ## Median :15.50 Median :76.00 Median :1.000 toyota corolla : 5 ## Mean :15.54 Mean :75.98 Mean :1.577 amc gremlin : 4 ## 3rd Qu.:17.02 3rd Qu.:79.00 3rd Qu.:2.000 amc hornet : 4 ## Max. :24.80 Max. :82.00 Max. :3.000 chevrolet chevette: 4 ## (Other) :365 (a) Scatterplot\npairs(cleanAuto) Figure 11: Scatterplot  (b) Correlation matrix. For this we exclude the qualitative variables either by using select or by using the existing noFactors dataset\n# A full set ISLR::Auto %\u0026gt;% na.omit %\u0026gt;% select(-name,-origin) %\u0026gt;% cor## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## acceleration year ## mpg 0.4233285 0.5805410 ## cylinders -0.5046834 -0.3456474 ## displacement -0.5438005 -0.3698552 ## horsepower -0.6891955 -0.4163615 ## weight -0.4168392 -0.3091199 ## acceleration 1.0000000 0.2903161 ## year 0.2903161 1.0000000 \u0026copy; Multiple Linear Regression\n# Fit against every variable lm.fit=lm(mpg~.,data=noFactors) summary(lm.fit)## ## Call: ## lm(formula = mpg ~ ., data = noFactors) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6927 -2.3864 -0.0801 2.0291 14.3607 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.454e+01 4.764e+00 -3.051 0.00244 ** ## cylinders -3.299e-01 3.321e-01 -0.993 0.32122 ## displacement 7.678e-03 7.358e-03 1.044 0.29733 ## horsepower -3.914e-04 1.384e-02 -0.028 0.97745 ## weight -6.795e-03 6.700e-04 -10.141 \u0026lt; 2e-16 *** ## acceleration 8.527e-02 1.020e-01 0.836 0.40383 ## year 7.534e-01 5.262e-02 14.318 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 3.435 on 385 degrees of freedom ## Multiple R-squared: 0.8093, Adjusted R-squared: 0.8063 ## F-statistic: 272.2 on 6 and 385 DF, p-value: \u0026lt; 2.2e-16# Fit against one variable noFactors %\u0026gt;% lm(mpg~horsepower,data=.) %\u0026gt;% summary## ## Call: ## lm(formula = mpg ~ horsepower, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5710 -3.2592 -0.3435 2.7630 16.9240 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 39.935861 0.717499 55.66 \u0026lt;2e-16 *** ## horsepower -0.157845 0.006446 -24.49 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 4.906 on 390 degrees of freedom ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 ## F-statistic: 599.7 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16noFactors %\u0026gt;% lm(mpg~year,data=.) %\u0026gt;% summary## ## Call: ## lm(formula = mpg ~ year, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.0212 -5.4411 -0.4412 4.9739 18.2088 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -70.01167 6.64516 -10.54 \u0026lt;2e-16 *** ## year 1.23004 0.08736 14.08 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 6.363 on 390 degrees of freedom ## Multiple R-squared: 0.337, Adjusted R-squared: 0.3353 ## F-statistic: 198.3 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16noFactors %\u0026gt;% lm(mpg~acceleration,data=.) %\u0026gt;% summary## ## Call: ## lm(formula = mpg ~ acceleration, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.989 -5.616 -1.199 4.801 23.239 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 4.8332 2.0485 2.359 0.0188 * ## acceleration 1.1976 0.1298 9.228 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 7.08 on 390 degrees of freedom ## Multiple R-squared: 0.1792, Adjusted R-squared: 0.1771 ## F-statistic: 85.15 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16noFactors %\u0026gt;% lm(mpg~weight,data=.) %\u0026gt;% summary## ## Call: ## lm(formula = mpg ~ weight, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.9736 -2.7556 -0.3358 2.1379 16.5194 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 46.216524 0.798673 57.87 \u0026lt;2e-16 *** ## weight -0.007647 0.000258 -29.64 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 4.333 on 390 degrees of freedom ## Multiple R-squared: 0.6926, Adjusted R-squared: 0.6918 ## F-statistic: 878.8 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16noFactors %\u0026gt;% lm(mpg~displacement,data=.) %\u0026gt;% summary## ## Call: ## lm(formula = mpg ~ displacement, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.9170 -3.0243 -0.5021 2.3512 18.6128 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 35.12064 0.49443 71.03 \u0026lt;2e-16 *** ## displacement -0.06005 0.00224 -26.81 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 4.635 on 390 degrees of freedom ## Multiple R-squared: 0.6482, Adjusted R-squared: 0.6473 ## F-statistic: 718.7 on 1 and 390 DF, p-value: \u0026lt; 2.2e-16  Clearly there is a relationship between the predictors and variables, mostly as described previously, with the following broad trends:\n Inversely proportional to Horsepower, Weight, and Displacement  The predictors which have a relationship to the response are (based on R squared values): \\[ all \u0026gt; weight \u0026gt; displacement \u0026gt; horsepower \u0026gt; year \u0026gt; acceleration \\] However, things lower than horsepower are not statistically significant.\n The visual analysis of the year variable suggests that the mileage grows every year. However, it is clear from the summary, that there is no statistical significance of year when used to fit a single parameter linear model. We note that when we compare this to the multiple linear regression analysis, we see that the year factor accounts for \\(0.7508\\) of the total, that is, the cars become more efficient every year\n  (d) Lets plot these\npar(mfrow=c(2,2)) noFactors %\u0026gt;% lm(mpg~horsepower,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Horsepower\u0026#34;)  noFactors %\u0026gt;% lm(mpg~weight,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Weight\u0026#34;)  noFactors %\u0026gt;% lm(mpg~year,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Year\u0026#34;)  noFactors %\u0026gt;% lm(mpg~acceleration,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Acceleration\u0026#34;)  noFactors %\u0026gt;% lm(mpg~displacement,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage v/s Displacement\u0026#34;)  noFactors %\u0026gt;% lm(mpg~.,data=.) %\u0026gt;% plot(main=\u0026#34;Mileage Multiple Regression\u0026#34;)  Form this we can see that the fit is not very accurate as there is a clear curve to the residuals. The 14th point has high leverage, though it is of a small magnitude. Thus it is not expected to have affected the plot too much.\nWe know that an observation with a studentized residual greater than \\(3\\) in absolute value are possible outliers. Hence we must plot this.\n# Predict and get the plot fitPlot \u0026lt;- noFactors %\u0026gt;% lm(mpg~.,data=.) # See residuals plot(xlab=\u0026#34;Prediction\u0026#34;,ylab=\u0026#34;Studentized Residual\u0026#34;,x=predict(fitPlot),y=rstudent(fitPlot))  # Try a linear fit of studentized residuals par(mfrow=c(2,2)) plot(lm(predict(fitPlot)~rstudent(fitPlot)))  Clearly the studentized residuals are nonlinear w.r.t the prediction. Also, some points are above the absolute value of \\(3\\) so they might be outliers, in keeping with the leverage plot.\n(e) Interaction effects\nWe recall that x*y corresponds to x+y+x:y\n# View the correlation matrix cleanAuto %\u0026gt;% select(-name,-origin) %\u0026gt;% cor## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## acceleration year ## mpg 0.4233285 0.5805410 ## cylinders -0.5046834 -0.3456474 ## displacement -0.5438005 -0.3698552 ## horsepower -0.6891955 -0.4163615 ## weight -0.4168392 -0.3091199 ## acceleration 1.0000000 0.2903161 ## year 0.2903161 1.0000000summary(lm(mpg~weight*displacement*year,data=noFactors[(10:85),]))## ## Call: ## lm(formula = mpg ~ weight * displacement * year, data = noFactors[(10:85), ## ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3020 -0.9055 0.0966 0.8912 3.7049 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.961e+02 2.578e+02 1.537 0.129 ## weight -1.030e-01 1.008e-01 -1.021 0.311 ## displacement -1.587e+00 1.308e+00 -1.213 0.229 ## year -4.889e+00 3.623e+00 -1.349 0.182 ## weight:displacement 3.926e-04 3.734e-04 1.051 0.297 ## weight:year 1.317e-03 1.418e-03 0.929 0.356 ## displacement:year 2.150e-02 1.846e-02 1.165 0.248 ## weight:displacement:year -5.287e-06 5.253e-06 -1.007 0.318 ## ## Residual standard error: 1.8 on 68 degrees of freedom ## Multiple R-squared: 0.922, Adjusted R-squared: 0.914 ## F-statistic: 114.9 on 7 and 68 DF, p-value: \u0026lt; 2.2e-16summary(lm(mpg~weight*displacement*year,data=noFactors))## ## Call: ## lm(formula = mpg ~ weight * displacement * year, data = noFactors) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.6093 -1.6472 -0.0531 1.2289 14.5604 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -8.437e+01 3.128e+01 -2.697 0.0073 ** ## weight 8.489e-03 1.322e-02 0.642 0.5212 ## displacement 3.434e-01 1.969e-01 1.744 0.0820 . ## year 1.828e+00 4.127e-01 4.430 1.23e-05 *** ## weight:displacement -6.589e-05 5.055e-05 -1.303 0.1932 ## weight:year -2.433e-04 1.744e-04 -1.395 0.1638 ## displacement:year -5.566e-03 2.674e-03 -2.082 0.0380 * ## weight:displacement:year 1.144e-06 6.823e-07 1.677 0.0944 . ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.951 on 384 degrees of freedom ## Multiple R-squared: 0.8596, Adjusted R-squared: 0.8571 ## F-statistic: 336 on 7 and 384 DF, p-value: \u0026lt; 2.2e-16  Adding the interaction effects of the \\(3\\) most positive R value terms improves the existing prediction to be better than that obtained by considering all effects. We note that the best model is obtained by removing the range identified in chapter 2.  (f) Nonlinear transformations\nsummary(lm(mpg~weight*displacement*year+I(year^2),data=noFactors[(10:85),]))## ## Call: ## lm(formula = mpg ~ weight * displacement * year + I(year^2), ## data = noFactors[(10:85), ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.1815 -0.8235 0.0144 1.0076 3.9420 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -4.205e+03 1.810e+03 -2.324 0.0232 * ## weight -8.800e-02 9.709e-02 -0.906 0.3680 ## displacement -1.030e+00 1.276e+00 -0.807 0.4225 ## year 1.238e+02 5.026e+01 2.464 0.0163 * ## I(year^2) -9.000e-01 3.506e-01 -2.567 0.0125 * ## weight:displacement 2.471e-04 3.634e-04 0.680 0.4988 ## weight:year 1.113e-03 1.365e-03 0.815 0.4177 ## displacement:year 1.368e-02 1.800e-02 0.760 0.4501 ## weight:displacement:year -3.254e-06 5.111e-06 -0.637 0.5264 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.73 on 67 degrees of freedom ## Multiple R-squared: 0.929, Adjusted R-squared: 0.9205 ## F-statistic: 109.6 on 8 and 67 DF, p-value: \u0026lt; 2.2e-16summary(lm(mpg~.-I(log(acceleration^2)),data=noFactors[(10:85),]))## ## Call: ## lm(formula = mpg ~ . - I(log(acceleration^2)), data = noFactors[(10:85), ## ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.232 -1.470 -0.211 1.075 7.088 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 41.3787633 24.1208720 1.715 0.0907 . ## cylinders 0.0863161 0.6112822 0.141 0.8881 ## displacement -0.0148491 0.0103249 -1.438 0.1549 ## horsepower -0.0158500 0.0151259 -1.048 0.2984 ## weight -0.0039125 0.0008546 -4.578 2.02e-05 *** ## acceleration -0.1473786 0.1438220 -1.025 0.3091 ## year -0.0378187 0.3380266 -0.112 0.9112 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.262 on 69 degrees of freedom ## Multiple R-squared: 0.8751, Adjusted R-squared: 0.8642 ## F-statistic: 80.55 on 6 and 69 DF, p-value: \u0026lt; 2.2e-16  The best model I found was still the one without the non-linear transformation but with removed outliers and additional interaction effects of displacement,=year= and weight\n A popular approach is to use a log transform for both the inputs and the outputs\n   summary(lm(log(mpg)~.,data=noFactors[(10:85),]))## ## Call: ## lm(formula = log(mpg) ~ ., data = noFactors[(10:85), ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.285805 -0.052358 -0.001456 0.066521 0.209739 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.886e+00 1.028e+00 3.781 0.000328 *** ## cylinders -1.771e-02 2.604e-02 -0.680 0.498669 ## displacement -1.540e-04 4.399e-04 -0.350 0.727314 ## horsepower -2.343e-03 6.444e-04 -3.636 0.000529 *** ## weight -1.960e-04 3.641e-05 -5.383 9.51e-07 *** ## acceleration -1.525e-02 6.128e-03 -2.489 0.015224 * ## year 4.138e-03 1.440e-02 0.287 0.774703 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.09636 on 69 degrees of freedom ## Multiple R-squared: 0.919, Adjusted R-squared: 0.912 ## F-statistic: 130.5 on 6 and 69 DF, p-value: \u0026lt; 2.2e-16summary(lm(log(mpg)~log(weight*displacement*year),data=noFactors[(10:85),]))## ## Call: ## lm(formula = log(mpg) ~ log(weight * displacement * year), data = noFactors[(10:85), ## ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.41121 -0.04107 0.01266 0.07791 0.21056 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 8.91995 0.26467 33.70 \u0026lt;2e-16 *** ## log(weight * displacement * year) -0.34250 0.01508 -22.71 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1158 on 74 degrees of freedom ## Multiple R-squared: 0.8745, Adjusted R-squared: 0.8728 ## F-statistic: 515.6 on 1 and 74 DF, p-value: \u0026lt; 2.2e-16 Question 3.10 - Page 123 This question should be answered using the Carseats data set.\n(a) Fit a multiple regression model to predict Sales using Price, Urban, and US.\n(b) Provide an interpretation of each coefficient in the model. Be careful\u0026mdash;some of the variables in the model are qualitative!\n\u0026copy; Write out the model in equation form, being careful to handle the qualitative variables properly.\n(d) For which of the predictors can you reject the null hypothesis \\(H_0:\\beta_j=0\\)?\n(e) On the basis of your response to the previous question, ﬁt a smaller model that only uses the predictors for which there is evidence of association with the outcome.\n(f) How well do the models in (a) and (e) fit the data?\n(g) Using the model from (e), obtain \\(95%\\) confidence intervals for the coefficient(s).\n(h) Is there evidence of outliers or high leverage observations in the model from (e)?\nAnswer Load the dataset (and clean it)\ncleanCarSeats \u0026lt;- na.omit(ISLR::Carseats) Obtain summary statistics\ncleanCarSeats %\u0026gt;% sapply(unique) %\u0026gt;% sapply(length)## Sales CompPrice Income Advertising Population Price ## 336 73 98 28 275 101 ## ShelveLoc Age Education Urban US ## 3 56 9 2 2str(cleanCarSeats)## \u0026#39;data.frame\u0026#39;: 400 obs. of 11 variables: ## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels \u0026#34;Bad\u0026#34;,\u0026#34;Good\u0026#34;,\u0026#34;Medium\u0026#34;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 2 2 1 2 1 2 1 2 ...summary(cleanCarSeats)## Sales CompPrice Income Advertising ## Min. : 0.000 Min. : 77 Min. : 21.00 Min. : 0.000 ## 1st Qu.: 5.390 1st Qu.:115 1st Qu.: 42.75 1st Qu.: 0.000 ## Median : 7.490 Median :125 Median : 69.00 Median : 5.000 ## Mean : 7.496 Mean :125 Mean : 68.66 Mean : 6.635 ## 3rd Qu.: 9.320 3rd Qu.:135 3rd Qu.: 91.00 3rd Qu.:12.000 ## Max. :16.270 Max. :175 Max. :120.00 Max. :29.000 ## Population Price ShelveLoc Age Education ## Min. : 10.0 Min. : 24.0 Bad : 96 Min. :25.00 Min. :10.0 ## 1st Qu.:139.0 1st Qu.:100.0 Good : 85 1st Qu.:39.75 1st Qu.:12.0 ## Median :272.0 Median :117.0 Medium:219 Median :54.50 Median :14.0 ## Mean :264.8 Mean :115.8 Mean :53.32 Mean :13.9 ## 3rd Qu.:398.5 3rd Qu.:131.0 3rd Qu.:66.00 3rd Qu.:16.0 ## Max. :509.0 Max. :191.0 Max. :80.00 Max. :18.0 ## Urban US ## No :118 No :142 ## Yes:282 Yes:258 ## ## ## ## We can see that:\n Urban, US and ShelveLoc are factors with 2,2 and 3 levels respectively Education has only 9 unique values so we might as well consider it to be a factor too if we need to  (a) Multiple Regression Model\nFit it to things\nsummary(lm(Sales~.,data=cleanCarSeats))## ## Call: ## lm(formula = Sales ~ ., data = cleanCarSeats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8692 -0.6908 0.0211 0.6636 3.4115 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.6606231 0.6034487 9.380 \u0026lt; 2e-16 *** ## CompPrice 0.0928153 0.0041477 22.378 \u0026lt; 2e-16 *** ## Income 0.0158028 0.0018451 8.565 2.58e-16 *** ## Advertising 0.1230951 0.0111237 11.066 \u0026lt; 2e-16 *** ## Population 0.0002079 0.0003705 0.561 0.575 ## Price -0.0953579 0.0026711 -35.700 \u0026lt; 2e-16 *** ## ShelveLocGood 4.8501827 0.1531100 31.678 \u0026lt; 2e-16 *** ## ShelveLocMedium 1.9567148 0.1261056 15.516 \u0026lt; 2e-16 *** ## Age -0.0460452 0.0031817 -14.472 \u0026lt; 2e-16 *** ## Education -0.0211018 0.0197205 -1.070 0.285 ## UrbanYes 0.1228864 0.1129761 1.088 0.277 ## USYes -0.1840928 0.1498423 -1.229 0.220 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.019 on 388 degrees of freedom ## Multiple R-squared: 0.8734, Adjusted R-squared: 0.8698 ## F-statistic: 243.4 on 11 and 388 DF, p-value: \u0026lt; 2.2e-16summary(lm(Sales~US*Price*Urban,data=cleanCarSeats))## ## Call: ## lm(formula = Sales ~ US * Price * Urban, data = cleanCarSeats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.7952 -1.6659 -0.0984 1.6119 7.2433 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 13.456350 1.727210 7.791 6.03e-14 *** ## USYes 2.049051 2.322591 0.882 0.378 ## Price -0.061657 0.014875 -4.145 4.17e-05 *** ## UrbanYes -0.651545 2.071401 -0.315 0.753 ## USYes:Price -0.001567 0.019972 -0.078 0.937 ## USYes:UrbanYes -1.122034 2.759662 -0.407 0.685 ## Price:UrbanYes 0.010793 0.017796 0.606 0.545 ## USYes:Price:UrbanYes 0.001288 0.023619 0.055 0.957 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.473 on 392 degrees of freedom ## Multiple R-squared: 0.2467, Adjusted R-squared: 0.2333 ## F-statistic: 18.34 on 7 and 392 DF, p-value: \u0026lt; 2.2e-16summary(lm(Sales~US+Price+Urban,data=cleanCarSeats))## ## Call: ## lm(formula = Sales ~ US + Price + Urban, data = cleanCarSeats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 \u0026lt; 2e-16 *** ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## Price -0.054459 0.005242 -10.389 \u0026lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: \u0026lt; 2.2e-16 (b) Interpret stuff\nTo interpret the data, we need to determine which of the models fits the data best, we will use anova() to test this:\nlmCarSAll\u0026lt;-lm(Sales~.,data=cleanCarSeats) lmCarStimesPUU\u0026lt;-lm(Sales~US*Price*Urban,data=cleanCarSeats) lmCarSplusPUU\u0026lt;-lm(Sales~US+Price+Urban,data=cleanCarSeats) anova(lmCarSAll,lmCarStimesPUU,lmCarSplusPUU)## Analysis of Variance Table ## ## Model 1: Sales ~ CompPrice + Income + Advertising + Population + Price + ## ShelveLoc + Age + Education + Urban + US ## Model 2: Sales ~ US * Price * Urban ## Model 3: Sales ~ US + Price + Urban ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 388 402.83 ## 2 392 2397.10 -4 -1994.27 480.2082 \u0026lt; 2.2e-16 *** ## 3 396 2420.83 -4 -23.73 5.7149 0.0001772 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1anova(lmCarStimesPUU,lmCarSplusPUU)## Analysis of Variance Table ## ## Model 1: Sales ~ US * Price * Urban ## Model 2: Sales ~ US + Price + Urban ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 392 2397.1 ## 2 396 2420.8 -4 -23.734 0.9703 0.4236 Remember that it is not possible to use anova() unless the same variables are present in all the models being tested, so it is meaningless to use anova for lmCarSAll along with the others, because we can\u0026rsquo;t change the interaction model to get only the main effects.\n We note that due to the low value of the F-statistic and the non-zero value of the p-value we cannot disregard the null hypothesis, or in other words, the models are basically the same in terms of their performance.  This means that I would like to continue with the simpler model, since the increase in R squared is too small to account for dealing with the additional factors.\n We see immediately, that there is a positive correlation only with being in the US Increases in price and being in an urban area actually decrease the sales, which is not surprising since being in the an urban area is probably correlated to a higher price, which we can check immediately   summary(lm(Price~Urban,data=cleanCarSeats))## ## Call: ## lm(formula = Price ~ Urban, data = cleanCarSeats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -92.514 -15.514 1.205 14.595 74.486 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 114.076 2.180 52.330 \u0026lt;2e-16 *** ## UrbanYes 2.438 2.596 0.939 0.348 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 23.68 on 398 degrees of freedom ## Multiple R-squared: 0.002211, Adjusted R-squared: -0.0002965 ## F-statistic: 0.8817 on 1 and 398 DF, p-value: 0.3483 We see that our assumption is validated. Being in an urban area has a low t-statistic for a positive increase on the slope\n Returning to our previous model, we note that there is a high value of the p-value of the t-statistic for Urban being true, this means there isn\u0026rsquo;t a real relationship between being in an urban area and the sales. This makes intuitive sense as well  note t-test is essentially a linear model with one variable, that is, if we want to find out if there is a relation between having a store in an urban area, we could sum all the urban yes and divide by the number of observations and compare that to the sum of all the urban no divided by the number of observations which is essentially the t-test again.\n Price is significant, and has an inverse relation with the sales, so we should keep that in mind  \u0026copy; In Equation Form:\n\\[ Sales=1.200573*USYes - 0.054459*Price - 0.021916*UrbanYes + 13.043469 \\]\n(e) Other models\n We know from our case-study on testing the full multiple linear regression for Sales that there are definitely more important variables being ignored. However, we also know that Urban is not significant, so we can use a smaller model.   lmCarSplusPU\u0026lt;-lm(Sales~US+Price, data=cleanCarSeats) (f) Comparison of models\nsummary(lmCarSplusPU)## ## Call: ## lm(formula = Sales ~ US + Price, data = cleanCarSeats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9269 -1.6286 -0.0574 1.5766 7.0515 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 13.03079 0.63098 20.652 \u0026lt; 2e-16 *** ## USYes 1.19964 0.25846 4.641 4.71e-06 *** ## Price -0.05448 0.00523 -10.416 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.469 on 397 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2354 ## F-statistic: 62.43 on 2 and 397 DF, p-value: \u0026lt; 2.2e-16anova(lmCarSplusPUU,lmCarSplusPU)## Analysis of Variance Table ## ## Model 1: Sales ~ US + Price + Urban ## Model 2: Sales ~ US + Price ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 396 2420.8 ## 2 397 2420.9 -1 -0.03979 0.0065 0.9357 As expected, the low value of the F statistic and the high p-value for the anova() test asserts that the null hypothesis cannot be neglected, thus there are no differences between the model with the insignificant parameter, which is also seen in the R squared value, which is the same for both models\n(g) Confidence Intervals\nconfint(lmCarSplusPU)## 2.5 % 97.5 % ## (Intercept) 11.79032020 14.27126531 ## USYes 0.69151957 1.70776632 ## Price -0.06475984 -0.04419543confint(lmCarSplusPUU)## 2.5 % 97.5 % ## (Intercept) 11.76359670 14.32334118 ## USYes 0.69130419 1.70984121 ## Price -0.06476419 -0.04415351 ## UrbanYes -0.55597316 0.51214085  ☐ Look into trying to plot this with ggplot  (h) Outliers\n We will first check the leverage plots   par(mfrow=c(2,2)) plot(lmCarSplusPU) Figure 12: Leverage Plots  We can see there is a point with high leverage, but it has a low residual. In any case we should check further.\n Now we will check the studentized residuals to see if they are greater than 3   # See residuals plot(xlab=\u0026#34;Prediction\u0026#34;,ylab=\u0026#34;Studentized Residual\u0026#34;,x=predict(lmCarSplusPU),y=rstudent(lmCarSplusPU)) Figure 13: Studentized residuals  Thus I would say there are no outliers in our dataset, as none of our datapoints have an absolute studentized residual above 3.\n James, G., Witten, D., Hastie, T., \u0026amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Berlin, Germany: Springer Science \u0026amp; Business Media. [return]   ","permalink":"https://rgoswami.me/posts/islr-ch2-ch3/","tags":["solutions","R","ISLR"],"title":"ISLR :: Multiple Linear Regression"},{"categories":null,"contents":" Hi.\nI\u0026rsquo;m Rohit Goswami, better known across the web as HaoZeke. I\u0026rsquo;m not the first of my name, which is why instead of rgoswami, I occasionally use rg0swami when I need to be identified by something closer to my name.\nThe actual username is a throwback to back when people liked being anonymous (and with multiple personalities) online, so that ought to give an idea of how old I am. A curriculum vitae is available here.\nIt is difficult to keep this section short and not let it spill into an unstructured memoir. For a while I considered trying to consolidate my online presences but that turned out to be completely impossible without a series of posts and avatars1.\nIntangible Positions This is a set of things which are primarily online and/or voluntary in a non-academic sense.\n I administer and design a bunch of websites, mostly verified on Keybase I am a certified Software Carpentries instructor I officially maintain, for the Software Carpentries, the lesson on R (r-novice-inflammation) I also maintain some packages on the AUR (ArchLinux User Repository) I hone coursework development and teaching with univ.ai I maintain(ed) the official LineageOS image for the Xperia Z5 Dual  Historical Places What follows is a more informal set of places I am or have been associated with or are of significance to me2.\nReykjavík  I am associated with the reputed Jonsson group of the Science Institute at the University of Iceland, where I benefit from the guidance of the erudite and inspiring Prof. Hannes Jonsson My doctoral committee is here, which includes the very excellent inputs of Dr. Elvar Jonsson I have also benefited from sitting in on some formal coursework here, which has been a fascinatingly useful experience  Kanpur  I retain a close association with the fantastic Femtolab at IIT Kanpur under Prof. Debabrata Goswami, who has provided constant guidance throughout my career I am the co-lead developer of the FOSS scientific d-SEAMS software suite for graph theoretic approaches to structure determination of molecular dynamics simulations, along with my exceptional co-lead Amrita Goswami of the CNS Lab under Prof. Jayant K. Singh at IITK I worked with the Nair group as part of the Summer Undergraduate in Research Excellence (SURGE) program, also at IITK Harcourt Butler Technical Institute (HBTI) Kanpur, or the Harcourt Butler Technological University, as it is now called, was where I trained to be a chemical engineer  Bombay  I spent a formative summer under Prof. Rajarshi Chakrabarti of the IIT Bombay Chemistry department, who has been instrumental in developing my interests I also spent some time discussing experiments with Prof. Rajdip Bandyopadhyaya of the IIT Bombay Chemical Engineering department during an industrial internship in fragnance compounding at the R\u0026amp;D department of KEVA Ltd. under Dr. Debojit Chakrabarty  Bangalore  At IISc, I had the good fortune to meet Prof. Hannes Jonsson at a summer workshop on Rare events At the BIC, I undertook formal machine learning and artificial intelligence training under Harvard\u0026rsquo;s Dr. Rahul Dave and Dr. Pavlos Protopapas as part of the univ.ai summer course  Chennai  I spent a very fruitful summer on quantum tomography under Prof. Sibashish Ghosh at the Institute for Mathematical Sciences (IMSc Chennai)  Avatars I thought it might be of use to list a few of my more official visages. This is mostly to ensure people do not confuse me with a Sasquatch3. These mugshots are exactly that, mugshots for profile icons4.\nFigure 1: A collage of mugshots, shuffled and not ordered by date to confuse people trying to kill me  Donations If you\u0026rsquo;ve gotten this far, you might also want to check out the following\u0026nbsp;5:\n Patreon Librepay   I didn\u0026rsquo;t think it would be necessary, but just in case it isn\u0026rsquo;t clear, people listed here are not necessarily all references or anything, this is a personal list of people associated with each city, not a cover letter [return] I grew up on the verdant and beautiful TIFR Mumbai campus, and completed high school and undergraduate stuff while playing with peacocks and things on the IIT Kanpur campus [return] This is not a replacement for an Instagram feed or a Facebook wall, or even a ResearchGate or Publons or ORCID page; all of which I do sporadically remember I have [return] Made with the Mountain Tapir Collage Maker [return] There won\u0026rsquo;t ever be any content behind paywalls though [return]   ","permalink":"https://rgoswami.me/about/","tags":null,"title":"About"},{"categories":null,"contents":"","permalink":"https://rgoswami.me/search/","tags":null,"title":"Search"}]